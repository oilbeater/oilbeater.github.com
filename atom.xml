<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Oilbeater 的自习室</title>
  <icon>http://oilbeater.com/icon.png</icon>
  
  <link href="http://oilbeater.com/atom.xml" rel="self"/>
  
  <link href="http://oilbeater.com/"/>
  <updated>2025-04-14T17:16:50.278Z</updated>
  <id>http://oilbeater.com/</id>
  
  <author>
    <name>Oilbeater</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>DeepSeek MLA -- 为成本优化而生的注意力机制</title>
    <link href="http://oilbeater.com/2025/04/14/deepseek-mla/"/>
    <id>http://oilbeater.com/2025/04/14/deepseek-mla/</id>
    <published>2025-04-14T17:10:03.000Z</published>
    <updated>2025-04-14T17:16:50.278Z</updated>
    
    <content type="html"><![CDATA[<p>DeepSeek 第一次出名是因为 DeepSeek V2 做到了一百万 Token 只要 0.14 美元。同期的 GPT-4 是 30 美元，当时被认为极具性价比的 GPT-3.5 也要 1.5 美元。这个突破性价格的出现在国内引发了一轮价格战，大批大厂模型大幅降价甚至免费。然而和其他大厂烧钱补贴的逻辑不同，DeepSeek 是通过一系列的技术创新实现了成本数量级的下降。这篇文章就来介绍一下这背后最关键的一个技术创新 —— MLA(Multi-Head Latent Attention)。</p><p>MLA 最本质的数学技巧并不复杂，论文里也是一段话就说完了，看完让人感叹竟然还有如此精妙的解法。但是由于和整个 Transformer 架构耦合会导致理解起来有些困难，我这里会尽量简化描述，哪怕你之前完全不了解 Transformer 应该也可以领会到这个方法的精妙。</p><p>当然还是需要有一些线性代数的基础，如果你还记的一个形状为 5*4 的矩阵乘形状为 4*3 的矩阵，结果是一个形状为 5 * 3 的矩阵就可以继续了。</p><p><img src="/../images/20250414233740.png"></p><h2 id="KVCache"><a href="#KVCache" class="headerlink" title="KVCache"></a>KVCache</h2><p>大模型推理的成本的瓶颈在哪里？答案可能会出乎意料 —— 是显存。显卡有大量计算单元，而推理任务又是线性的一次只能出一个 Token，为了充分利用显卡的计算资源达到最大的吞吐，一般会同时运行尽可能多的生成任务。而每个任务在推理过程中都会占用大量显存，如果想运行尽可能多的任务就需要运行时显存占用足够小。而 MLA 在运行时的显存占用是原始注意力机制的 **6.7%**，你没看错，不是降低了 6.7%，是降低了 **93.3%**。打个比喻这一刀下去不是腰斩，而是脚踝斩。在不考虑模型本身的显存占用情况下，近似可以认为 MLA 在相同显存下可以容纳 <strong>15 倍</strong>的生成任务。</p><p>虽然 MLA 的论文里没有细说这个灵感的来源，但我认为他们是从原先的 KVCache 倒推，诞生了一种全新的注意力机制。到这里就不得不说下 KVCache 是什么。</p><p>大模型每生成一个 Token 都需要对之前所有的 Token 进行计算，来得出最新的一个 Token 是什么。但是一般任务不会生成一个 Token 就结束，往往要一直生成到结束符号，这就会导致前面的 Token 每一次都要重复计算。于是 KVCache 的作用就是把之前每个 Token 计算生成的中间结果保存下来，这样就可以避免重复计算了。你可以理解为每个 Token 都被映射成了一个 1000 * 1000 的矩阵，那么我们有没有什么办法来减少这个矩阵的内存占用呢？</p><p><img src="/../images/20250414234534.png"></p><h2 id="MLA"><a href="#MLA" class="headerlink" title="MLA"></a>MLA</h2><p>这里有意思的事情终于可以开始了，我们可以用两个小矩阵相乘来近似一个大矩阵。这里刚才你还记得的线性代数知识可以用上了，1000 * 2 的矩阵乘一个 2 * 1000 的矩阵也可以得到一个 1000 * 1000 的矩阵，而这两个矩阵总共只有 4000 个元素，是 1000 * 1000 矩阵元素数量的 0.4%。</p><p>这就是 MLA 在数学上最核心的思路了，在 DeepSeek V2 中，本来一个 Token 应该被映射为一个 1*16k 的向量，而在使用 MLA 后会先通过一个压缩矩阵将这个 Token 映射为 1*512 的向量，等到需要的时候再通过一个 512 * 16k 的解压矩阵还原成 1*16k 的向量。在这里压缩矩阵和解压矩阵都是通过训练得来，是模型的一部分只会占用固定的显存，而运行时针对每个 Token 的显存占用就只剩这个  1*512 的向量，只有原来的 3%。</p><p>一个完整的对比如下图所示，原先的 MHA 需要 Cache 完整矩阵，而 MLA 只需要 Cache 中间压缩后的一个向量，再还原出完整矩阵。</p><p><img src="/../images/20250415000024.png"><br><img src="/../images/20250414235538.png"></p><p>这一切真的这么美好嘛？让我们想想 KVCache 的最初目的是什么，是为了减少 Token 的重复中间计算，MLA 虽然压缩了 KVCache，但是每次还需要一个解压操作，计算量又回来了。</p><p>这里就是一个更精彩的故事了，按照 Transformer 的计算，中间的 Cache 乘一个解压矩阵后还要再乘一个输出矩阵得到最终的结果，可以粗略理解为最终计算的公式是 Cache * W<sup>解压</sup>  * W<sup>输出</sup> ，根据矩阵计算的结合率，可以先计算后两个矩阵的乘积，将后两个矩阵融合乘一个新的矩阵。由于 W<sup>解压</sup>  和 W<sup>输出</sup> 在训练后是确定的，因此做个简单的后处理把这部分提前算出来就好了。作者在论文中也用 <strong>Fortunately</strong> 来形容这件事情。</p><p>也就是说我们最初是出于压缩 KVCache 的思路去做了压缩和解压，但在实际推理过程中根本不存在解压的过程。在大幅压缩了显存的同时由于过程中的矩阵都变小了，推理所需的计算量也变小了。</p><h2 id="模型能力"><a href="#模型能力" class="headerlink" title="模型能力"></a>模型能力</h2><p>但在这里我其实还有个疑问没有解开，本质上 MLA 是用两个小矩阵相乘得到一个大矩阵，但是并不是所有的大矩阵都能完美分解成两个小矩阵。MLA 实际的搜索空间是小于 MHA 的，理论上来讲 MLA 的模型能力应该更弱。但是按照 DeepSeek 论文里的评测，MLA 的模型能力是要略强于 MHA 的。</p><p><img src="/../images/20250415003909.png"></p><p>这个事情其实就不太好理解了，我倾向于认为 MLA 虽然搜索空间降低了，但是最优解的概率反而变大了，收敛到了一个相比 MHA 更优的解。另外虽然 MLA 的优化是从 MHA 出发，但最终的结果其实是一套全新的注意力机制，模型的架构都发生了很大的变化，或许 DeepSeek 真的发现了一个更有效的注意力机制。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>有不少性能优化方案其实是在玩跷跷板游戏，比如用 GPU 计算时间交换显存空间，或者牺牲模型能力来换成本下降。而 MLA 在把显存打脚踝斩的情况下同时还做到了计算需求下降和模型能力提升，简直匪夷所思。</p><p>另一个感触是在经历了国内移动互联网时代的我们很容易认为价格战就是要赔本赚吆喝，却忘了技术创新才应该是那个最大的杠杆。</p><blockquote><p>这篇博客只是介绍了 MLA 最核心的理念，在实际应用中还有很多具体的问题，例如：如何处理旋转位置编码？K 和 V 的解压矩阵融合其实略有不同，一个是直接应用结合律，一个是转置后再结合，等等。还是建议大家阅读 DeepSeek V2 的原始论文，有这篇文章做基础应该容易理解很多。</p><p>博客里部分图片源自 <a href="https://www.bilibili.com/video/BV1BYXRYWEMj/">DeepSeek-v2 MLA 原理讲解</a>,也建议大家看下这个视频。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;DeepSeek 第一次出名是因为 DeepSeek V2 做到了一百万 Token 只要 0.14 美元。同期的 GPT-4 是 30 美元，当时被认为极具性价比的 GPT-3.5 也要 1.5 美元。这个突破性价格的出现在国内引发了一轮价格战，大批大厂模型大幅降价甚至免</summary>
      
    
    
    
    
    <category term="DeepSeek" scheme="http://oilbeater.com/tags/DeepSeek/"/>
    
    <category term="LLM" scheme="http://oilbeater.com/tags/LLM/"/>
    
    <category term="Paper" scheme="http://oilbeater.com/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>混乱的 Llama 4</title>
    <link href="http://oilbeater.com/2025/04/06/llama-4-chaos/"/>
    <id>http://oilbeater.com/2025/04/06/llama-4-chaos/</id>
    <published>2025-04-06T15:57:23.000Z</published>
    <updated>2025-04-14T17:16:50.278Z</updated>
    
    <content type="html"><![CDATA[<p>前段时间 Meta AI 的负责人离职让人怀疑 Llama 4 的进度出现了问题，结果没过两天 Meta 就发布了 Llama 4，似乎是为了打破传言。然而看完了现在已经公布的模型基础信息，我反而更觉得 Llama 项目内部已经极度混乱了。下面是我根据已有信息的分析，欢迎指正。</p><h2 id="模型基础信息"><a href="#模型基础信息" class="headerlink" title="模型基础信息"></a>模型基础信息</h2><p>Llama 这次正式发布了一个 109B 和 一个 400B 参数量的模型，以及一个未发布的 2T 参数量模型的信息，我把关键的架构信息汇总如下，以下信息均来自 Llama 自己的博客和 huggingface 模型页面：</p><table><thead><tr><th>名称</th><th>参数量</th><th>激活参数量</th><th>专家数</th><th>上下文</th><th>语料量</th><th>GPU 时间</th></tr></thead><tbody><tr><td>Scout</td><td>109B</td><td>17B</td><td>16</td><td>10M</td><td>40T</td><td>5.0M</td></tr><tr><td>Maverick</td><td>400B</td><td>17B</td><td>128+1</td><td>1M</td><td>22T</td><td>2.38M</td></tr><tr><td>Behemoth</td><td>2T</td><td>288B</td><td>16</td><td>-</td><td>-</td><td>-</td></tr></tbody></table><p>这里都不用再看模型的评分表现了，这几个模型架构方面的对比就能看出很多问题了。</p><h2 id="奇怪的-MoE-架构"><a href="#奇怪的-MoE-架构" class="headerlink" title="奇怪的 MoE 架构"></a>奇怪的 MoE 架构</h2><p>Llama 4 这次从 Dense 模型全面转向了 MoE，但是诡异的点在于他们三个模型采用了两套 MoE 架构。最大的 Behemoth 和最小的 Scout 采用的是传统的 MoE，专家数也是 16 这个传统认为比较常规的一个专家数量，而中间的那个 Maverick 采用的却是 DeepSeek MoE 提出的一个新的细粒度专家加共享专家的模型是个 128 专家加 1 共享专家的架构。</p><p>一般来说一代模型都是采用同一个架构，只是在模型的层数和每层的宽度上做调整，两个有很大差异的模型在同一代就很奇怪。而且就算有变化也不应该是最大和最小的保持一致，把中间规模的给换了，给人的感觉是中间的这个 Maverick 其实是被 DeepSeek 冲击下重新仿照 DeepSeek 模型重新训练的，但是时间上来不及把三个都重做，只好就放在一块发布了。</p><h2 id="奇怪的成本投入"><a href="#奇怪的成本投入" class="headerlink" title="奇怪的成本投入"></a>奇怪的成本投入</h2><p>一般来讲，模型参数规模越大，需要投入的成本越高。一方面是更大的模型可以容纳更多的知识，会提供给更大规模模型更多的语料；另一方在语料相同的情况下，更大的模型需要训练的参数更多 GPU 开销也会更高。所以通常来讲模型规模越大，需要的成本会越高。</p><p>然而到了 Llama 4 这里出现了两个指标都相反的情况。Maverick 的参数规模是 Scout 的接近 4 倍，但是 Maverick 训练的语料量只有 Scout 的二分之一，消耗的 GPU 时间同样也只有二分之一。考虑到这两个模型的激活参数量是一致的这个 GPU 时间可以理解，但是语料量也只给一半这个事情就很奇怪了。给我的感觉是要么这次只是试水新型的 MoE 架构，并没有想做完整训练，要么就是训到后面训崩了，从中间那个 snapshot 出来了。</p><h2 id="奇怪的上下文长度"><a href="#奇怪的上下文长度" class="headerlink" title="奇怪的上下文长度"></a>奇怪的上下文长度</h2><p>一般来讲更大的模型，能力会越强。可在这一代 Llama，最让人感到震撼的 10M 上下文是给的最小规模的 Scout，更大的 Maverick 反而是 1M 上下文。考虑到目前扩充上下文的主流方法还是在后训练做微调，更大的 Maverick 在后训练的投入上还不如更小的 Scout。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>给我的感觉是 Llama 4 这一代本来是想走传统 MoE，被 DeepSeek 冲击后又半路开始看 DeepSeek MoE。但是训练可能已经开始了，停下来又有阻力，所以中间又插了一个中规模的 Maverick。按照这个参数量选择来看是想用比 DeepSeek V3 小的参数量实现类似的性能。但是 17B 的激活要追平 DeepSeek V3 的 39B 激活我觉得还是有很大难度的。不过最后能让这一代的模型以这么混乱的形式发布，还加了个期货模型，我还是觉得 Llama 项目内部出了不少的问题。</p><p><img src="/../images/llama4.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;前段时间 Meta AI 的负责人离职让人怀疑 Llama 4 的进度出现了问题，结果没过两天 Meta 就发布了 Llama 4，似乎是为了打破传言。然而看完了现在已经公布的模型基础信息，我反而更觉得 Llama 项目内部已经极度混乱了。下面是我根据已有信息的分析，欢迎指</summary>
      
    
    
    
    
    <category term="LLM" scheme="http://oilbeater.com/tags/LLM/"/>
    
    <category term="Llama" scheme="http://oilbeater.com/tags/Llama/"/>
    
  </entry>
  
  <entry>
    <title>DeepSeek MoE -- 创新型的 MoE 架构</title>
    <link href="http://oilbeater.com/2025/03/29/deepseek-moe/"/>
    <id>http://oilbeater.com/2025/03/29/deepseek-moe/</id>
    <published>2025-03-29T12:54:37.000Z</published>
    <updated>2025-04-14T17:16:50.278Z</updated>
    
    <content type="html"><![CDATA[<p>从 DeepSeek V3&#x2F;R1 开始关注 DeepSeek 工作的人很容易认为 DeepSeek 大量的工作都是在工程上优化效率，但是回看 DeepSeek 过去一年的论文才会发现他们其实一直在模型架构和训练方法上做各种创新，而 V3 和 R1 只是在之前架构创新的基础上进行 Scale。DeepSeek MoE 这篇论文就介绍了 DeepSeek 在 MoE 架构上的主要创新，现在看上去也很有希望成为未来 MoE 架构的标准。</p><h2 id="MoE-vs-Dense"><a href="#MoE-vs-Dense" class="headerlink" title="MoE vs Dense"></a>MoE vs Dense</h2><p>先说一下 MoE 和传统的 Dense 架构的区别。早期的 LLM 基本都是 Dense 架构，也就是每生成一个 Token 需要激活所有的神经元参与计算，这种方式其实和人脑的思考方式是有很大区别的，人脑是不会任何问题都需要调动所有脑细胞的，如果这样的话人早就累死了。所以很自然的一个想法就是生成 Token 的时候不要再激活所有的神经元了，每次只激活和当前任务最相关的神经元，于是就有了 MoE(Mixture of Experts) 架构，把 LLM 里每一层的神经元分割成 N 个 Expert，通过 Router 去选择 K 个最相关的 Expert 激活。</p><p><img src="/../images/20250329161016.png"></p><p>这个架构的好处就是在推理的时候不需要激活所有的神经元，计算量会大幅下降。在 DeepSeek MoE 前最常见的 8 选 2 模式下计算量可以下降到接近 Dense 模型的三分之一。</p><p>MoE 的架构看上去很理想，但本质上是在用少量 Experts 来模拟 Dense 模型的表现，所以关键是在每个 Expert 是否有足够的专业性，能否真的模拟 Dense 模型的表现。如果类比人脑，当神经元足够特化时，特定任务只需要激活少量神经元即可完成。</p><p>DeepSeek MoE 这篇论文就介绍了他们为了把每个 Expert 专业性推到极致所做的两个创新：</p><ul><li>更多更小的 Expert</li><li>知识共享 Expert</li></ul><h2 id="更多更小的-Expert"><a href="#更多更小的-Expert" class="headerlink" title="更多更小的 Expert"></a>更多更小的 Expert</h2><p><img src="/../images/20250329165838.png"></p><p>使用更多更小的 Expert 来增加每个 Expert 的专业性看似是个很符合直观的思路，但是之前主流 MoE 都是 8 个或者 16 个 Expert。可以想象 LLM 要处理的问题类型千千万，这个数量规模的 Expert 显然不可能做到高度的专业化，每个 Expert 都会有大量当前任务无关的知识。</p><p>但是随着 Expert 的数量变大，训练的难度也会变大，Router 很容易只选择少数几个 Expert 导致负载的极度不均衡。最终，理论上的 MoE 架构可能会变成每次只激活同一组 Expert 的小模型。因此，之前大部分 MoE 架构的 Expert 数量都不会太多。</p><p>DeepSeek 经过一组设计的损失函数，给重复选择同一个 Expert 增加了惩罚，从而迫使 Router 更均衡的去选择 Expert。通过这个方式 DeepSeek 解决了训练的问题，开始一步步尝试 scale Expert 的数量。从这篇论文里的 64 选 6，扩展到 128 选 12，到 V2 的 160 选 6，再到 V3 的 256 选 8。</p><p>可以看到 DeepSeek 一步步将 Expert 数量扩展，而且所需要选中的 Expert 比例也从 9% 一步步降低到 2%，证明了确实在 Expert 足够专业化后只需要更少部分的激活就可以完成对应的任务。</p><h2 id="知识共享-Expert"><a href="#知识共享-Expert" class="headerlink" title="知识共享 Expert"></a>知识共享 Expert</h2><p><img src="/../images/20250329170955.png"></p><p>随着 Expert 变小和 Expert 数量增加其实还会带来另外一个问题，那就是每个 Expert 除了需要特定领域的知识外，其实还需要一些通用知识，例如一些通用的语言理解和逻辑分析，可能是每个 Expert 都需要的。如果每个 Expert 都记忆了相关知识那么其实会造成大量的知识冗余，当 Expert 数量变多时，问题会更加明显。这其实会限制每个 Expert 的专业化，训练和推理过程中也会造成资源的浪费。</p><p>DeepSeek 提出的做法是增加一组共享 Expert，这一组 Expert 每个训练样本都会被激活，希望他们在训练过程中可以学到通用的知识，这样其他的 Expert 就无需再去学习这些通用知识，只需要学习专业知识了。当推理过程中这组共享 Expert 也会每次都被激活，来提供通用的知识信息。</p><p>这同样是一个很符合直觉的架构创新，但是由于之间 MoE 架构的 Expert 规模本来就不大，这个优化的意义其实并不明显，只有当规模上去了这个问题才会暴露出来。在这篇论文里 DeepSeek 还根据 Expert 数量按比例扩充了共享型 Expert 数量，但是随着更多的训练和实践，发现其实并不需要那么多共享型 Expert，等到 V3 的时候其实只使用到了 1 个共享型 Expert。</p><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>看完这篇论文我最大的感受是 DeepSeek 并不是拿一个已经验证过的架构无脑堆数据，而是真正的在做模型层面的创新。这也导致在 V3&#x2F;R1 大火之后很多框架第一时间都无法运行 DeepSeek 的模型或者性能也很差，因为 DeepSeek 的模型架构和其他人都有明显的差别。</p><p>并且相比 DeepSeek 其他论文里提到的 MLA、GRPO 和 NSA 这些需要复杂数学功底的创新不同，这两个模型创新都还是相对符合直觉的，但在那个时间点只有 DeepSeek 敢于这么尝试，其他人还在 Follow Llama 的 Dense 模型，敢于去做非主流的尝试，还是需要很大的勇气，这里只能对 DeepSeek 团队再次表达 Respect。</p><p><img src="/../images/20250329200735.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;从 DeepSeek V3&amp;#x2F;R1 开始关注 DeepSeek 工作的人很容易认为 DeepSeek 大量的工作都是在工程上优化效率，但是回看 DeepSeek 过去一年的论文才会发现他们其实一直在模型架构和训练方法上做各种创新，而 V3 和 R1 只是在之前架构创</summary>
      
    
    
    
    
    <category term="DeepSeek" scheme="http://oilbeater.com/tags/DeepSeek/"/>
    
    <category term="LLM" scheme="http://oilbeater.com/tags/LLM/"/>
    
    <category term="Paper" scheme="http://oilbeater.com/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>从 DeepSeek LLM 到 DeepSeek R1 —— DeepSeek LLM</title>
    <link href="http://oilbeater.com/2025/03/14/deepseek-from-llm-to-r1/"/>
    <id>http://oilbeater.com/2025/03/14/deepseek-from-llm-to-r1/</id>
    <published>2025-03-14T10:48:50.000Z</published>
    <updated>2025-04-14T17:16:50.278Z</updated>
    
    <content type="html"><![CDATA[<p>最近找到了 DeepSeek 发表过的论文合集，包含了从 DeepSeek 第一版的 LLM 到最新的 R1 的演变过程。从当下的角度我们当然知道 DeepSeek R1 在模型能力上已经接近了业界最领先的水平，但他是如何一步步从一个在中国一开始都没有被重视的量化公司走到这里的其实更吸引我的注意。</p><p>这个系列的博客我会从论文阅读的角度，试图去寻找他们一步步探索的轨迹，从论文的路径上来看就是 DeepSeek LLM -&gt; DeepSeek MoE -&gt; DeepSeek V2 -&gt; DeepSeek V3 -&gt; DeepSeek R1。在整理论文时我才发现，DeepSeek 第一篇对外发布的论文是在 2024 年的 1 月，当时他们刚发布第一版模型，即使在 AI 行业内也不被认为是个主要竞争者。然而仅仅一年后的 2025 年 1 月，就已经进化到了 R1 这种业界领先水平。都说 AI 一天，人间一年，但是当真看到人间一年的进展时，还是深深的被 DeepSeek 的速度所震撼。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>DeepSeek LLM 是 DeepSeek 开源的第一个 LLM，当时开源 LLM 里最受关注的是 LLaMA-2，很多模型也是基于它的架构和基础进行的。现在从后视的角度我们知道 DeepSeek 最终选择了和 LLaMA 这类 Dense 架构不同的 MoE 架构，但是在当时第一版的时候还是基本上照搬了 LLaMA-2 的架构，进行局部的调整。可以猜测当时团队内部还处于探索模型架构的阶段。</p><p>尽管架构大体和 LLaMA-2 相同，训练的数据量也都是 2T tokens，但是在性能评测上，如下图所示 DeepSeek LLM 基本上是全面超越了 LLaMA-2。论文里介绍了他们发现的一些有趣的关于数据，训练和微调的方法。</p><p><img src="/../images/deepseekllm.png" alt="alt text"></p><p>值得注意的是 DeepSeek 在训练过程中是可以使用更多的数据，使用更大参数量的模型的，显然这样做会提升模型性能。但是这篇论文目的主要是和 LLaMA-2 对比，因此特意把数据规模和参数量都做到尽可能相近，来比较在其他方面还有哪些地方可以提升。</p><h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>LLaMA-2 和 DeepSeek LLM 在数据的选择上还是有很大的区别的。虽然都是 2T 的 token 量，LLaMA-2 里接近 90% 的语料都是英文，而 DeepSeek LLM 虽然没有详细说语言的比例，但是看表述语料里的英文和中文比例应该是比较接近的。所以逻辑上来看 DeepSeek LLM 在中文的评测上大幅领先并不是个意外。意外的反而是在英文评测指标上，DeepSeek LLM 在训练量明显少的情况下依然取得了接近的性能结果。</p><p>我猜测这种现象的原因有两个，第一是 DeepSeek LLM 的语料质量更高，弥补了数量上的劣势。LLaMA-2 在介绍预训练语料的时候说除了一些敏感信息没有对数据集进行过滤，而 DeepSeek LLM 中介绍了为了提高数据质量专门做了模型去评估数据质量，还特意把一些冷门领域的数据占比放大，已获得更好的数据多样性。因此可以推测英文部分的语料质量 DeepSeek LLM 要高一些。作为参考 LLaMA-3 也在数据准备过程中引入了去重和质量过滤来提升语料的质量。</p><p>另一个原因我猜大概是中文语料的引入也提升了模型最终在英文上的表现。OpenAI GPT 3.5 的时候训练语料也是英文为主，但是最终在中文的表现上不差，一个猜测的原因就是在英文语料上学习到的一些知识迁移到了中文。同样中文语料里学习到的一些知识也可以迁移到英文。此外由于中文和英文在语法和表现形式上也有比较大的区别，这种多样化的数据是不是一定程度上也提升了模型的能力？还有就是不同语言本身就有不同的文化背景和内容倾向，这其实也是进一步增加了数据的多样性。如果这个猜测成立的话，那准备语料其实应该刻意地去增加不同语言的比重，让模型可以学习更丰富的语言表达形式。</p><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>模型的架构层面 DeepSeek LLM 和 LLaMA-2 几乎完全一样，各个路径上用到的技术比如 Pre-Norm，FFN 的激活函数，位置编码器都一模一样。最大的区别在于使用 GQA（Group<br>Query Attention）上。GQA 相比最原始的 MHA（Multi Head Attention），可以理解为为了节省训练和推理的 kv cache 占用，直接让多个 Query 头共享一组 Key 和 Value 的参数矩阵，这样可以大幅压缩显存的使用。但是带来的问题就是减少了 Key 和 Value 的潜空间个数，模型的表达能力也出现了下降。LLaMA-2 的做法是通过增加 FFN 网络的宽度来提供更多的非线性表达能力，而 DeepSeek LLM 的做法是增加 Attention 的层数。可以粗略理解尽管模型参数量相同，但是 LLaMA-2 是一个更宽的模型，而 DeepSeek LLM 是一个更深的模型。当然从后视的角度来看，DeepSeek 后续在 V2 公布的 MLA 对 Attention 层做了一个极其激进的更新，直接把推理所需的 KV Cache 降低了一个数量级。</p><p>另一个区别在于 LLaMA-2 使用的是 cosine learning scheduler 而 DeepSeek LLM 使用的是 Multi-Step learning scheduler。给出的理由是当增加数据量的时候，Multi-Step 可以更好的利用前一阶段的结果，持续训练速度会更快。</p><p>此外论文里还花了很大篇幅来介绍如何在不同的数据规模，数据质量，模型规模下选择合适的超参，如何去画 scaling law 曲线。这块是作者当成最大亮点来讲的，但是我看上去感觉和炼丹一样，看的我脑壳疼，感兴趣的同学可以自己看看。</p><h2 id="后训练"><a href="#后训练" class="headerlink" title="后训练"></a>后训练</h2><p>在论文发表的那个时间点，后训练主要是做对齐，也就是通过 SFT 和 RLHF 来对齐人类的偏好，增加模型的安全性。用到的数据基本上也都是一些带标记的对话文本，并没有对数据的分布做特别的处理。DeepSeek LLM 在这里对数据的选择又做出了和 LLaMA-2 很不一样的选择。</p><p>如果看最上面模型性能评估的对比图，可以看到 DeepSeek LLM 在 MATH，HumanEval 和 MBPP 几个非中文的指标表现也要好很多。因为 DeepSeek 在后训练的 SFT 阶段将近 70% 的样本都是 Math 和 Code 相关数据。可见他们根本就没把对齐作为后训练的重点，而是把提升模型能力作为后训练的重点，所以这更像是一个鸡贼的刷榜优化。</p><p>当时主流的做法还是在 base model 训练好了后再 SFT 一个代码和数学领域的模型，比如 Code LLaMA  和 OpenAI Codex 是分别在 LLaMA-2 和 OpenAI GPT3 上 SFT 出来的。Meta 当时甚至还在 Code LLaMA 上再 SFT 一个 Python 专用的 LLM 出来。</p><p>现在我们当然知道在后训练阶段通过在 Math 和 Code 样本上进行 RL 可以激发出模型 CoT 的推理能力，R1 的想法可能在这个时候就已经诞生了。</p><p>此外 DeepSeek LLM 在这里并没有用当时很流行的 RLHF，而是选择 DPO(Direct Preference Optimization) 进行和人类偏好的对齐。这种方法直接对两个不同生成结果的概率差作为优化目标进行训练，这相比 RL 其实更直观也更容易设计，在 LLaMA-3 的后训练过程中也用到了 DPO。可以看出 DeepSeek 团队当时对已有的 RL 算法还是不太满意的，还在探索。这也就造就了后来在 DeepSeek Math 中公布的 GRPO。 </p><h2 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h2><p>在我上学的时候，老师和我说真正的 Future Work 不要写在论文里，自己偷偷做再发下一篇，论文的 Future Work 里就写你觉得没戏的和你觉得做不出来的。而 DeepSeek LLM 最后 Future Work 的几句话从现在的视角来看都太真诚了，几乎已经把 R1 的路子给指出来了。</p><blockquote><p>DeepSeek LLM 将会是一个长期项目，专注于促进开源模型进步，</p></blockquote><p>这个不好说，毕竟满打满算也就一年多。</p><blockquote><p>Soon，我们将会发布 Code 和 MoE 架构的技术报告。MoE 的架构看上去很有希望。</p></blockquote><p>这个 Soon 指的是一周发布 MoE，半个月发布 DeepSeek Code。而我们已经知道 MoE 成为了 V2，V3 和 R1 模型的基础架构，参数量也上升到了 671B。</p><blockquote><p>我们现在已经有了大得多质量好得多的数据集，下一代模型所有指标都会显著提升。</p></blockquote><p>数据量半年后从 2T 变成了 8T，不过同期 LLaMA-3 变成了 15T。DeepSeek V2 的各项指标相比同期的 LLaMA-3 在英文上是稍微落后的，而且在 V2 的时候他们的重点就已经变成了疯狂降低成本。</p><blockquote><p>我们的对齐团队发现强化学习能够增强模型的复杂推理能力。</p></blockquote><p>从今天回头来看，这不就是 R1 最重要的方法么。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从今天的视角来看，DeepSeek 当时应该还在探索期，还在和业界的开源模型对齐，还做了很多理论上的研究。但是从论文的各个细节上来看，一年后那个石破天惊的 R1 诞生的条件已经差不多具备了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近找到了 DeepSeek 发表过的论文合集，包含了从 DeepSeek 第一版的 LLM 到最新的 R1 的演变过程。从当下的角度我们当然知道 DeepSeek R1 在模型能力上已经接近了业界最领先的水平，但他是如何一步步从一个在中国一开始都没有被重视的量化公司走到这</summary>
      
    
    
    
    
    <category term="DeepSeek" scheme="http://oilbeater.com/tags/DeepSeek/"/>
    
    <category term="LLM" scheme="http://oilbeater.com/tags/LLM/"/>
    
    <category term="Paper" scheme="http://oilbeater.com/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>从 Network Binding Plugin 看 KubeVirt 的扩展方式</title>
    <link href="http://oilbeater.com/2025/01/12/kubevirt-network-binding/"/>
    <id>http://oilbeater.com/2025/01/12/kubevirt-network-binding/</id>
    <published>2025-01-12T08:16:07.000Z</published>
    <updated>2025-04-14T17:16:50.278Z</updated>
    
    <content type="html"><![CDATA[<p>在 KubeVirt v1.4 的新版本里将 <a href="https://kubevirt.io/user-guide/network/network_binding_plugins/">Network Binding Plugin</a> 提升到了 Beta，提供了一种新的扩展 KubeVirt 网络的方式。虽然名义上是为了扩展网络的能力，但实际上从实现上来看，这个机制能做的事情远不止网络，所有和 libvirt domain xml 相关的变更都可以通过这个机制来实现。</p><h2 id="KubeVirt-Network-Overview"><a href="#KubeVirt-Network-Overview" class="headerlink" title="KubeVirt Network Overview"></a>KubeVirt Network Overview</h2><p>先从网络的角度来看下 KubeVirt 之前的网络机制有什么问题，新的机制又是如何进行扩展的。</p><p>由于 KubeVirt 使用的是 Pod 里面跑 VM 的架构，所以复用了 CNI 的网络机制。这样的话就将网络分成了两个部分，一个是 Pod 网络由各个 CNI 提供。另一部分就是如何将 CNI 提供的网络接入 VM，在 libvirt 里这部分叫做 Domain 网络。</p><p>KubeVirt 之前的各种网络机制（Bridge，Masquerade，Passt，Slirp）所做的事情就是通过不同的技术方案将 Pod 里的 eth0 接入到 VM 的 tap0 网卡。例如 Bridge 将 tap0 和 eth0 接入到同一个网桥，Masquerade 将 tap0 的流量经过 iptables nat 规则导入 eth0，Passt 和 Slirp 通过用户态的网络栈做流量重定向。</p><p><img src="/../images/kubevirt-networking-tradition.png" alt="alt text"></p><p>这些方法在实现上都是类似的，在 Pod 内部做一些网络相关的配置，然后修改 libvirt 的启动参数接入对应的网络。但是现有的机制都是写死在 KubeVirt Core 里的，并没有扩展机制，想要新增一种机制或者修改已有的机制都需要修改 KubeVirt 的代码很不灵活，例如默认的 bridge 插件会劫持 DHCP 请求，但是又不支持 IP, 所以 bridge 模式下的双栈就很难实现，而 Kube-OVN 中已经实现的 DHCP 又被这个机制绕过去了，之前想做 bridge 的双栈就需要改 KubeVirt 的代码来关闭默认的 DHCP 十分麻烦。因此新版本中将这套机制抽象出来提供了一套通用的机制。</p><h2 id="Hook-Sidecar"><a href="#Hook-Sidecar" class="headerlink" title="Hook Sidecar"></a>Hook Sidecar</h2><p>先来看一种在 KubeVirt 中已经存在的扩展机制 <a href="https://kubevirt.io/user-guide/user_workloads/hook-sidecar/">Hook Sidecar</a>。</p><p>这套机制是在 VM 正式创建前，可以加载一个用户自定义的镜像，或者一段 ConfigMap 里保存的 Shell 或者 Python 脚本，来修改 VM 启动前 libvirt 的启动参数和 cloud-init 参数。</p><p>它的执行机制和 CNI 有些类似，virt-handler 在启动 VM 前会去对应目录寻找 <code>/usr/bin/onDefineDomain</code> 和 <code>/usr/bin/preCloudInitIso</code> 两个二进制文件，前者传入 virt-handler 生成的 libvirt XML 配置，返回修改后的配置；后者传入 cloudInit 配置，返回修改后的 cloudInit 配置。这样的话所有 KubeVirt 本身不支持的 libvirt 和 cloudInit 参数都可以通过这种机制来注入修改。并且由于 Sidecar 内实际可以执行任意代码，所能做的事情远不止修改这两个配置，所有初始化阶段 KubeVirt 没有实现的能力其实都可以在这里来实现。</p><h2 id="Network-Binding-Plugin"><a href="#Network-Binding-Plugin" class="headerlink" title="Network Binding Plugin"></a>Network Binding Plugin</h2><p>现在可以到 Network Binding Plugin 这个机制了，这个机制其实和 Hook Sidecar 基本上大同小异。主要区别是将二进制调用改成了 gRPC 调用，gRPC 里注册的方法还是  <code>onDefineDomain</code> 和 <code>preCloudInitIso</code> 参数传递从命令行参数改为了 gRPC Request 里的参数，其他都是一样的。</p><p>具体的例子可以参考目前还在 KubeVirt 代码里的 <a href="https://github.com/kubevirt/kubevirt/tree/main/cmd/sidecars/network-slirp-binding">Slirp Binding</a> 的实现。尽管在 Network Binding Plugin 的规范里还增加了<code>networkAttachmentDefinition</code> 字段可以选择一个 CNI，但这个其实使用之前的网卡选择机制也能实现，甚至由于 Sidecar 里可以执行任意代码，在里面再实现一个 CNI 覆盖 Pod 原先的网络也是可以的。</p><p>那么之后的网络架构就变成了下图这样：</p><p><img src="/../images/networking-binding.png" alt="alt text"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>虽然 Network Binding Plugin 的机制是为 Network 扩展准备的，但实际上几乎可以扩展所有 KubeVirt 在 virt-handler 侧的处理逻辑。甚至可以把 KubeVirt 也只当一个框架，所有的逻辑都通过 Sidecar 来处理，相信未来可以玩出不少花活来。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://kubevirt.io/user-guide/user_workloads/hook-sidecar/">https://kubevirt.io/user-guide/user_workloads/hook-sidecar/</a></li><li><a href="https://kubevirt.io/user-guide/network/network_binding_plugins/">https://kubevirt.io/user-guide/network/network_binding_plugins/</a></li><li><a href="https://github.com/kubevirt/kubevirt/blob/main/docs/network/network-binding-plugin.md">https://github.com/kubevirt/kubevirt/blob/main/docs/network/network-binding-plugin.md</a></li><li><a href="https://github.com/kubevirt/kubevirt/tree/main/cmd/sidecars/network-slirp-binding">https://github.com/kubevirt/kubevirt/tree/main/cmd/sidecars/network-slirp-binding</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在 KubeVirt v1.4 的新版本里将 &lt;a href=&quot;https://kubevirt.io/user-guide/network/network_binding_plugins/&quot;&gt;Network Binding Plugin&lt;/a&gt; 提升到了 Beta，提供了</summary>
      
    
    
    
    
    <category term="kubevirt" scheme="http://oilbeater.com/tags/kubevirt/"/>
    
    <category term="networking" scheme="http://oilbeater.com/tags/networking/"/>
    
  </entry>
  
  <entry>
    <title>加速容器镜像下载：从缓存到按需加载</title>
    <link href="http://oilbeater.com/2024/10/31/docker-pull/"/>
    <id>http://oilbeater.com/2024/10/31/docker-pull/</id>
    <published>2024-10-31T11:59:25.000Z</published>
    <updated>2025-04-14T17:16:50.278Z</updated>
    
    <content type="html"><![CDATA[<p>在容器的启动过程中，镜像下载速度往往是影响启动速度的最主要因素，通常占据了启动时间的 70% 以上。特别是对于体积庞大的 VM、AI 镜像，它们的大小可能达到数十 GB，导致下载和解压速度都成为启动的瓶颈。本文将探讨镜像下载的主要瓶颈、常见的优化方案以及最新的按需加载技术，以加速容器启动。</p><h3 id="镜像下载速度慢的原因"><a href="#镜像下载速度慢的原因" class="headerlink" title="镜像下载速度慢的原因"></a>镜像下载速度慢的原因</h3><p>容器镜像下载慢的原因主要有以下几点：</p><ul><li><strong>镜像体积过大</strong>：VM、AI 镜像体积通常较大，可能达到数十 GB，使得下载时间显著。</li><li><strong>gzip 解压耗时</strong>：特别是在内网环境中，解压时间往往远高于网络传输时间，导致解压成为新的瓶颈。</li></ul><h3 id="常见的镜像优化思路"><a href="#常见的镜像优化思路" class="headerlink" title="常见的镜像优化思路"></a>常见的镜像优化思路</h3><p>为了解决下载和解压的速度问题，业界提出了多种优化方案：</p><ol><li><p><strong>镜像缓存</strong><br>镜像缓存是提升镜像下载速度的一种方法，通过缓存镜像可以避免重复下载。然而，缓存无法解决冷启动问题，并且镜像频繁变更（如应用更新或安全更新）会导致缓存失效。要实现高效的缓存管理，还需要较复杂的机制来管理缓存更新。</p></li><li><p><strong>减小镜像体积</strong><br>减少镜像体积也有助于缩短下载时间，但在某些场景下，例如 VM、AI、CUDA 镜像，体积优化空间有限。它们通常需要使用超过 7 GB 的存储空间，难以进一步缩减。</p></li></ol><h3 id="按需加载：是否可行？"><a href="#按需加载：是否可行？" class="headerlink" title="按需加载：是否可行？"></a>按需加载：是否可行？</h3><p>目前，大多数容器在启动时并不需要完整的镜像内容。一些论文表明，启动期间仅需 6.4% 的镜像内容，因此理论上可以通过按需下载来优化启动速度。然而，现有的镜像格式存在以下问题，限制了按需下载的实现：</p><ul><li><strong>OverlayFS 的限制</strong>：需要所有镜像层下载完毕后才能得知最终文件结构。</li><li><strong>gzip 不支持随机访问</strong>：即使只需下载单个文件，也要下载并解压整个层。</li><li><strong>校验问题</strong>：镜像 digest 是按整个层计算的，无法针对单个文件校验。</li></ul><h3 id="eStargz：实现按需加载"><a href="#eStargz：实现按需加载" class="headerlink" title="eStargz：实现按需加载"></a>eStargz：实现按需加载</h3><p>为了解决上述问题，eStargz 提出了针对 gzip 层的优化方案，即每个文件单独压缩并增加文件级别索引。eStargz 引入了如下优化：</p><ol><li><strong>独立压缩</strong>：每个文件单独压缩并索引，解决了 gzip 无法随机访问的问题。</li><li><strong>文件校验</strong>：可以对单个文件进行校验，无需校验整个层。</li></ol><p>具体的存储格式如下图：</p><p><img src="/../images/estartgz.png" alt="alt text"></p><p>每个文件被单独压缩合并成一个大的 blob，在 blob 最后增加一个 TOC 的描述文件记录每个文件的偏移量和校验值，这样就实现了按文件的索引和校验。</p><p>以下是 eStargz 的 TOC 格式示例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;entries&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bin/&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;dir&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;modtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-08-20T10:30:43Z&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;mode&quot;</span><span class="punctuation">:</span> <span class="number">16877</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;NumLink&quot;</span><span class="punctuation">:</span> <span class="number">0</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bin/busybox&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;reg&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;size&quot;</span><span class="punctuation">:</span> <span class="number">833104</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;modtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-06-12T17:52:45Z&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;mode&quot;</span><span class="punctuation">:</span> <span class="number">33261</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;offset&quot;</span><span class="punctuation">:</span> <span class="number">126</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;NumLink&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;digest&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sha256:8b7c559b8cccca0d30d01bc4b5dc944766208a53d18a03aa8afe97252207521f&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;chunkDigest&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sha256:8b7c559b8cccca0d30d01bc4b5dc944766208a53d18a03aa8afe97252207521f&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>通过这种改进，eStargz 实现了对单个文件的按需加载，且可以实现文件级别校验。</p><h3 id="性能权衡：优先级加载"><a href="#性能权衡：优先级加载" class="headerlink" title="性能权衡：优先级加载"></a>性能权衡：优先级加载</h3><p>虽然按需加载大幅优化了下载性能，但也可能带来运行时性能的下降。为此，eStargz 采用特殊标识来实现优先级加载，将启动所需文件放置于 <code>prioritized zone</code> 中，确保这些文件优先下载，进而提升运行时性能。</p><p><img src="/../images/estargz-optimize.png" alt="alt text"></p><p>按照作者测试的性能表现如下：</p><p><img src="/../images/estargz-perf.png" alt="alt text"></p><h3 id="代价与挑战"><a href="#代价与挑战" class="headerlink" title="代价与挑战"></a>代价与挑战</h3><p>尽管 eStargz 带来了按需加载的性能提升，但也带来了以下代价：</p><ul><li><strong>存储空间增加</strong>：每个文件单独压缩会增加额外的 metadata，降低压缩率。</li><li><strong>额外插件支持</strong>：eStargz 需要插件支持，例如在容器镜像推送和拉取时需要特定处理插件。</li></ul><h3 id="如何使用-eStargz"><a href="#如何使用-eStargz" class="headerlink" title="如何使用 eStargz"></a>如何使用 eStargz</h3><p>以下是 eStargz 的使用方法，适用于 containerd 的子项目以及一些支持 eStargz 的工具：</p><ol><li><p><strong>Docker, kaniko, nerdctl 命令行参数</strong>：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker buildx build -t ghcr.io/ktock/hello:esgz \</span><br><span class="line">    -o <span class="built_in">type</span>=registry,oci-mediatypes=<span class="literal">true</span>,compression=estargz,force-compression=<span class="literal">true</span> \</span><br><span class="line">    /tmp/buildctx/</span><br><span class="line"></span><br><span class="line">nerdctl image convert --estargz --oci ghcr.io/ktock/hello:1 ghcr.io/ktock/hello:esgz</span><br></pre></td></tr></table></figure></li><li><p><strong>containerd 插件配置</strong>：</p> <figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version</span> = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="section">[proxy_plugins]</span></span><br><span class="line">  <span class="section">[proxy_plugins.stargz]</span></span><br><span class="line">    <span class="attr">type</span> = <span class="string">&quot;snapshot&quot;</span></span><br><span class="line">    <span class="attr">address</span> = <span class="string">&quot;/run/containerd-stargz-grpc/containerd-stargz-grpc.sock&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd]</span></span><br><span class="line">  <span class="attr">snapshotter</span> = <span class="string">&quot;stargz&quot;</span></span><br><span class="line">  <span class="attr">disable_snapshot_annotations</span> = <span class="literal">false</span></span><br></pre></td></tr></table></figure></li></ol><p>此外，GKE 等云平台的集群已默认启用类似方案，进一步加速了镜像启动速度。看阿里也发表了基于 block device 的按需加载，这类的实现看上去在云厂商都有了比较大规模的落地。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>从传统的镜像缓存、镜像体积优化，到按需加载，eStargz 提供了一种兼顾性能和灵活性的方案，使得容器可以在仅下载部分内容的情况下启动。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="https://github.com/containerd/stargz-snapshotter/blob/main/docs/estargz.md">eStargz: Standard-Compatible Extension to Container Image Layers for Lazy Pulling</a></li><li><a href="https://medium.com/nttlabs/startup-containers-in-lightning-speed-with-lazy-image-distribution-on-containerd-243d94522361">Startup Containers in Lightning Speed with Lazy Image Distribution on Containerd</a></li><li><a href="https://www.usenix.org/conference/fast16/technical-sessions/presentation/harter">Slacker: Fast Distribution with Lazy Docker Containers</a></li><li><a href="https://github.com/containerd/accelerated-container-image">Accelerated Container Image</a></li><li><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/image-streaming">Use Image streaming to pull container images</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在容器的启动过程中，镜像下载速度往往是影响启动速度的最主要因素，通常占据了启动时间的 70% 以上。特别是对于体积庞大的 VM、AI 镜像，它们的大小可能达到数十 GB，导致下载和解压速度都成为启动的瓶颈。本文将探讨镜像下载的主要瓶颈、常见的优化方案以及最新的按需加载技术，</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>AI Gateway 调研之 Kong, Gloo 和 Higress</title>
    <link href="http://oilbeater.com/2024/08/26/kong-gloo-higress/"/>
    <id>http://oilbeater.com/2024/08/26/kong-gloo-higress/</id>
    <published>2024-08-26T07:39:23.000Z</published>
    <updated>2025-04-14T17:16:50.278Z</updated>
    
    <content type="html"><![CDATA[<p>上篇<a href="2024-08-25-ai-gateway-cloudflare">博客</a>介绍了 Cloudflare AI Gateway，这篇集中介绍一下 Kong, Gloo 和 Higress 因为这三者有一定的相似性，都是从原有的 API 网关基础上进行扩展，通过插件的方式支持了一系列 AI 相关的功能，在交付上也是传统的软件部署方式。这几个算是传统 API Gateway 迎接 AI 浪潮的代表，其中 Higress 更是把产品 Slogan 直接从 Cloud Native API Gateway 变成了 AI Gateway，虽然打不过就加入，但这样变来变去不怕人说没根么：）</p><p>由于这三款产品都需要额外的部署开通，有的 AI 功能还是商业版才有，所以下面的分析都是根据看文档总结而来，可能存在着和实际不符的情况。</p><table><thead><tr><th>功能</th><th>Kong</th><th>Gloo</th><th>Higress</th><th>备注</th></tr></thead><tbody><tr><td>技术栈</td><td>Nginx + Lua</td><td>Envoy + Go</td><td>Envoy + WASM</td><td>虽然几家都提供了插件机制，但是和网关的耦合程度都比价高，非网关的开发者上手还是有一定难度</td></tr><tr><td>日志监控</td><td>每个 AI 插件会将元信息，如模型名、Token 开销费用等信息加入到 Audit Log 中，但是似乎没有自定义元信息的功能，需要通过其他插件来辅助完成</td><td>似乎没有在 AI 这块对日志有功能增强，还是通用的监控</td><td>日志和监控中增加了 Token 的用量，提供的信息和 Kong 类似，也不具备自定义元信息的功能</td><td>如果能增加一些自定义元信息，并支持记录 Request 和 Response 里 Message 信息就更好了</td></tr><tr><td>Proxy</td><td>Kong 提供了归一化的 API 能够用一套统一的 API 去调用不同的 LLM API，这对开发者还是比较友好的能够不需要大改应用代码就能用不同的 LLM</td><td>Gloo 没有提供归一化的 API，只是反向代理到上游 LLM API</td><td>Higress 支持将不同的 LLM API 统一转换成 OpenAI API，这对开发者来说也比较友好，毕竟目前生态里还是直接用 OpenAI API 的比较多</td><td>虽然我觉得是否提供归一化的 API 没那么重要，不过一定要归一化的话归一化成 OpenAI 格式的会好些</td></tr><tr><td>API Key 管理</td><td>客户端的 Key 可以和上游的 Key 不一样，相当于把 Key 在网关层做了一层屏蔽</td><td>客户端的 Key 可以和上游的 Key 不一样，相当于把 Key 在网关层做了一层屏蔽</td><td>直接从客户端透传 Key 给上游</td><td>个人感觉 Gloo 这个功能还比较实用，避免了在 LLM 那里真实的 Key 被过多业务方知道，安全和可控性会更好一些</td></tr><tr><td>Cache</td><td>当前版本没有提供 LLM Cache 相关能力，据说会在 3.8 版本提供</td><td>提供了语义 Cache，看配置是调用了 OpenAI 的 Embedding 和 Redis 的 Vector，不过没看到更细粒度的比如 TTL 相似度的配置</td><td>提供的还是文本匹配的缓存，相比全文本可以通过 JSON PATH 的语法选择部分 Message 做缓存，看配置也是利用 Redis，不过不支持语义 Cache</td><td>Gloo 提供的语义 Cache 看起来更高级一些</td></tr><tr><td>请求&#x2F;响应改写</td><td>可以在 Request 和 Response 阶段分别加 prompt 对 message 进行改写，相当于一个小型的 workflow</td><td>只提供了 prepend system prompt 的能力，感觉提升有限</td><td>和 Kong 类似提供了用 prompt 进行改写的能力，不过现在只支持通义千问的 LLM 感觉不够开放</td><td></td></tr><tr><td>RAG</td><td>目前没有相关功能的插件</td><td>可以对接一个 postgres 和 OpenAI 的 embedding Token 这样可以自己提供一些文本来做 RAG</td><td>和 Gloo 的功能类似，不过只支持阿里云的向量服务和通义千问，还是感觉不够开放</td><td>感觉 RAG 的配置参数都比较少没有相似度，或者爬取网页的接口，只能做比较简单的 RAG</td></tr></tbody></table><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这三款产品由于只是看文档没有真实使用过，里面的内容很可能有不准确的地方，希望了解的同学可以指正。</p><p>总体来看三款产品都是 LLM 爆发前就存在的，之前也不是专门为 AI 场景设计的，很多使用的配置可能懂 Kubernetes 的更能看懂。尤其是 Gloo 的文档全是 YAML 和 CRD 的配置，浓浓的 Cloud Native 味道，Higress 看上去也是各种 ConfigMap 脱离 Kubernetes 是否还能用好我是心里存在疑虑的。</p><p>Higress 虽然没根了，但是整体看 AI 功能做的还是最完整的，发力也比较明显。Kong 感觉还只是试探性的做了些功能，而 Gloo 是把所有 AI 相关功能都放到商业版里了。如果 Higress 能把开发性做好不是被通义千问和阿里云上各种服务绑定的话我觉得还是个不错的项目。</p><p>最后的依赖还是这三款产品的扩展性可能都存在一定难度，需要高度了解网关相关的逻辑并掌握 Lua 或者 WASM 这样非主流的语言。而 AI 应用现在的形态其实还存在很多变化的可能，对应的 API 和需要的通用能力可能也有比较大的变化，比如怎么做 RAG，怎么做 Cache，怎么编排 LLM 都没有确定下来。不知道现在的架构会不会对他们未来的功能灵活变化产生影响。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;上篇&lt;a href=&quot;2024-08-25-ai-gateway-cloudflare&quot;&gt;博客&lt;/a&gt;介绍了 Cloudflare AI Gateway，这篇集中介绍一下 Kong, Gloo 和 Higress 因为这三者有一定的相似性，都是从原有的 API 网关基础上进</summary>
      
    
    
    
    
    <category term="AI" scheme="http://oilbeater.com/tags/AI/"/>
    
    <category term="Gateway" scheme="http://oilbeater.com/tags/Gateway/"/>
    
    <category term="Kong" scheme="http://oilbeater.com/tags/Kong/"/>
    
    <category term="Gloo" scheme="http://oilbeater.com/tags/Gloo/"/>
    
    <category term="Higress" scheme="http://oilbeater.com/tags/Higress/"/>
    
  </entry>
  
  <entry>
    <title>AI Gateway 调研之 Cloudflare AI Gateway</title>
    <link href="http://oilbeater.com/2024/08/25/ai-gateway-cloudflare/"/>
    <id>http://oilbeater.com/2024/08/25/ai-gateway-cloudflare/</id>
    <published>2024-08-25T14:10:45.000Z</published>
    <updated>2025-04-14T17:16:50.278Z</updated>
    
    <content type="html"><![CDATA[<p>随着 AI 的火热，眼看着之前调研的各家竞品 API 网关产品纷纷把自己的介绍改为 AI Gateway，于是就想调研一下这些所谓的 AI Gateway 究竟做了些啥。这次调研的对象有一些之前靠 API 网管或者云原生 Ingress Controller 起家加入 AI 功能的，例如：<a href="https://konghq.com/products/kong-ai-gateway">Kong</a>，<a href="https://www.solo.io/products/gloo-ai-gateway/">Gloo</a> 和 <a href="https://higress.io/en/">Higress</a>。也包括一些第一天就是借着 AI 起来的我认为真正 AI 原生的网关，例如 <a href="https://portkey.ai/features/ai-gateway">Portkey</a> 和 <a href="https://github.com/songquanpeng/one-api">OneAPI</a>。以及这篇博客介绍的基于公有云 Serverless 的 <a href="https://developers.cloudflare.com/ai-gateway/">Cloudflare AI Gateway</a>。</p><p>大体来看目前的 AI Gateway 主要能力在三个方面：</p><p><strong>常规 API 网关功能在 AI API 上的应用</strong>，例如：监控，日志，限速，反向代理，请求或响应改写，集成用户系统等。这些功能其实和 AI 关系不大就是把 LLM 的 API 当成了一个普通的 API 进行接入。</p><p><strong>部分 API 网关功能针对 AI 进行优化</strong>，例如限速功能增加基于 Token 的限速，缓存功能增加基于 Prompt 的缓存，防火墙基于 prompt 和 LLM 返回进行过滤，多个 LLM API Key 之间的负载均衡，多个 LLM Provider 的 API 转换。这些功能在原有的 API 网关就存在类似的概念，不过在 AI 场景下又有了相应的扩展。</p><p><strong>基于 AI 应用的场景增加的新功能</strong>，例如部分 AI 网关增加了 Embedding 和 RAG 的功能，把向量数据库和文本数据库的功能通过 API 的形式提供出来。还有一些针对 token 用量的性能优化，比如 Prompt 简化，语义化 Cache 等。还有一些更偏应用层的功能，例如对 LLM Output 提供打分功能等。</p><p>这篇博客介绍 <a href="https://developers.cloudflare.com/ai-gateway/">Cloudflare AI Gateway</a> 这款 AI Gateway 的特点。</p><h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><p>Cloudflare 的这款 AI Gateway 主要功能其实就是一个反向代理，看完了我甚至觉得我用 Cloudflare Worker 捣鼓一阵也能做个功能类似的。如果你原来用的是 OpenAI 的 API 那么现在你要做的就是把 SDK 里的 baseURL 换成 <code>https://gateway.ai.cloudflare.com/v1/$&#123;accountId&#125;/$&#123;gatewayId&#125;/openai</code> 就可以了。在这个过程中由于流量进出都是过 Cloudflare 的，Cloudflare 平台上就可以提供对应的监控，日志，缓存等功能。</p><p>这个方案有下面几个优点：</p><ul><li>接入很简单，改一下 baseURL 就接入进来了，API 格式也没有任何变化。并且完全是 Serverless 的，不需要自己额外管理任何服务器，这个功能现在是免费的，直接就白嫖了监控数据。</li><li>借助 Cloudflare 的全球网络可以实现一定的用户接入加速，不过这个用户接入的加速相比 LLM 本身的延迟比重应该很小，顶多在首个 Token 的延迟会有明显变化。</li><li>通用借助 Cloudflare 的全球网络可以一定程度隐藏掉源 IP，对于一些 OpenAI API 访问受限的区域用这个可以绕过去。</li></ul><p>但对应的也有下面的缺点：</p><ul><li>所有请求信息包括 API Key 都要在 Cloudflare 上过一道，会有安全方面的一些隐患。</li><li>Gateway 本身没有什么插件机制，想扩展功能的话会比较麻烦，只能在外面再套一层。</li><li>同样是因为 Cloudflare 的全球网路欧，如果一个 Key 一直变换 IP 地址访问，不知道会不会触发 OpenAI 那边的拉黑。</li></ul><h1 id="主要能力"><a href="#主要能力" class="headerlink" title="主要能力"></a>主要能力</h1><h2 id="多个-Provider-支持"><a href="#多个-Provider-支持" class="headerlink" title="多个 Provider 支持"></a>多个 Provider 支持</h2><p>由于 Cloudflare AI Gateway 并没有对 LLM API 进行修改，只是做反向代理，所以几乎主流的 LLM API 它都可以支持，只需要把 baseURL 改成对应 Provider 如 <code>https://gateway.ai.cloudflare.com/v1/$&#123;accountId&#125;/$&#123;gatewayId&#125;/&#123;provider&#125;</code> 即可。</p><p>它唯一多提供的一个 API 叫做 <a href="https://developers.cloudflare.com/ai-gateway/providers/universal/">Universal Endpoint</a> 可以做简单的 fallback。用法是在一个 API 请求里可以填写多个 Provider 的<br>query，这样当前面的 Provider 请求失败时会自动调用下一个 Provider。</p><h2 id="可观测"><a href="#可观测" class="headerlink" title="可观测"></a>可观测</h2><p>监控层面除了基础的 QPS 和 Error Rate 这些监控面板，还针对 LLM 的场景提供了 Token，Cost 以及 Cache 命中率的面板。</p><p>日志方面和 Worker 的日志很类似，只有实时日志无法查询历史日志。这里感觉做的不太好，Worker 至少还有第三方的方案能保存日志，但是 Gateway 这里却没有了。虽然通过一些实时日志 API 再自己保存的方式也可以，但还是太麻烦了。分析 LLM 请求和响应日志应该是很多 AI 应用后续做优化甚至 fine-tuning 的一个重要环节，这里没有直接集成持久化的方案其实是个硬伤。</p><h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><p>缓存方面，Cloudflare 提供的还是基于文本内容完全匹配的缓存，目测是通过 <a href="https://developers.cloudflare.com/kv/">Cloudflare Workers KV</a> 来实现的。也可以通过 <code>cf-aig-cache-key</code> 来实现自定义 Cache Key，包括设置缓存的 TTL 以及忽略 Cache。但是整体看起来基于现在的功能是无法实现语义缓存的，官方文档的说法是语义缓存会在未来提供。</p><h2 id="Rate-Limiting"><a href="#Rate-Limiting" class="headerlink" title="Rate Limiting"></a>Rate Limiting</h2><p>限速方面，Cloudflare 提供的还是传统的基于 QPS 的限速，这块并没有基于 AI 的场景提供基于 Token 的限速，这里未来还有改善的空间。</p><h2 id="Custom-metadata"><a href="#Custom-metadata" class="headerlink" title="Custom metadata"></a>Custom metadata</h2><p>可以在请求的 Header 中增加一些自定义字段，比如用户信息。这些信息可以通过日志进行检索。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>整体来看 Cloudflare AI Gateway 胜在简单易用，对于之前没有使用 AI Gateway 的用户可以两三分钟就接进来，提供了基础的监控和缓存能力。而且 Cloudflare 还有一些其他配套的 AI 服务例如 Works AI 提供了大量的开源模型的 Serving 和 Worker 提供边缘计算，几个一结合就能搭一套完全 Serverless 的 AI 系统。</p><p>他的问题主要在于更深入的功能提供的比较少，而且功能扩展比较麻烦，只能在外围通过 Worker 再来包一层。与其这样 Cloudflare 还不如直接把 AI Gateway 开源出来变成一个模板，用户可以根据自己需求去更改代码或者写插件，没准还能形成一个新的生态。毕竟我高度怀疑现在的 AI Gateway 其实就是个 Worker 模板。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;随着 AI 的火热，眼看着之前调研的各家竞品 API 网关产品纷纷把自己的介绍改为 AI Gateway，于是就想调研一下这些所谓的 AI Gateway 究竟做了些啥。这次调研的对象有一些之前靠 API 网管或者云原生 Ingress Controller 起家加入 AI</summary>
      
    
    
    
    
    <category term="AI" scheme="http://oilbeater.com/tags/AI/"/>
    
    <category term="Gateway" scheme="http://oilbeater.com/tags/Gateway/"/>
    
    <category term="Cloudflare" scheme="http://oilbeater.com/tags/Cloudflare/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 多集群通信的五种方案</title>
    <link href="http://oilbeater.com/2024/05/24/5-way-kubernetes-multcluster-communication/"/>
    <id>http://oilbeater.com/2024/05/24/5-way-kubernetes-multcluster-communication/</id>
    <published>2024-05-24T08:25:30.000Z</published>
    <updated>2025-04-14T17:16:50.277Z</updated>
    
    <content type="html"><![CDATA[<p>随着企业的业务规模不断扩大，Kubernetes 的使用也从单集群逐步扩展到多集群部署。多集群环境下，集群之间的通信成为一个重要的研究课题。本文将介绍五种跨 Kubernetes 集群通信的方案的基本原理，优点及其局限性。</p><h2 id="1-Underlay-网络"><a href="#1-Underlay-网络" class="headerlink" title="1. Underlay 网络"></a>1. Underlay 网络</h2><p>这类网络插件包括 <a href="https://www.cni.dev/plugins/current/main/macvlan/">macvlan</a>&#x2F;<a href="https://www.cni.dev/plugins/current/main/ipvlan/">ipvlan</a>&#x2F;<a href="https://kubeovn.github.io/docs/stable/start/underlay/">Kube-OVN underlay</a> 以及各种云上的 VPC CNI 等。</p><p><strong>基本原理</strong>：</p><p>Underlay 网络从 CNI（容器网络接口）的角度来看是最简单的方式。这种方式依赖于底层基础设施在网络层面实现打通。例如，使用公有云上的 VPC Peering 或者物理网络配置路由和大二层。当底层网络实现了连通，跨集群的容器网络就自然连通了。</p><p><strong>优点</strong>：</p><ul><li>CNI 角度最为简单，无需额外操作。</li><li>架构上清晰，把跨集群通信的责任交给底层网络。</li></ul><p><strong>局限性</strong>：</p><ul><li>依赖于特定 CNI，Underlay 类型的网络场景使用范围有限，有些情况只能使用 Overlay 网络。</li><li>在异构的环境，比如多云之间、公有云和私有云之间打通存在困难。</li><li>只是做了基础的容器网络通信打通，更上层的服务发现，域名和网络策略等功能存在缺失。</li><li>一次性打通所有集群的容器网络，缺乏细粒度的控制。</li></ul><h2 id="2-提供跨集群通信能力的-Overlay-CNI"><a href="#2-提供跨集群通信能力的-Overlay-CNI" class="headerlink" title="2. 提供跨集群通信能力的 Overlay CNI"></a>2. 提供跨集群通信能力的 Overlay CNI</h2><p>在无法使用 Underlay 网络的情况下，一些特定的 CNI 在 Overlay 层面也实现了跨集群。例如 <a href="https://cilium.io/use-cases/cluster-mesh/">Cilium Cluster Mesh</a>, <a href="https://antrea.io/docs/v2.0.0/docs/multicluster/quick-start/">Antrea Multi-Cluster</a> 和 <a href="https://kubeovn.github.io/docs/stable/en/advance/with-ovn-ic/">Kube-OVN with ovn-ic</a>。</p><p><strong>基本原理</strong>：</p><p>这些 CNI 的实现方案其实大体一致，通过选择一组集群内的节点作为网关节点，然后网关节点之间建立隧道，跨集群流量通过网关节点转发。</p><p><strong>优点</strong>：</p><ul><li>CNI 自包含跨集群功能，无需额外组件支持。</li></ul><p><strong>局限性</strong>：</p><ul><li>依赖特定 CNI，无法实现不同 CNI 集群之间的通信。</li><li>无法处理 CIDR 重叠的情况，需要提前规划好网段。</li><li>除了 Cilium 实现的比较完整，把跨集群的服务发现和网络策略都实现了，其他的仅实现基础容器网络通信。</li><li>一次性打通所有集群的容器网络，缺乏细粒度的控制。</li></ul><h2 id="3-Submariner"><a href="#3-Submariner" class="headerlink" title="3. Submariner"></a>3. Submariner</h2><p>由于跨集群网络互通存在着通用的需求，在实现上也存在着类似，各个 CNI 其实存在着一些重复造轮子的工作。<a href="https://submariner.io/">Submariner</a> 作为一款 CNI 无关的跨集群网络插件提供了一种通用的能力，能将不同 CNI 的集群组成一个网络达到互通。Submariner 最早由 Rancher 的工程师创建，现在已经是 CNCF Sandbox 项目了，目前看红帽的工程师也在积极参与这个项目。</p><p><strong>基本原理</strong>：</p><p>Submariner 选择集群内的网关节点，网关节点通过 VXLAN 通信。跨集群流量通过 VXLAN 传输。Submariner 依赖 CNI 将 egress 流量先发送到宿主机的网络内，然后再进行转发。此外，Submariner 部署了一组 CoreDNS 实现跨集群服务发现，并使用 Globalnet Controller 解决 CIDR 重叠问题。</p><p><strong>优点</strong>：</p><ul><li>一定程度上 CNI 无关，可以连接不同 CNI 的集群。</li><li>实现了跨集群服务发现，支持 Service 和域名解析。</li><li>支持 CIDR 重叠的集群通信，避免了集群部署后想互联却发现 IP 冲突的尴尬。</li></ul><p><strong>局限性</strong>：</p><ul><li>并不是所有的 CNI 都可用，如果像 macvlan 或者 cilium 短路这种宿主机看不到流量的情况，就没有办法拦截流量到自己的隧道了。</li><li>Gateway 目前是主备模式，没法横向负载均衡，在大流量的场景下可能存在性能瓶颈。</li><li>一次性打通所有集群的容器网络，缺乏细粒度的控制。</li></ul><h2 id="4-Skupper"><a href="#4-Skupper" class="headerlink" title="4. Skupper"></a>4. Skupper</h2><p><a href="https://skupper.io/index.html">Skupper</a> 是我认为几个方案里最有意思的，它能够按需进行 Service 层面的网络打通，避免了一次完全打通的控制问题。而且很创新的使用了七层消息队列的方式来实现，可以说是对底层网络和 CNI 完全无依赖，上手也十分简单。目前 Skupper 看贡献者主要是红帽的工程师。</p><p><strong>基本原理</strong>：</p><p>和上述几个方案使用隧道将容器 IP 直接打通不同，Skupper 提出了一个 VAN（Virtual Application Networks）的概念，要在 7 层将网络打通。简单说就是不把 IP 直接打通而是把 Service 拉通，概念上和 ServiceExporter，ServiceImporter 类似，但是这个项目启动的比较早，当时还没有这些社区提出来的概念，在当时应该算个很创新的想法了</p><p>在实现上 Skupper 也另辟蹊径，用的是个消息队列的实现，多个集群之间组成了一个大的消息队列，跨集群通信的数据包发送到这个消息队列里。另一端再去消费这个数据包。思路上其实类似反向代理，但用消息队列来实现也是个很开脑洞的想法。把 Service 变成了一个消息的订阅点来提供消费，这样就可以按需的在服务端和客户端之间建立一个消息队列，通过消息队列的概念去管理和控制这个消息通路。</p><p><strong>优点</strong>：</p><ul><li>CNI 兼容性好，完全不依赖 CNI 的行为在应用层进行数据包的打通</li><li>上手简单，不需要太复杂的前期网络规划，也没有 CIDR 不重叠要求。提供了 CLI 方便临时测试和快速演示</li><li>并不是容器网络互通而是按需将 Service 打通，可以做到细粒度的控制，对底层要求也更低</li></ul><p><strong>局限性</strong>：</p><ul><li>文档介绍目前只支持 TCP 协议，UDP 和更底层的协议比如 ICMP 会有问题</li><li>由于是通过消息队列转发消息，IP 信息会丢失</li><li>通过消息队列转发的思路还是有些奇怪，在性能方面比如延迟和吞吐量上可能会有一些损失</li><li>而且 TCP 本身是有状态的，能否完全转换成消息队列的形式，兼容性如何是个疑问</li></ul><h2 id="5-KubeSlice"><a href="#5-KubeSlice" class="headerlink" title="5. KubeSlice"></a>5. KubeSlice</h2><p><a href="https://kubeslice.io/documentation/open-source/1.3.0">KubeSlice</a> 是刚刚进入 CNCF Sandbox 的一个项目。上述的方案基于隧道做的没法做到细粒度控制和 CNI 完全兼容，基于应用层做的又没法做到网络协议完全兼容。KubeSlice 提供了一个新的思路，尝试同时解决这两个问题。</p><p><strong>基本原理</strong>：</p><p>KubeSlice 最底层的思路十分简单粗暴，就是按需给 Pod 动态插入一块网卡，在这个网卡上面做跨集群的 Overlay，然后再在这个 Overlay 网络上面实现服务发现，和网络策略。用户可以按需的将跨集群的几个 Namespace 或者 某几个 Pod 组成一个网络，在可以使实现灵活的细粒度管控基础上由于走的是基于网卡的二层网络，也实现了网络协议的最大兼容性。</p><p><strong>优点</strong>：</p><ul><li>CNI 兼容性好，由于是额外插入了一块网卡完全不关心原先的 CNI 是什么，并且是额外的网络也不用担心原先网络地址是否有冲突的问题</li><li>网络协议兼容性好，同样是由于额外有了一块网卡做流量转发，所有的网络协议都是兼容的。</li><li>灵活度高，KubeSlice 提供了命令行工具可以动态的创建和加入网络，用户可以选择按应用的需求创建多个跨集群的虚拟网络</li><li>功能完善，按照文档的说法，该项目在额外的这个 Overlay 网络上实现了服务发现，DNS，QoS，Networkpolicy 和监控，可以说把各个方面都覆盖到了</li></ul><p><strong>局限性</strong>：</p><ul><li>由于本质上是双网卡，应用侧需要感知这个事情选择合适的网络，可能会涉及到应用改造</li><li>由于跨集群走的是另一个网卡不是 Pod 主网卡的 IP，对监控和追踪等外部系统可能涉及到改造</li><li>目前看文档应该是一个内部的项目进行开源，文档里很多使用方式和 API 介绍的不是特别清楚，需要对着 reference 慢慢猜每个参数到底是什么意思。功能上看上去确实比较全，但能让外部用户很好的使用的话文档还有很多需要完善的地方。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在 Kubernetes 多集群环境中实现高效通信有多种方案可供选择。每种方案都有其优点和局限性，用户可以根据具体需求和环境选择合适的方案。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;随着企业的业务规模不断扩大，Kubernetes 的使用也从单集群逐步扩展到多集群部署。多集群环境下，集群之间的通信成为一个重要的研究课题。本文将介绍五种跨 Kubernetes 集群通信的方案的基本原理，优点及其局限性。&lt;/p&gt;
&lt;h2 id=&quot;1-Underlay-网络</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/tags/kubernetes/"/>
    
    <category term="network" scheme="http://oilbeater.com/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>k8gb: 云原生最佳开源 GSLB 方案</title>
    <link href="http://oilbeater.com/2024/04/18/k8gb-best-cloudnative-gslb/"/>
    <id>http://oilbeater.com/2024/04/18/k8gb-best-cloudnative-gslb/</id>
    <published>2024-04-18T10:01:10.000Z</published>
    <updated>2025-04-14T17:16:50.277Z</updated>
    
    <content type="html"><![CDATA[<p>如何将流量在多个 Kubernetes 集群之间进行负载均衡，并做到自动的灾备切换一直是一个让人头疼的问题。我们之前调研了公有云，<a href="https://github.com/karmada-io/multi-cluster-ingress-nginx">Karmada Ingress</a>，自己也做了一些手动 DNS 的方案，但这些方案在成本，通用性，灵活度以及自动化程度上都有所欠缺。直到调研到了 <a href="https://www.k8gb.io/">k8gb</a> 这个由南非银行 <a href="https://www.absa.africa/">Absa Group</a> 为了做金融级别的多活而启动的一个项目。k8gb 巧妙的利用了 DNS 的各种协议完成了一个通用且高度自动化的 GSLB 方案，看完后我就再也不想用其他的方案了。这篇博客会简单介绍一下其他几个方案各自存在的问题，以及 k8gb 究竟是如何巧妙的利用 DNS 来实现的 GSLB。</p><h2 id="什么是-GSLB"><a href="#什么是-GSLB" class="headerlink" title="什么是 GSLB"></a>什么是 GSLB</h2><p>GSLB（Global Service Load Balancer）是相对于单集群负载均衡的一个概念，单集群负载均衡主要作为一个集群的入口将流量分发到集群内部，而 GSLB 通常作为再上一层多个集群的流量入口，进行流量负载均衡和故障处理。一方面 GSLB 可以设置一些地理亲和的规则达到流量就近转发提升整体的性能，另一方面当某个集群出现问题后可以自动将流量切换到正常集群，减少单个集群故障对用户的影响。</p><h2 id="其他方案的问题"><a href="#其他方案的问题" class="headerlink" title="其他方案的问题"></a>其他方案的问题</h2><h3 id="商用负载均衡"><a href="#商用负载均衡" class="headerlink" title="商用负载均衡"></a>商用负载均衡</h3><p>GSLB 并不是一个新出的概念，所以不少商业公司都在这方面有很成熟的产品，例如 <a href="https://www.f5.com/solutions/use-cases/global-server-load-balancing-gslb">F5 GSLB</a>。这类产品通常有以下几个缺点：</p><ol><li>没有很好的和云原生对接，通常需要在 Kubernetes 集群外独立部署专有的软硬件，无法做到统一管理。</li><li>成本高，且有厂商锁定的风险。</li></ol><h3 id="公有云全局负载均衡"><a href="#公有云全局负载均衡" class="headerlink" title="公有云全局负载均衡"></a>公有云全局负载均衡</h3><p>公有云为了解决流量多地域分发会提供多集群负载均衡的产品，例如 AWS 的 <a href="https://aws.amazon.com/global-accelerator/">Global Accelerator</a> 和 GCP 的 <a href="https://cloud.google.com/load-balancing/docs/https">External Application Load Balancer</a>。GCP 上甚至还有一组自定义的 <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress">Multi Cluster Ingress</a> 资源，可以很好的和 Kubernetes 里的 Ingress 进行对接。但是他们也有一下几个问题：</p><ol><li>虽然是多集群负载均衡，但是多个集群必须是同一个公有云，不能在多云间进行流量调度。</li><li>私有云无法使用这个方案。</li></ol><h3 id="Karmada-Multi-Cluster-Ingress"><a href="#Karmada-Multi-Cluster-Ingress" class="headerlink" title="Karmada Multi  Cluster Ingress"></a>Karmada Multi  Cluster Ingress</h3><p>Karmada 是一个多集群的编排工具，也提供了自己的多集群流量调度方案 <a href="https://karmada.io/docs/userguide/service/multi-cluster-ingress/">Karmada Multi Cluster Ingress</a>。该方案通过在某个集群内部署一个由 Karmada 社区提供的 ingress-nginx 以及定义 <code>MultiClusterIngress</code> 来完成多集群流量调度，但这个方案有以下几个问题：</p><ol><li>依赖多集群间容器网络打通，和 ServiceImporter ServiceExporter 等 CRD，整体要求比较高。</li><li>需要额外再去管理这个提供 GSLB 服务的 ingress-nginx 实例，该部署在哪，该部署多少，怎么分配都是运维期间需要考虑的问题。</li><li>这个社区改造后的 <a href="https://github.com/karmada-io/multi-cluster-ingress-nginx">multi-cluster-ingress-nginx</a> 近两年基本没有什么代码提交，是否可用会让人有担忧。</li></ol><h3 id="简单的-DNS-方案"><a href="#简单的-DNS-方案" class="headerlink" title="简单的 DNS 方案"></a>简单的 DNS 方案</h3><p>如果手动攒出来一个简单的基于 DNS 的方案其实也是可以的，大部分 DNS 厂商都提供了健康检查的功能，因此我们可以将多个集群的出口 IP 地址加入到 DNS 的解析记录里，同时配置健康检查来做故障切换。但是这个简单的方案在规模扩大后就会有一些明显的限制：</p><ol><li>无法很好的自动化，一个集群下可能有多个不同的域名和多个不同的 Ingress IP 的组合，手动去管理他们的映射关系会随着规模增加变得难以维护。</li><li>DNS 厂商的健康检查通常是基于 TCP 和 ICMP 的，因此如果一个集群的出口完全挂掉这种故障是可以检测到并进行切换的。但是如果是局部故障就无法探测，例如多个 Ingress 复用一个 ingress-controller 并通过域名进行流量转发的情况下，如果其中一个服务的后端实例全部异常了，但是 ingress-controller 上的其他服务正常，那么健康检查还是会正常通过，并没有办法把流量切换到另一个集群。</li><li>DNS 本身存在各级缓存，更新时间可能较长。</li><li>DNS 健康检查本身也是个厂商提供的能力，并不能保证所有厂商都能提供这个能力，尤其是在私有云的场景。</li></ol><h2 id="k8gb-的解决方案"><a href="#k8gb-的解决方案" class="headerlink" title="k8gb 的解决方案"></a>k8gb 的解决方案</h2><p>k8gb 的解决方案其实也是用 DNS，但是通过自己的一系列巧妙的设计，解决了上面提到的简单 DNS 方案的一系列缺陷。</p><p>简单 DNS 方案的问题本质是没有和 Kubernetes 进行很好的对接，Kubernetes 内的一些动态信息，例如新增的 Ingress，新增的域名，服务的健康状态没法很好的同步到上游 DNS 服务器，而上游 DNS 服务器简单的健康检查也没办法应付 Kubernetes 里这种复杂的变化。因此 k8gb 最核心的一个变化就是上游的 DNS 记录不再是通过 A 记录或者 AAAA 记录指向集群的一个出口地址，而是 forward 到集群内的一个自己配置的 CoreDNS 进行 DNS 解析，将真正复杂的 DNS 逻辑下沉到集群里来自己控制。这样上游 DNS 只需要做简单的代理，不再需要配置健康检查，也不再需要动态的调整多个地址映射。</p><p>调整后用户请求 DNS 的流程如下图所示：</p><ul><li><img src="https://www.k8gb.io/docs/images/gslb-basic-multi-cluster.svg" alt="K8GB multi-cluster interoperability"></li></ul><ol><li>用户向外部 DNS 服务商请求一个域名的 IP 记录。</li><li>外部 DNS 将这个请求代理发送给集群内由 k8gb 管控的一个 CoreDNS。</li><li>k8gb 根据 Ingress Spec 里的域名，Ingress Status 里的 Ingress IP 以及集群内对应后端 Pod 的健康状态，负载均衡策略等信息分析出一个可用的 Ingress IP 返回给用户。</li><li>用户通过这个 IP 就可以直接访问到对应的 Ingress Controller。</li><li>当某一个集群的 k8gb 管控的 CoreDNS 出问题时由于上游 DNS 会同时将 DNS 请求代理到多个集群，另一个集群也可以返回自己的 Ingress IP，用户端可以通过多个返回的可用 IP 选择一个进行访问。</li></ol><p>这种方式管理员只需要在上游 DNS 注册几个域名后缀并代理到每个集群的 CoreDNS 就可以了，k8gb 本身也提供了自动化的能力，只要配好证书可以自动分析 Ingress 内使用的域名自动注册给上游 DNS，大幅简化了管理员的操作。</p><p>整个流程中还有一个特殊的点要注意，就是每个集群自己的 CoreDNS 不能只记录本集群 Ingress IP 的地址，还需要记录其他集群同一个 Ingress 的 Ingress IP 地址。因为如果只记录本集群的，当本集群对应服务的 Pod 都 Not Ready 时，CoreDNS 会返回 NXDomain 如果客户端收到了这个返回就会按照域名无法解析处理，此时另一个集群服务其实是可以正常提供服务的。因此 k8gb 还需要同步所有集群同一个域名 Ingress 对应的 Ingress IP 和它的健康状态。</p><p>众所周知多集群之间的数据同步也是一个世界难题，但是 k8gb 同样通过 DNS 巧妙的实现了数据的同步。</p><p>同步的流程图如下所示：</p><p><img src="https://www.k8gb.io/docs/images/k8gb-multi-cluster-interoperabililty.svg" alt="k8gb multi-cluster-interoperability"></p><p>大致的思路是每个集群的 k8gb 会把自己的 CoreDNS 的 Ingress IP 同样注册到上游 DNS，这样每个集群就可以直接访问另一个集群的 CoreDNS 了。然后每个集群内的 CoreDNS 再按照一个特殊的域名格式比如 <code>localtargets-app.cloud.example.com</code> 来保存本集群内 <code>app.cloud.example.com</code> Ingress 的 Ingress IP 并维护其健康状态。这样每个集群就都可以通过这个特殊的域名来获得其他集群这个域名对应的 Ingress IP 然后加入到自己的返回结果里，实现了域名解析的多集群同步。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>k8gb 作为一款开源的 GSLB 实现了和 Kubernetes 的无缝对接，能够很好的对跨集群的域名和流量进行自动化管理，并且对外部的依赖降到了只需要一个添加 DNS 记录的 API，真正实现了一套可以多云统一的 GSLB 方案。尽管目前这个项目还没有那么火热，但在我心里它已经是这个领域内的最佳方案了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;如何将流量在多个 Kubernetes 集群之间进行负载均衡，并做到自动的灾备切换一直是一个让人头疼的问题。我们之前调研了公有云，&lt;a href=&quot;https://github.com/karmada-io/multi-cluster-ingress-nginx&quot;&gt;Karm</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/tags/kubernetes/"/>
    
    <category term="network" scheme="http://oilbeater.com/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>如何轻松将 AI 生成内容整合入 Logseq 笔记</title>
    <link href="http://oilbeater.com/2024/03/21/sentence-to-logseq/"/>
    <id>http://oilbeater.com/2024/03/21/sentence-to-logseq/</id>
    <published>2024-03-21T10:39:37.000Z</published>
    <updated>2025-04-14T17:16:50.277Z</updated>
    
    <content type="html"><![CDATA[<p>我的日常生活已经融入了很多 AI 的协助，比如翻译一段文字、解释某个名词、回答一个问题等等。但是频繁使用后发现了另一个问题就是这些 AI 的产出对我的知识来说是个有益的补充，可是使用的时候通常是一次性的解决问题，并没有把这些内容沉淀下来。当需要把他们收集到笔记里的时候还要再回去整理，很是不方便。想象一下，如果这些临时查询的宝贵信息可以自动保存到自己的笔记系统中，就再也不用担心丢失重要的知识片段了。于是就萌生了做个工作流把这些 AI 生成的内容自动存到我的笔记系统里，然后再定期回顾。这里会介绍我调研的一些工具的局限，以及如何用了一个简单的框架把 AI 生成的内容自动导入到 logseq，并做成 flashcard。</p><h1 id="问题分解和初步调研"><a href="#问题分解和初步调研" class="headerlink" title="问题分解和初步调研"></a>问题分解和初步调研</h1><p>这里以我一个日常翻译的场景为例：当遇到读不懂的句子是我希望能够划词翻译，然后让 AI 帮我分析这个句子里疑难的单词，最后把这些内容以 flashcard 的格式保存到 logseq 里，这样借助手机端的同步我就可以在碎片时间来复习这个句子了。所以问题大致就是三个，1. 取出划线的文本 2. 编写一个 prompt 生成我想要的内容 3. 保存到 logseq。</p><p>前两步其实很多工具都已经集成了包括 Raycast AI 和 OpenAI-translator，但是第他们都没提供把前两步的结果导出到第三方的扩展接口。Raycast 理论上可以通过自己的插件体系来完成这件事，但是我这边不太熟悉 JS，并且也不想太依赖 Raycast AI，毕竟后面不打算续费了。于是决定自己写个 python 脚本做这些事，然后通过 Raycast 的 script command 和快捷键直接调用脚本，这样理论上也可以一个快捷键完成整个工作流。</p><h1 id="如何获取划选的文本"><a href="#如何获取划选的文本" class="headerlink" title="如何获取划选的文本"></a>如何获取划选的文本</h1><p>第一个难题就是获取划选的文本，看上去是个操作系统级别的操作，并不是很好实现。直到我看到了一个查词软件的实现我才恍然大悟：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/raycast/script-commands/blob/master/commands/apps/dictionary/look-up-in-dictionary.applescript</span></span><br><span class="line">tell application <span class="string">&quot;System Events&quot;</span></span><br><span class="line">    keystroke <span class="string">&quot;c&quot;</span> using &#123;<span class="built_in">command</span> down&#125;</span><br><span class="line">    delay 0.1</span><br><span class="line">    <span class="keyword">do</span> shell script <span class="string">&quot;open dict://&quot;</span> &amp; the clipboard</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>这个脚本本质上是模拟键盘摁下了 <code>cmd + v</code>，然后直接读剪切板去了，就这么绕过了获取选中文本的问题。考虑到我在用 Raycast 的时候经常碰到划选的文本识别错误，已经养成了 <code>cmd + v</code> 的习惯，那就不用费劲心思找获取划选的文本的方法了，直接读剪贴板就完了。</p><h1 id="调用-AI"><a href="#调用-AI" class="headerlink" title="调用 AI"></a>调用 AI</h1><p>这块基本经轻车熟路了，关键的是设置一个合适的 prompt，在翻译的同时生成一个句子的教学指南，并尽可能的按照 logseq 的格式来输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">message_text = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a university English teacher, below is a paragraph in English. &quot;</span> +</span><br><span class="line">        <span class="string">&quot;Please first translate it into Chinese. Then extract difficult words and phrases from the source paragraph, sort them in descending order of importance choose only the top 3 output them with explain of their usage to me in detail from a linguistic perspective.&quot;</span> +</span><br><span class="line">        <span class="string">&quot;The overall output should look like this: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- &#123;The Chinese Translation&#125; \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- Explanation: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;  - &#123;word or phrase&#125;: &#123;explanation&#125;\n&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>:<span class="string">&quot;user&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: content</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h1 id="输出到-logseq"><a href="#输出到-logseq" class="headerlink" title="输出到 logseq"></a>输出到 logseq</h1><p>在输出到 logseq 这一步的时候本来是打算调用 logseq 的 API，logseq 的开发者模式提供了一个本地的 HTTP 服务可以通过 HTTP 去调用。但是当我看了他们的 API 文档后差点给整崩溃了，所有的操作都要用 id，page id 又没办法去直接索引，要 getAll 后自己过滤。appendBlock 也不允许插入带层级的 Block，要串好几个 API 才能完成一个简单的操作。</p><p>崩溃的时候转念一想，logseq 不就是一堆 markdown 的渲染器么，既然 API 那么难用我直接去写文件不就好了，于是一组复杂的 API 调用变成了轻松愉快的文件 append 操作。这样既绕开了 logseq API 的限制，甚至有可能接入其他基于 markdown 的笔系统。</p><p>最后完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyperclip</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> AzureOpenAI</span><br><span class="line"></span><br><span class="line">content = pyperclip.paste()</span><br><span class="line"></span><br><span class="line">azure_endpoint = <span class="string">&quot;YOUR AZURE ENDPOINT&quot;</span></span><br><span class="line">api_key = <span class="string">&quot;YOUR API KEY&quot;</span></span><br><span class="line">model = <span class="string">&quot;YOUR MODEL NAME&quot;</span></span><br><span class="line">logseq_path = <span class="string">&quot;YOUR LOGSEQ PAGE FILE PATH&quot;</span></span><br><span class="line"></span><br><span class="line">client = AzureOpenAI(</span><br><span class="line">  azure_endpoint = azure_endpoint,</span><br><span class="line">  api_key=api_key,</span><br><span class="line">  api_version=<span class="string">&quot;2024-02-15-preview&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">message_text = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a university English teacher, below is a paragraph in English. &quot;</span> +</span><br><span class="line">        <span class="string">&quot;Please first translate it into Chinese. Then extract difficult words and phrases from the source paragraph, sort them in descending order of importance choose only the top 3 output them with explain of their usage to me in detail from a linguistic perspective.&quot;</span> +</span><br><span class="line">        <span class="string">&quot;The overall output should look like this: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- &#123;The Chinese Translation&#125; \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- Explanation: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;  - &#123;word or phrase&#125;: &#123;explanation&#125;\n&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>:<span class="string">&quot;user&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: content</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">completion = client.chat.completions.create(</span><br><span class="line">  model=model, <span class="comment"># model = &quot;deployment_name&quot;</span></span><br><span class="line">  messages = message_text,</span><br><span class="line">  temperature=<span class="number">0.7</span>,</span><br><span class="line">  max_tokens=<span class="number">500</span>,</span><br><span class="line">  top_p=<span class="number">0.95</span>,</span><br><span class="line">  frequency_penalty=<span class="number">0</span>,</span><br><span class="line">  presence_penalty=<span class="number">0</span>,</span><br><span class="line">  stop=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> completion.choices[<span class="number">0</span>].message.content == <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;No response&quot;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">response = completion.choices[<span class="number">0</span>].message.content</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(logseq_path, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&quot;\n- &quot;</span> + content.rstrip() + <span class="string">&quot; #card #English&quot;</span> + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> response.splitlines():</span><br><span class="line">        <span class="keyword">if</span> line.startswith(<span class="string">&quot;```&quot;</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> line.rstrip() == <span class="string">&quot;&quot;</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line.startswith(<span class="string">&quot;- &quot;</span>):</span><br><span class="line">            line = <span class="string">&quot;- &quot;</span> + line.rstrip()</span><br><span class="line">        f.write(<span class="string">&quot;  &quot;</span> + line + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure><p>后需要做的就是在 Raycast 里给这个脚本设置一个快捷键，这样下次碰到不会的句子，选中复制后就可以通过快捷键完成翻译，难点提取，和生成 flashcard 的整个工作流。</p><p>最后生成的 flashcard 效果大概如下：</p><p><img src="/../images/logseq-card.png" alt="alt text"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过将 AI 生成的内容自动化集成到 Logseq 笔记中，我们不仅提高了信息管理的效率，还优化了学习和工作流程。通过一个简单的脚本每个人都可以轻松地将这种强大的技术整合到日常生活中，实现知识的积累和复习。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我的日常生活已经融入了很多 AI 的协助，比如翻译一段文字、解释某个名词、回答一个问题等等。但是频繁使用后发现了另一个问题就是这些 AI 的产出对我的知识来说是个有益的补充，可是使用的时候通常是一次性的解决问题，并没有把这些内容沉淀下来。当需要把他们收集到笔记里的时候还要再</summary>
      
    
    
    
    
    <category term="logseq" scheme="http://oilbeater.com/tags/logseq/"/>
    
    <category term="ai" scheme="http://oilbeater.com/tags/ai/"/>
    
    <category term="learning" scheme="http://oilbeater.com/tags/learning/"/>
    
  </entry>
  
  <entry>
    <title>实现 VS Code Remote SSH 下的自动关机</title>
    <link href="http://oilbeater.com/2024/03/13/shutdown-remote-ssh-server/"/>
    <id>http://oilbeater.com/2024/03/13/shutdown-remote-ssh-server/</id>
    <published>2024-03-13T10:09:26.000Z</published>
    <updated>2025-04-14T17:16:50.277Z</updated>
    
    <content type="html"><![CDATA[<p>我现在所有的开发环境都转移到了 GCP 的 Spot Instance 实例，然后用 VS Code Remote SSH 插件进行连接。这种方式的好处是 GCP 可以按秒计费，即使开了很高规格的机器只要及时关机费用也是可控的，缺点是如果中间忘了关机赶上过节那费用就烧开了。再被烧掉了 10 多美元后，痛定思痛决定找个能在 VS Code 空闲时自动关机的方法。过程中为了能够在 Docker 容器内执行 shutdown 命令还搞了一些黑魔法。</p><h1 id="如何判断空闲"><a href="#如何判断空闲" class="headerlink" title="如何判断空闲"></a>如何判断空闲</h1><p>这个功能其实类似 CodeSpace 里的 idle timeout，但是 Remote SSH 这个插件并没有暴露这个功能，所以只能自己实现。其实主要的难点在于如何在 Server 端判断空闲，找了一圈没在 VS Code 里看到有暴露的接口，于是就想能不能跳出 VS Code 看看有什么方法能够简洁的判断空闲发生。</p><p>最直接的想法就是去看 SSH 的连接，因为 Remote SSH 也是通过 SSH 连接上来的，如果当前机器没有存活的 SSH 连接，那么就可以认为是空闲直接关机了。但是问题是 Remote SSH 的连接超时时间会特别长，搜索了一些 Issue 有说是 4 个小时的，我也尝试了直接关闭 VS Code 的客户端，发现 Server 端的 SSH 连接也一直没有消失。如果在 Server 端设置 SSH 超时，Client 那边很快就会重连，连接数量也不会减少。</p><p>既然没法通过直接看 SSH 连接数量来判断，那么就进一步去看能不能判断已有的 SSH 连接是不是已经没有流量了。用 <code>tcpdump</code> 抓包看了一下，即使客户端没有任何交互，还是会有 1s 一次的 TCP 心跳数据包，所以也不能有流量为 0 来判断。不过观察下来心跳数据包的大小都是固定的，都是 44 字节，正好可以根据这个特征来判断，如果一段时间内 SSH 端口没有大于 44 字节的数据包就可以判断空闲了。</p><p>于是第一版本的脚本就出来了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span></span><br><span class="line">  <span class="keyword">if</span> [ -f /root/dump ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">timeout</span> 1m tcpdump -nn -i ens4 tcp and port 22 and greater 50 -w /root/dump</span><br><span class="line"></span><br><span class="line">  line_count=$(<span class="built_in">wc</span> -l &lt; /root/dump)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> [ <span class="variable">$line_count</span> -gt 0 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">    shutdown -h now</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>这样就实现了 1 分钟内 SSH 端口没有非心跳数据包就关机的功能，下一步就是要让这个脚本自动运行了。</p><h1 id="Docker-黑魔法"><a href="#Docker-黑魔法" class="headerlink" title="Docker 黑魔法"></a>Docker 黑魔法</h1><p>在把脚本打包成 Docker 镜像时发现了一个有趣的问题，那就是所有的 Base Image 里都没有 <code>shutdown</code> 命令，<code>shutdown</code> 命令也没法很容易的安装。为了能够执行主机上的 <code>shutdown</code> 命令，就需要在 Docker 容器里切换到主机的命名空间，再去关机。所以需要把之前的脚本稍微修改一下，包装一下 <code>shutdown</code> 命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsenter -a -t 1 shutdown -h now</span><br></pre></td></tr></table></figure><p>这个 <code>nsenter</code> 命令的作用是选定 Pid 为 1 的进程，然后进入这个进程的所有(pid, mount, network) Namespaces，这样当 Docker 运行在共享主机 Pid 模式下我们相当于就进入了主机 1 号进程的所有 Namespaces，看上去就和 SSH 到主机上一样可以执行操作了。这样只要再用下面的命令启动容器，就可以不担心忘记随时关机了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name=close --pid=host --network=host --privileged --restart=always -d close:v0.0.1</span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过监控 SSH 的流量情况可以一定程度上猜测 VS Code 已经空闲了，然后再用 Docker 的一些黑魔法就可以实现自动关机了。不过整个链路的黑魔法都太多了，有没有什么简单的方式呢？</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我现在所有的开发环境都转移到了 GCP 的 Spot Instance 实例，然后用 VS Code Remote SSH 插件进行连接。这种方式的好处是 GCP 可以按秒计费，即使开了很高规格的机器只要及时关机费用也是可控的，缺点是如果中间忘了关机赶上过节那费用就烧开了。</summary>
      
    
    
    
    
    <category term="docker" scheme="http://oilbeater.com/tags/docker/"/>
    
    <category term="vscode" scheme="http://oilbeater.com/tags/vscode/"/>
    
    <category term="tools" scheme="http://oilbeater.com/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>minikube，kind 和 k3d 大比拼</title>
    <link href="http://oilbeater.com/2024/02/22/minikube-vs-kind-vs-k3d/"/>
    <id>http://oilbeater.com/2024/02/22/minikube-vs-kind-vs-k3d/</id>
    <published>2024-02-22T14:42:22.000Z</published>
    <updated>2025-04-14T17:16:50.277Z</updated>
    
    <content type="html"><![CDATA[<p>作为云原生生态的一个开发者，开发中经常碰到的一个需求是要频繁测试应用在 Kubernetes 环境下的运行状态，在 CI 中可能还要快速测试多个不同 Kubernetes 集群的配置，例如单点，高可用，双栈，多集群等等。因此能够低成本的在本地单机环境快速创建管理 Kubernetes 集群就成了一个刚需。本文将介绍几个常见的单机 Kubernetes 管理工具 minikube, kind 和 k3d 各自的特点、使用场景以及可能的坑。</p><blockquote><p>TL;DR<br>如果你只关心快不快，那么 k3d 是最好的选择。如果你关心的是兼容性以及测试尽可能模拟真实场景，那么 minikube 是最稳妥的选择。kind 算是在这两个之间的一个平衡。</p></blockquote><ul><li><a href="#%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF%E6%AF%94%E8%BE%83">技术路线比较</a><ul><li><a href="#minikube">minikube</a></li><li><a href="#kind">kind</a></li><li><a href="#k3d">k3d</a></li></ul></li><li><a href="#%E5%90%AF%E5%8A%A8%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83">启动性能比较</a><ul><li><a href="#%E6%B5%8B%E8%AF%95%E6%96%B9%E6%B3%95">测试方法</a></li><li><a href="#%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C">测试结果</a></li></ul></li><li><a href="#%E6%80%BB%E7%BB%93">总结</a></li></ul><h1 id="技术路线比较"><a href="#技术路线比较" class="headerlink" title="技术路线比较"></a>技术路线比较</h1><p>这三者大体功能是类似的，都能够完成单机管理 Kubernetes 的任务，但是由于一些历史原因和技术选项导致了一些细节和使用场景的差异。</p><h2 id="minikube"><a href="#minikube" class="headerlink" title="minikube"></a>minikube</h2><p><a href="https://minikube.sigs.k8s.io/docs/">minikube</a> 是 Kubernetes 社区最早的一款快速在本地创建 Kubernetes 的软件，也是很多老人第一次上手 Kubernetes 的工具。早期版本是通过在本机创建 VM 来模拟多节点集群，这个方案的好处是能够最大程度还原真实场景，一些操作系统级别的变化，例如不同 OS，不同内核模块都可以覆盖到。缺点就是资源消耗太大，而且在一些虚拟化环境如果没有嵌套虚拟化的支持是没办法运行的，并且启动的速度也比较慢。不过社区最近也推出了 Docker 的 Driver 这些问题都得到了比较好的解决，不过对应代价就是一些虚拟机级别的模拟就不好做了。此外 minikube 还提供了不少的 addon，比如 dashboard，nginx-ingress 等常见的社区组件都能快速的安装使用。</p><h2 id="kind"><a href="#kind" class="headerlink" title="kind"></a>kind</h2><p><a href="https://kind.sigs.k8s.io/">kind</a> 是近几年流行起来的一个本地部署 Kubernetes 的工具，他的主要特点就是用 Docker 容器模拟节点，并且基本只专注在 Kubernetes 标准部署这一个事情上，其他社区组件都需要额外自己去安装。目前 Kubernetes 本身的 CI 也是通过 kind 来跑的。优点就是启动速度很快，熟悉 Docker 的人用起来也很顺手。缺点是用了容器模拟缺乏操作系统级别的隔离，而且和宿主机共享内核，一些操作系统相关的测试就不好测试了。我之前在测一个内核模块的时候就因为宿主机加了一些 netfilter 功能，结果 kind 里的 Kubernetes 集群挂了。</p><h2 id="k3d"><a href="#k3d" class="headerlink" title="k3d"></a>k3d</h2><p><a href="https://k3d.io/stable/">k3d</a> 是一个超轻量的本地部署 Kubernetes 工具，他的大体思路和 kind 类似，都是通过 Docker 来模拟节点，主要区别在于部署的不是个标准 Kubernetes 而是一个轻量级的 <a href="https://k3s.io/">k3s</a>，所以他的大部分优缺点也来自于下面这个 k3s。优点就是安装极致的快，你先别管对不对，你就问快不快吧。缺点主要来自于为了速度做出的一些牺牲，比如镜像用的是个超精简的操作系统，连 glibc 都没有，因此一些要在操作系统层面的操作都会无比困难。还有就是他的安装方式和常见的 kubeadm 也不一样，Kubernetes 组件都不是容器启动的，如果依赖标准部署的一些特性可能都会比较困难。</p><h1 id="启动性能比较"><a href="#启动性能比较" class="headerlink" title="启动性能比较"></a>启动性能比较</h1><p>minikube 社区有一些<a href="https://minikube.sigs.k8s.io/docs/benchmarks/timetok8s/v1.32.0/">性能测试报告</a>，正好对比的就是本文关注的三款软件的启动速度，不过我更关注的是其他的一些方面，比如镜像大小，内存占用以及最小化安装的启动时间，所以还是再做了一组测试。</p><h2 id="测试方法"><a href="#测试方法" class="headerlink" title="测试方法"></a>测试方法</h2><p>由于三款软件都是一行命令就可以启动，测试还是比较方便的，主要注意以下几个点：</p><ol><li>minikube 采用 Docker Driver，因为真要测启动速度还用虚拟机的 Driver 就没什么意义了。</li><li>所有测试都是镜像已经下载到本地的结果，不会涉及网络下载时间。</li><li>测试的每个软件都是当前的最新版，但是他们支持的 Kubernetes 版本不一致，不是很严谨，但是定性分析应该够了。</li><li>都只启动最基本的组件，不安装其他插件，但是基础 CNI 和 CoreDNS 以及 CSI 都是有的，保证应用的基本运行。</li><li>使用 <code>docker image</code> 命令查看镜像大小，使用 <code>docker stat</code> 查看内存用量。</li></ol><p>测试的命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#minikube</span></span><br><span class="line">time minikube start --driver=docker --force</span><br><span class="line"></span><br><span class="line"><span class="comment">#kind</span></span><br><span class="line">time kind create cluster</span><br><span class="line"></span><br><span class="line"><span class="comment">#k3d</span></span><br><span class="line">time k3d cluster create mycluster --k3s-arg <span class="string">&#x27;--disable=traefik,metrics-server@server:*&#x27;</span> --no-lb</span><br></pre></td></tr></table></figure><h2 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h2><table><thead><tr><th>名称</th><th>软件版本</th><th>Kubernetes 版本</th><th>镜像大小</th><th>启动时间</th><th>内存消耗</th></tr></thead><tbody><tr><td>minikube</td><td>v1.32.0</td><td>v1.28.3</td><td>1.2GB</td><td>29s</td><td>536MiB</td></tr><tr><td>kind</td><td>v0.22</td><td>v1.29.2</td><td>956MB</td><td>20s</td><td>463MiB</td></tr><tr><td>k3d</td><td>v5.6.0</td><td>v1.27.4</td><td>263MB</td><td>7s</td><td>423MiB</td></tr></tbody></table><p>可见单从启动性能这个指标，k3d 在镜像大小，启动时间和内存消耗几个方面都有比较大的优势，对于用 Github 免费 Action 跑 CI 的穷人还是很有吸引力的。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>如果快和更少的资源占用是最重要的目标，那么 k3d 相当合适，如果要测试需要操作系统级别隔离的功能，那么 minikube 的虚拟机 Driver 是唯一的选择，其他场景下 kind 会在兼容和性能之间比较平衡。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;作为云原生生态的一个开发者，开发中经常碰到的一个需求是要频繁测试应用在 Kubernetes 环境下的运行状态，在 CI 中可能还要快速测试多个不同 Kubernetes 集群的配置，例如单点，高可用，双栈，多集群等等。因此能够低成本的在本地单机环境快速创建管理 Kuber</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/tags/kubernetes/"/>
    
    <category term="ci" scheme="http://oilbeater.com/tags/ci/"/>
    
  </entry>
  
  <entry>
    <title>Golang 中预分配 slice 内存对性能的影响（续）</title>
    <link href="http://oilbeater.com/2024/01/09/alloc-slice-for-golang-2-md/"/>
    <id>http://oilbeater.com/2024/01/09/alloc-slice-for-golang-2-md/</id>
    <published>2024-01-09T08:16:22.000Z</published>
    <updated>2025-04-14T17:16:50.277Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#%E5%9F%BA%E7%A1%80%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95">基础性能测试</a></li><li><a href="#%E6%95%B4%E4%B8%AA-slice-append">整个 Slice Append</a></li><li><a href="#%E5%A4%8D%E7%94%A8-slice">复用 Slice</a></li><li><a href="#syncpool">sync.Pool</a></li><li><a href="#bytebufferpool">bytebufferpool</a></li><li><a href="#%E6%80%BB%E7%BB%93">总结</a></li></ul><p>之前写了一篇 <a href="https://oilbeater.com/2023/07/19/pre-alloc-slice-for-golang/">Golang 中预分配 slice 内存对性能的影响</a>，探讨了一下在 Slice 中预分配内存对性能的影响，之前考虑的场景比较简单，最近又做了一些其他测试，补充一下进一步的信息。包括整个 Slice append，sync.Pool 对性能的影响。</p><h1 id="基础性能测试"><a href="#基础性能测试" class="headerlink" title="基础性能测试"></a>基础性能测试</h1><p>最初的 BenchMark 代码，只考虑了 Slice 是否初始化分配空间的情况，具体的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> prealloc_test</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;sync&quot;</span></span><br><span class="line">    <span class="string">&quot;testing&quot;</span></span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">var</span> length = <span class="number">1024</span></span><br><span class="line"><span class="keyword">var</span> testtext = <span class="built_in">make</span>([]<span class="type">byte</span>, length, length)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocateByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">        <span class="keyword">var</span> init []<span class="type">byte</span></span><br><span class="line">        <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">            init = <span class="built_in">append</span>(init, testtext[j])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocateByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">        <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">            init = <span class="built_in">append</span>(init, testtext[j])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        569978              2151 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          804807              1304 ns/op            1024 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>可见没有预分配的情况下多了 8 次内存分配，两个相对比可以粗略的认为 40% 的时间消耗在了这额外的 8 次内存分配。</p><p>这两个测试用例使用的是循环里逐个 append 元素，但是 Slice 还支持整个 Slice 进行 append 在这种情况下的性能差距是没有体现出来的。而且在这两个测试用例里我们其实无法知道内存分配所占的时间消耗占整个时间的占比。</p><h1 id="整个-Slice-Append"><a href="#整个-Slice-Append" class="headerlink" title="整个 Slice Append"></a>整个 Slice Append</h1><p>因此加入两个整个 Slice Append 的测试用例，观察预分配内存对性能还有没有这么大的影响。新增的用例代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">        <span class="keyword">var</span> init []<span class="type">byte</span></span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        569978              2151 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          804807              1304 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                3829890               311.5 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3968048               306.7 ns/op          1024 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>可见两个用例都只用了一次内存分配，消耗时间几乎相同，而且大幅低于逐个元素进行 append 的情况。一方面整个 Slice append，在 Slice 扩容时就知道了最终的大小没必要进行动态内存分配，降低了内存分配的开销。另一方面整个 Slice append 在实现上会进行整段复制，降低了循环的开销，性能会提升很多。</p><p>但在这里每次还是会有一次内存分配，我们依然无法确定这一次内存分配所占的整体时间比例。</p><h1 id="复用-Slice"><a href="#复用-Slice" class="headerlink" title="复用 Slice"></a>复用 Slice</h1><p>为了计算一次内存分配的消耗，我们设计一个新的测试用例，将 Slice 的创建放到循环外，循环内每次最后将 Slice 的 length 设为 0，给下次进行复用。这样在大量的测试下只会进行一次内存分配，平均下来就可以忽略不计了。具体的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate2</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">        init = init[:<span class="number">0</span>]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        514904              2171 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          761772              1333 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                4041459               320.9 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3854649               320.1 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate2-12                63147178                18.63 ns/op            0 B/op          0 allocs/op</span><br></pre></td></tr></table></figure><p>可见这次测试统计上没有内存分配了，整体消耗时间也降为了之前的 5%。因此大致可以计算出在之前的测试用例里每一次内存分配会消耗 95% 的时间，这个占比还是很惊人的。因此对于性能敏感的场景还是需要尽可能的复用对象，避免反复的对象创建的内存开销。</p><h1 id="sync-Pool"><a href="#sync-Pool" class="headerlink" title="sync.Pool"></a>sync.Pool</h1><p>简单的场景下可以像上个测试用例里一样手动的清空 Slice 在循环内进行复用，但是真实场景里对象的创建通常会发生在代码的各个地方，就需要统一的进行管理和复用了，Golang 里的 <code>sync.Pool</code> 就是做这个事情的，而且使用起来也很简单。但是内部实现还是比较复杂的，为了性能进行了大量无锁化的设计，具体实现可以参考<a href="https://unskilled.blog/posts/lets-dive-a-tour-of-sync.pool-internals/">Let’s dive: a tour of sync.Pool internals</a>。</p><p>使用 <code>sync.Pool</code> 重新设计的测试用例如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> sPool = &amp;sync.Pool&#123; </span><br><span class="line">        New: <span class="function"><span class="keyword">func</span><span class="params">()</span></span> any &#123;</span><br><span class="line">                b := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">                <span class="keyword">return</span> &amp;b</span><br><span class="line">        &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPoolByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">        b.ResetTimer()</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">                <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">                b := sPool.Get().(*[]<span class="type">byte</span>)</span><br><span class="line">                buf := *b</span><br><span class="line">                <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">                        buf = <span class="built_in">append</span>(buf, testtext[j])</span><br><span class="line">                &#125;</span><br><span class="line">                buf = buf[:<span class="number">0</span>]</span><br><span class="line">                sPool.Put(b)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPool</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">        b.ResetTimer()</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">                <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">                bufPtr := sPool.Get().(*[]<span class="type">byte</span>)</span><br><span class="line">                buf := * bufPtr</span><br><span class="line">                buf = <span class="built_in">append</span>(buf, testtext...)</span><br><span class="line">                buf = buf[:<span class="number">0</span>]</span><br><span class="line">                sPool.Put(bufPtr)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中 <code>New</code> 用来给 <code>sync.Pool</code> 一个在没有可用对象时创建对象的构造函数，使用的时候使用 <code>Get</code> 方法从 Pool 里获取一个对象，用完了再用 <code>Put</code> 方法把对象还给 <code>sync.Pool</code>。这里主要注意一下对象的生命周期，以及放回到 <code>sync.Pool</code> 时需要清空对象，避免脏数据。测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        469431              2313 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          802392              1339 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPoolByElement-12                1212828               961.5 ns/op             0 B/op          0 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                3249004               370.2 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3268851               368.2 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate2-12                62596077                18.63 ns/op            0 B/op          0 allocs/op</span><br><span class="line">BenchmarkPool-12                        32707296                35.59 ns/op            0 B/op          0 allocs/op</span><br></pre></td></tr></table></figure><p>可见使用 <code>sync.Pool</code> 也可以避免内存分配，由于 <code>sync.Pool</code> 还有一些额外的处理性能消耗会比手动复用 Slice 稍高一些，不过考虑到使用的便利性以及相比不使用还是有明显的性能提升，还是一个不错的方案。</p><p>但是直接使用 <code>sync.Pool</code> 也有下面两个问题：</p><ol><li>对于 Slice 的情况 <code>New</code> 分配的初始内存是固定的，运行时使用空间超出的话，可能还会有大量动态的内存分配调整。</li><li>另一个极端是 Slice 被动态扩容很大后放回到 <code>sync.Pool</code> 中，可能会造成内存的泄漏和浪费。</li></ol><h1 id="bytebufferpool"><a href="#bytebufferpool" class="headerlink" title="bytebufferpool"></a>bytebufferpool</h1><p>为了达到实际运行时更优的性能，<a href="https://github.com/valyala/bytebufferpool">bytebufferpool</a> 这个项目在 <code>sync.Pool</code> 的基础上运用了一些简单的统计规律，尽可能的减少了上面提到的两个问题在运行时的影响。（该项目的作者是俄罗斯人，手下还有 fasthttp, quicktemplate 和 VictoriaMetrics 几个项目，个顶个都是性能优化的优秀案例，战斗民族经常会搞这种性能推极限的项目。</p><p>代码里主要的结构如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// bytebufferpool/pool.go</span></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">  minBitSize = <span class="number">6</span> <span class="comment">// 2**6=64 is a CPU cache line size</span></span><br><span class="line">  steps      = <span class="number">20</span></span><br><span class="line"> </span><br><span class="line">  minSize = <span class="number">1</span> &lt;&lt; minBitSize</span><br><span class="line">  maxSize = <span class="number">1</span> &lt;&lt; (minBitSize + steps - <span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">  calibrateCallsThreshold = <span class="number">42000</span></span><br><span class="line">  maxPercentile           = <span class="number">0.95</span></span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">type</span> Pool <span class="keyword">struct</span> &#123;</span><br><span class="line">  calls       [steps]<span class="type">uint64</span></span><br><span class="line">  calibrating <span class="type">uint64</span></span><br><span class="line"> </span><br><span class="line">  defaultSize <span class="type">uint64</span></span><br><span class="line">  maxSize     <span class="type">uint64</span></span><br><span class="line"> </span><br><span class="line">  pool sync.Pool</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中 <code>defaultSize</code> 的作用是 <code>New</code> 的时候给 Slice 分配的大小，<code>maxSize</code> 的作用是超过这个大小的 Slice <code>Put</code> 时会拒绝。核心的算法其实就是在运行时根据统计到的 Slice 使用大小信息动态的去调整 <code>defaultSize</code> 和 <code>maxSize</code> ，避免额外的内存分配同时还要避免内存泄漏。</p><p>这个动态统计的过程也比较简单，就是将 <code>Put</code> 到 Pool 里的 Slice 大小划分了 20 个区间范围进行统计，当 <code>Put</code> 次数达到 <code>calibrating</code> 后就进行一次排序，将这个时间段内使用最为频繁的区间大小作为 <code>defaultSize</code> 这样在统计上就可以避免不少额外的内存分配。然后按大小排序，将 95% 分位大小设置为  <code>maxSize</code>，这样就避免了在统计上长尾大的对象进入 Pool。就靠着这样动态调整这两个值，在统计上可以在运行时获得更优的性能。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>Slice 初始化尽可能指定 capacity</li><li>避免在循环中初始化 Slice</li><li>性能敏感路径考虑使用 <code>sync.Pool</code></li><li>内存分配的性能开销可能远大于业务逻辑</li><li>bytebuffer 的复用可以考虑看下 bytebufferpool</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%9F%BA%E7%A1%80%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95&quot;&gt;基础性能测试&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%95%B4%E4%B8%AA-slice-append&quot;&gt;整个</summary>
      
    
    
    
    
    <category term="性能" scheme="http://oilbeater.com/tags/%E6%80%A7%E8%83%BD/"/>
    
    <category term="golang" scheme="http://oilbeater.com/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>不需要封装的 Overlay 容器网络</title>
    <link href="http://oilbeater.com/2023/07/30/xmasq-overlay-without-encap/"/>
    <id>http://oilbeater.com/2023/07/30/xmasq-overlay-without-encap/</id>
    <published>2023-07-30T05:24:33.000Z</published>
    <updated>2025-04-14T17:16:50.277Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8-overlay-%E7%BD%91%E7%BB%9C">为什么要用 Overlay 网络</a></li><li><a href="#%E5%AE%B9%E5%99%A8-overlay-%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BC%80%E9%94%80%E6%9D%A5%E6%BA%90">容器 Overlay 网络的开销来源</a></li><li><a href="#%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3-overlay-%E5%B0%81%E8%A3%85%E7%9A%84%E5%BC%80%E9%94%80">如何解决 Overlay 封装的开销</a></li><li><a href="#%E6%88%91%E7%9A%84%E6%83%B3%E6%B3%95">我的想法</a></li></ul><p>上周在看 <a href="https://antrea.io/">Antrea</a> 的<a href="https://github.com/antrea-io/antrea/wiki/Community-Meetings#july-17-2023">会议纪要</a>时，发现了一篇上海交大和 VMware 合作的论文 —— <a href="https://arxiv.org/pdf/2305.05455.pdf">XMasq: Low-Overhead Container Overlay Network Based on eBPF</a>。论文里介绍不少通过 eBPF 做容器网络性能优化的方法，不过最令人印象深刻的是他们最终无需给容器数据包封装额外的隧道头就可以实现 Overlay 网络，这里就简单介绍下作者们是如何做到这一点的。</p><h2 id="为什么要用-Overlay-网络"><a href="#为什么要用-Overlay-网络" class="headerlink" title="为什么要用 Overlay 网络"></a>为什么要用 Overlay 网络</h2><p>Overlay 网络相比 Underlay 网络可以完全解耦应用层和底层物理网络,确保两者的灵活性。应用层不需要关心底层物理网络的路由规则等细节,物理网络也不需要针对容器 IP 进行专门路由配置。因此 Overlay 网络作为一种不调底层网络实现的容器组网技术，快速的在容器领域铺开了，主流的开源网络插件处于安装兼容的考虑，基本上都会将 Overlay 作为默认的安装选项。</p><h2 id="容器-Overlay-网络的开销来源"><a href="#容器-Overlay-网络的开销来源" class="headerlink" title="容器 Overlay 网络的开销来源"></a>容器 Overlay 网络的开销来源</h2><p>但是在灵活性的便利下，带来的是性能方面的开销。根据论文里的测量，主流的 Overlay 网络插件，相比于 HOST 网络，吞吐量会下降 25%，CPU 消耗会增加 34%~44%，延迟会上升 45%~58%。</p><p>当然，这里面的性能开销并不完全是 Overlay 的封装带来的，由于容器网络本身的流量路径和 HOST 网络就有比较大的区别，Overlay 封装其实只是其中的一环。如下图所示：<br><img src="/../images/container-network-overhead.png" alt="Alt text"></p><p>其中 1~4，7~10 都是容器网络相比宿主机网络额外带来的链路，Overlay 的封装主要在 4 和 7，分别占到了 egress 和 ingress 开销的 24% 和 29%。</p><p>其他几个链路上的优化在 Cilium 和其他开源项目上其实已经见到过一些了，主要思路是通过 eBPF bypaas 掉部分链路，将数据包能够从容器网络栈直通到物理网卡，下面主要介绍下这篇论文是如何解决 Overlay 封装的问题。</p><h2 id="如何解决-Overlay-封装的开销"><a href="#如何解决-Overlay-封装的开销" class="headerlink" title="如何解决 Overlay 封装的开销"></a>如何解决 Overlay 封装的开销</h2><p>常见的 Overlay 隧道如 VXlan 和 Geneve 都是通过给容器网络的数据包封装一个 UDP 的 Header 实现的 Overlay。外层的 UDP 头部记录宿主机实际的 IP 和 Mac，内层数据包的 Header 里记录容器网络的 IP 和 Mac，外层作为真实网络的通信标识，内层作为容器网络的通信标识。那么有没有可能把两层的信息通过一层全带走，从而实现不需要额外的 UDP Header 呢？这就是这篇论文作者的一个很有意思的工作。</p><p>外层的 IP 和 Mac 作为真实通信的标识是不可能省略的，但是 IP 的 Header 中有一部分字段比如 DSCP 和 ID 是很少被使用的。如果能将这两个字段利用起来，编码内层的信息，那么我们就可以不用内层的容器 IP 和 Mac 这一层了。这里可以想象一下用 iptables 来做 nat 其实是将源 IP 和端口映射成目标 IP 和端口，也可以理解为一种编码的映射关系。</p><p>作者在内核中通过 eBPF 的 Map 缓存了容器网络的 Mac 和 IP 信息，并生成了一个 key 来对应每一组 IP 和 Mac。这样，当容器再进行跨主机网络通信时，可以直接将目标地址修改为对应宿主机的地址，同时将这个 key 写入 IP 中的指定的保留字段。等数据包到达对端后，对端就可以通过这个 key 查询本地的缓存将数据包的地址再还原为容器的地址。这样并不需要额外的封装，通过 IP 和 Mac 头的直接替换，就完成了跨主机的 Overlay 网络。</p><p>由于论文中还应用了很多其他 bypass 的优化，没有单独衡量 Overlay 改造带来的性能提升，从整体效果来看性能优化后的效果较普通 Overlay 网络有很大的提升基本接近 HOST 网络。</p><h2 id="我的想法"><a href="#我的想法" class="headerlink" title="我的想法"></a>我的想法</h2><p>按照我之前的经验使用 UDP 进行 Overlay 封装还有一个很大的性能问题，那就是如果内部数据包是 TCP 的时候，TCP 其实有很多网卡的 Offload 优化，能大幅提升吞吐量，但如果是封装后这种 TCP in UDP 的形式，很多网卡 Offload 的能力就失效了。我们之前在一些虚拟化的平台，使用 UDP 的封装吞吐量只有使用 STT 这种 TCP 进行封装的十分之一。不过这个差别也和网卡的能力，操作系统内核相关，在作者测试的网卡上并没有表现出来明显的差异，有可能是物理网卡支持这种 TCP in UDP 的 Offload。</p><p>这篇论文的目的是希望把这套性能优化做成一个 CNI 无关的插件，所有的容器网络都可以使用，但是论文里大量的 eBPF bypaas 其实也会跳过 CNI Datapath 的处理，会导致原有 CNI 功能的缺失。所以做一个通用插件的意义可能并不大。直接做一个新的 CNI ，不要求其他功能，只追求 Overlay 的通用性和极致的性能其实更合适一些，没准能拿下不少的细分领域市场。反而是 Overlay 封装这块如果能单独拿出来，做成和 VXlan，Geneve 并列的一种封装格式，倒是能真正做到 CNI 无关的一个通用模块。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8-overlay-%E7%BD%91%E7%BB%9C&quot;&gt;为什么要用 Overlay 网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%AE%</summary>
      
    
    
    
    
    <category term="network" scheme="http://oilbeater.com/tags/network/"/>
    
    <category term="performance" scheme="http://oilbeater.com/tags/performance/"/>
    
  </entry>
  
  <entry>
    <title>如何在本地用 CPU 跑大模型</title>
    <link href="http://oilbeater.com/2023/07/27/local-cpu-llm/"/>
    <id>http://oilbeater.com/2023/07/27/local-cpu-llm/</id>
    <published>2023-07-27T10:51:07.000Z</published>
    <updated>2025-04-14T17:16:50.277Z</updated>
    
    <content type="html"><![CDATA[<p>之前觉得大模型在本地运行是个不可能的事情，我甚至没有一块 GPU。但是尝试了一下 llama.cpp 发现几分钟就把 Meta 最新开源的 Llama 2 在 CPU 上运行起来了，速度和质量都还可以接受，很多想法突然就变得可行了。记录一下搭建的过程，希望对感兴趣的人有帮助。</p><h1 id="需要的配置"><a href="#需要的配置" class="headerlink" title="需要的配置"></a>需要的配置</h1><p>尽管不需要 GPU 和加速卡，但是大模型对磁盘和内存还是有需求的，我运行的是 Llama 2 7B 的模型，经过了 GGML 处理，选择了 int8 的精度。这个模型跑起推理来大概需要 7G 的磁盘空间，运行起来也需要 7G 左右的内存，长期运行的话内存有 10G 会比较保险。CPU 的话 1 核也能跑，但给到 6 核会比较流畅。</p><h1 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h1><p>下载模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin</span><br></pre></td></tr></table></figure><p>这是一个处理过的模型，减小了体积和精度，专门为端侧运行做了优化，也不需要申请 Meta 的授权直接就能下载。</p><p>下载并安装 <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ggerganov/llama.cpp.git</span><br><span class="line"><span class="built_in">cd</span> llama.cpp</span><br><span class="line">make</span><br></pre></td></tr></table></figure><p>这是一个高性能的 Llama 推理工具集，可以方便的做 chat，api 等等服务。</p><p>移动模型文件到对应目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> ../llama-2-7b-chat.ggmlv3.q8_0.bin models/</span><br></pre></td></tr></table></figure><p>好了现在你就可以在本地运行一个 chat 了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./main -m ./models/llama-2-7b-chat.ggmlv3.q8_0.bin -c 512 -b </span><br><span class="line">1024 -n 256 --keep 48 \</span><br><span class="line">    --repeat_penalty 1.0 --color -i -t 4 \</span><br><span class="line">    -r <span class="string">&quot;User:&quot;</span> -f prompts/chat-with-bob.txt</span><br></pre></td></tr></table></figure><h1 id="更多配置"><a href="#更多配置" class="headerlink" title="更多配置"></a>更多配置</h1><p>在 examples 目录下还可以看到 llama.cpp 的其他用法，比如提供 API 服务，提供 embedding 和 fine-tune，甚至还有一个兼容 OpenAI API 的转换器。</p><p>如果你的机器有 GPU 或者用的是 M 系列芯片的 Mac 那么可以通过 make 参数提高推理的性能。</p><p>如果你的机器再厉害一些可以考虑去 huggingface 上去下载更大参数量的模型。</p><h1 id="为什么要本地运行大模型"><a href="#为什么要本地运行大模型" class="headerlink" title="为什么要本地运行大模型"></a>为什么要本地运行大模型</h1><p>其实我手头 OpenAI 的 GPT3&#x2F;GPT3，Google Bard，Claude 2 的 API 使用权都有，那为啥还要费心思在本地用 CPU 跑质量并不如他们的大模型呢？</p><p>第一个原因是穷，和我想做的事情相关。我想用大模型做代码分析和信息摘要，一个项目可能有上万个 commit 跑一遍就破产了。</p><p>第二个原因是穷，买不起 GPU 和 M 系列的 Mac，只能找穷人的方法。</p><p>第三个原因是我觉得未来端侧的大模型会更实用，也更能贴近个人的场景进行垂直方向的定制化，想借着这个机会看看这个领域到啥地步了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;之前觉得大模型在本地运行是个不可能的事情，我甚至没有一块 GPU。但是尝试了一下 llama.cpp 发现几分钟就把 Meta 最新开源的 Llama 2 在 CPU 上运行起来了，速度和质量都还可以接受，很多想法突然就变得可行了。记录一下搭建的过程，希望对感兴趣的人有帮助</summary>
      
    
    
    
    
    <category term="llama" scheme="http://oilbeater.com/tags/llama/"/>
    
    <category term="AI" scheme="http://oilbeater.com/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>新的网络性能优化技术 —— BIG TCP</title>
    <link href="http://oilbeater.com/2023/07/24/big-tcp/"/>
    <id>http://oilbeater.com/2023/07/24/big-tcp/</id>
    <published>2023-07-24T15:21:15.000Z</published>
    <updated>2025-04-14T17:16:50.277Z</updated>
    
    <content type="html"><![CDATA[<p>在翻 Cilium Release Blog 时发现了一个叫 Big TCP 的内核技术，这个技术今年 2 月左右刚被合并到内核。看介绍是专门为 100Gbit 以上的高速网络设计的，在将吞吐量提升 50% 的时候还能大幅降低延迟，于是就去了解了一下这个技术。</p><h1 id="高速网络的性能的瓶颈和误区"><a href="#高速网络的性能的瓶颈和误区" class="headerlink" title="高速网络的性能的瓶颈和误区"></a>高速网络的性能的瓶颈和误区</h1><p>提到网络性能，很多人会很自然的联想到网卡的性能，但是做过网络性能优化的人会知道瓶颈更多其实是在 CPU 上。一般使用 iperf3、qperf 这类压测软件进行测试，尤其在小包情况下基本是无法打满网卡带宽的，这时候 CPU 会先于网卡到达瓶颈。</p><p>因此做网络性能的优化通常做的都是 CPUU 相关的优化，不管是 DPDK，Offload，XDP 加速的原理要么是绕过内核栈，要么是卸载部分工作给网卡，本质上都是为了节省 CPU 资源，来处理更多的数据包。</p><p>以 100G 的网卡为例，以太网的 MTU 是 1500 字节，那么在不做任何优化的情况下 CPU 想要跑满网卡，每秒要处理将近 800 万个数据包，如果每个数据包都要走完整的网络栈，现代 CPU 单核还远远处理不了这个量级的数据包。</p><p>为了能让单核跑出尽可能好的网络性能，就需要内核和网卡驱动共同协作，将一部分工作 Offload 给网卡，这也就是 GRO(Generic Receive Offload) 和 TSO(TCP Segment Offload) 相关的技术。</p><h1 id="GRO-和-TSO"><a href="#GRO-和-TSO" class="headerlink" title="GRO 和 TSO"></a>GRO 和 TSO</h1><p>CPU 处理不了那么多数据的一个原因是需要给每个数据包封装协议头，计算校验和等等会浪费大量的资源，而在 100G 这种带宽下 1500 的 MTU 又实在太小了，不得不拆分出这么多小的数据包。如果将数据包的拆分和组装交给网卡来做，那么内核需要处理的数据包的规模就降下来了，这就是 GRO 和 TSO 提升性能的基本原理。</p><p><img src="/../images/tso-gro.png" alt="Alt text"></p><p>如上图所示，在发送数据包的时候内核可以按照 64K 来处理数据包，网卡通过 TSO 拆分成 1.5K 的小包进行传输，接收端网卡通过 GRO 将小包聚合成 64K 的大包在发送给内核处理。这样一来，内核需要处理包的数量就降为了原来的四十分之一。因此理论上开了这两个 Offload 网络的吞吐量在大包的情况下会有数十倍的提升，在我们之前的测试中这个性能大概会差十倍左右。</p><p>而且在主流内核和网卡驱动里只支持 TCP 的 Offload，所以经常会看到的一个不太符合直觉的现象就是 iperf3 的 TCP 性能要远远好于 UDP 的性能，差距大概也在十倍左右。</p><h1 id="Big-TCP-又做了什么？"><a href="#Big-TCP-又做了什么？" class="headerlink" title="Big TCP 又做了什么？"></a>Big TCP 又做了什么？</h1><p>既然已经有了这个 Offload 那么 Big TCP 又做了什么呢？Big TCP 要做的事情就是让内核里处理的这个数据包变的更大，这样整体要处理的数据包就会进一步下降来提升性能。之前内核处理的数据包大小为 64K 的主要限制在于 IP 包的头部有个长度字段，这个字段长度为 16bit，因此理论上一个 IP 包最大长度就是 64K。</p><p>怎样才能突破这个长度限制呢，内核的作者在这里用了一些很 hack 的方法，在 IPv6 的数据包中有一个 hop-by-hop 的 32 位字段可以存储一些附加信息，那么内核里就可以把 IP 包的长度设置为 0，然后从 hop-by-hop 字段中获取真实的数据包长度，这样一个数据包最大就可以到 4GB 的容量。但是处于稳妥的考虑，目前最大只能设置为 512K，即使这样要处理的数据包也变为了原来的八分之一，相比没有卸载的情况就是将近三百分之一。</p><p>根据开发者的<a href="https://lwn.net/Articles/883713/">测试</a>，吞吐量有将近 50% 的提升，延迟也有将近 30% 的下降，效果可以说相当显著了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;Standard&#x27;</span> performance with current (74KB) limits.</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..10&#125;; <span class="keyword">do</span> ./netperf -t TCP_RR -H iroa23  -- -r80000,80000 -O MIN_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT|<span class="built_in">tail</span> -1; <span class="keyword">done</span></span><br><span class="line">77           138          183          8542.19    </span><br><span class="line">79           143          178          8215.28    </span><br><span class="line">70           117          164          9543.39    </span><br><span class="line">80           144          176          8183.71    </span><br><span class="line">78           126          155          9108.47    </span><br><span class="line">80           146          184          8115.19    </span><br><span class="line">71           113          165          9510.96    </span><br><span class="line">74           113          164          9518.74    </span><br><span class="line">79           137          178          8575.04    </span><br><span class="line">73           111          171          9561.73    </span><br><span class="line"></span><br><span class="line">Now <span class="built_in">enable</span> BIG TCP on both hosts.</span><br><span class="line"></span><br><span class="line">ip <span class="built_in">link</span> <span class="built_in">set</span> dev eth0 gro_ipv6_max_size 185000 gso_ipv6_max_size 185000</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..10&#125;; <span class="keyword">do</span> ./netperf -t TCP_RR -H iroa23  -- -r80000,80000 -O MIN_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT|<span class="built_in">tail</span> -1; <span class="keyword">done</span></span><br><span class="line">57           83           117          13871.38   </span><br><span class="line">64           118          155          11432.94   </span><br><span class="line">65           116          148          11507.62   </span><br><span class="line">60           105          136          12645.15   </span><br><span class="line">60           103          135          12760.34   </span><br><span class="line">60           102          134          12832.64   </span><br><span class="line">62           109          132          10877.68   </span><br><span class="line">58           82           115          14052.93   </span><br><span class="line">57           83           124          14212.58   </span><br><span class="line">57           82           119          14196.01 </span><br></pre></td></tr></table></figure><p>而 <a href="https://lwn.net/Articles/920017/">Big TCP 的 IPv4 支持</a>要晚一些，主要原因是 IPv4 内并没有类似 IPv6 中 hop-by-hop 的可选信息字段，因此也就不能在 IP 头中保存真实长度信息。不过作者另辟蹊径，直接从内核的 skb-&gt;len 中计算数据包的真实长度信息，反正这个数据包只要在送出去之后长度是正确的就可以，在机器内部其实可以完全依赖 skb 保存状态信息，这样实现了超过 64K 的 TCP 数据包。理论上 IPv4 的做法会更通用，不过 IPv6 已经先实现了，所以就存在了两种不同实现 Big TCP 的方案。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这个技术对内核和硬件驱动都有一定的要求，内核需要 6.3 才正式支持，而网卡驱动的支持可能还需要联系硬件厂商。但 Big TCP 还是一个值得令人期待的技术，能够在不需要应用程序调整的情况下显著提升网络的性能，在特定场景下的收益还是很大的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在翻 Cilium Release Blog 时发现了一个叫 Big TCP 的内核技术，这个技术今年 2 月左右刚被合并到内核。看介绍是专门为 100Gbit 以上的高速网络设计的，在将吞吐量提升 50% 的时候还能大幅降低延迟，于是就去了解了一下这个技术。&lt;/p&gt;
&lt;h</summary>
      
    
    
    
    
    <category term="network" scheme="http://oilbeater.com/tags/network/"/>
    
    <category term="performance" scheme="http://oilbeater.com/tags/performance/"/>
    
    <category term="kernel" scheme="http://oilbeater.com/tags/kernel/"/>
    
  </entry>
  
  <entry>
    <title>Golang 中预分配 slice 内存对性能的影响</title>
    <link href="http://oilbeater.com/2023/07/19/pre-alloc-slice-for-golang/"/>
    <id>http://oilbeater.com/2023/07/19/pre-alloc-slice-for-golang/</id>
    <published>2023-07-19T10:48:09.000Z</published>
    <updated>2025-04-14T17:16:50.277Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#slice-%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80">Slice 内存分配理论基础</a></li><li><a href="#%E5%AE%9A%E9%87%8F%E6%B5%8B%E9%87%8F">定量测量</a></li><li><a href="#lint-%E5%B7%A5%E5%85%B7-prealloc">Lint 工具 prealloc</a></li><li><a href="#%E6%80%BB%E7%BB%93">总结</a></li></ul><p>在我代码 review 的过程中经常会关注代码里 slice 的初始化是否分配了预期的内存空间，也就是凡是 <code>var init []int64</code> 的我都会要求尽可能改成 <code>init := make([]int64, 0, length)</code> 格式。但是这个改进对性能究竟有多少影响并没有什么定量的概念，只是教条的去要求。这篇博客会介绍一下预分配内存提升性能的理论基础，定量测量，和自动化检测发现的工具。</p><h1 id="Slice-内存分配理论基础"><a href="#Slice-内存分配理论基础" class="headerlink" title="Slice 内存分配理论基础"></a>Slice 内存分配理论基础</h1><p>Golang Slice 扩容的代码在<a href="https://github.com/golang/go/blob/go1.20.6/src/runtime/slice.go#L157">slice.go 下的 growslice</a>。大体思路是在 Slice 容量小于 256 时<br>每次扩容会创建一个容量翻倍的新 slice；当容量大于 256 后，每次扩容会创建一个容量为原先的 1.25 倍的新 slice。之后会将旧 slice 的数据复制到新的 slice，最终返回新的 slice。</p><p>扩容的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">newcap := oldCap</span><br><span class="line">doublecap := newcap + newcap</span><br><span class="line"><span class="keyword">if</span> newLen &gt; doublecap &#123;</span><br><span class="line">newcap = newLen</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">const</span> threshold = <span class="number">256</span></span><br><span class="line"><span class="keyword">if</span> oldCap &lt; threshold &#123;</span><br><span class="line">newcap = doublecap</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// Check 0 &lt; newcap to detect overflow</span></span><br><span class="line"><span class="comment">// and prevent an infinite loop.</span></span><br><span class="line"><span class="keyword">for</span> <span class="number">0</span> &lt; newcap &amp;&amp; newcap &lt; newLen &#123;</span><br><span class="line"><span class="comment">// Transition from growing 2x for small slices</span></span><br><span class="line"><span class="comment">// to growing 1.25x for large slices. This formula</span></span><br><span class="line"><span class="comment">// gives a smooth-ish transition between the two.</span></span><br><span class="line">newcap += (newcap + <span class="number">3</span>*threshold) / <span class="number">4</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Set newcap to the requested cap when</span></span><br><span class="line"><span class="comment">// the newcap calculation overflowed.</span></span><br><span class="line"><span class="keyword">if</span> newcap &lt;= <span class="number">0</span> &#123;</span><br><span class="line">newcap = newLen</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因此理论上如果预分配好 slice 的容量，不需要动态扩张我们可以在好几个地方有性能的提升：</p><ol><li>内存只需要一次分配，不需要反复分配。</li><li>不需要反复进行数据复制。</li><li>不需要反复对旧的 slice 进行垃圾回收。</li><li>内存准确分配，不存在动态分配导致的容量浪费。</li></ol><p>理论上来看，预分配 slice 容量相比动态分配会带来性能提升，但具体提升有多少就需要定量测量了。</p><h1 id="定量测量"><a href="#定量测量" class="headerlink" title="定量测量"></a>定量测量</h1><p>我们参考 <a href="https://github.com/alexkohler/prealloc/blob/master/prealloc_test.go">prealloc</a> 的代码进行简单修改来测量不同容量的 slice 预分配和动态分配对性能的影响。</p><p>测试代码如下，通过修改 <code>length</code> 可以观察不同情况下的性能数据：</p><figure class="highlight go"><figcaption><span>title</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> prealloc_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;testing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> length = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">b.ResetTimer()</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line"><span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line"><span class="keyword">var</span> init []<span class="type">int64</span></span><br><span class="line"><span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">init = <span class="built_in">append</span>(init, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">b.ResetTimer()</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line"><span class="comment">// Preallocate our initial slice</span></span><br><span class="line">init := <span class="built_in">make</span>([]<span class="type">int64</span>, <span class="number">0</span>, length)</span><br><span class="line"><span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">init = <span class="built_in">append</span>(init, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一个函数测试动态分配的性能，第二个函数测试预分配的性能。通过下面的命令可以执行测试：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go <span class="built_in">test</span> -bench=. -benchmem prealloc_test.go</span><br></pre></td></tr></table></figure><p>在 <code>length = 1</code> 情况下的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocate-12       40228154                27.36 ns/op            8 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12         55662463                19.97 ns/op            8 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>在 <code>length</code> 为 1 的情况下，理论上动态分配和静态分配都要进行一次初始化的内存分配，性能不应该有差异，但是实测下来，预分配的耗时为动态分配的 70%，即使在两者内存分配次数一直的情况下，预分配依然有 1.4x 的性能优势。目测性能提升和变量的连续分配相关。</p><p>在 <code>length = 10</code> 情况下的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocate-12        5402014               228.3 ns/op           248 B/op          5 allocs/op</span><br><span class="line">BenchmarkPreallocate-12         21908133                50.46 ns/op           80 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>在 &#96;length&#96;&#96; 为 10 的情况下，预分配依然只进行了一次性能分配，动态分配进行了 5 次性能分配，预分配的性能是动态分配性能的 4 倍。可见即使在 slice 规模较小的时候，预分配依然会有比较明显的性能提升。</p><p>下面是在 <code>length</code> 分别为 129,1025 和 10000 情况下的测试结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># length = 129</span></span><br><span class="line">BenchmarkNoPreallocate-12         743293              1393 ns/op            4088 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocate-12          3124831               386.1 ns/op          1152 B/op          1 allocs/op</span><br><span class="line"></span><br><span class="line"><span class="comment"># length = 1025</span></span><br><span class="line">BenchmarkNoPreallocate-12         169700              6571 ns/op           25208 B/op         12 allocs/op</span><br><span class="line">BenchmarkPreallocate-12           468880              2495 ns/op            9472 B/op          1 allocs/op</span><br><span class="line"></span><br><span class="line"><span class="comment"># length = 10000</span></span><br><span class="line">BenchmarkNoPreallocate-12          14430             86427 ns/op          357625 B/op         19 allocs/op</span><br><span class="line">BenchmarkPreallocate-12            56220             20693 ns/op           81920 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>在更大容量下，静态分配依然只做一次内存分配，但是性能提升并没有相应成倍增长，整体性能会是动态分配的 2 到 4 倍。应该是在这个过程中有一些其他的消耗，或者 golang 对大容量的复制有特殊的优化，因此性能差距并没有拉大。</p><p>当把 slice 的内容换成更复杂的 struct 时，原以为复制会带来更大的性能开销，但实测复杂 struct 预分配和动态分配的性能差距反而更小，看上去还是有很多内部的优化，表现和直觉并不一致。</p><h1 id="Lint-工具-prealloc"><a href="#Lint-工具-prealloc" class="headerlink" title="Lint 工具 prealloc"></a>Lint 工具 prealloc</h1><p>尽管预分配内存可以带来一定的性能提升，但是在比较大的项目中完全依赖人工去 review 这个问题很容易出现纰漏。这时候就需要用到一些 lint 工具来自动做代码扫描了。<a href="https://github.com/alexkohler/prealloc">prealloc</a> 就是这样一个工具可以扫描潜在的能够预分配但却没有预分配的 slice，并且可以整合到 <a href="https://golangci-lint.run/usage/linters/#prealloc">golangci-lint</a> 中。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>整体来看 slice 的内存预分配是个比较简单但却能有比较好优化效果的方法，即使在 slice 容量很小的情况下，预分配依然能有比较明显的性能提升。通过 prealloc 这种静态代码扫描工具，可以比较方便的实现这类潜在优化的检测并集成到 CI 中简化日后的操作。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#slice-%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80&quot;&gt;Slice 内存分配理论基础&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5</summary>
      
    
    
    
    
    <category term="性能" scheme="http://oilbeater.com/tags/%E6%80%A7%E8%83%BD/"/>
    
    <category term="golang" scheme="http://oilbeater.com/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>无需改动代码的性能优化方法 —— PGO</title>
    <link href="http://oilbeater.com/2023/06/24/optimization-without-changing-code-pgo/"/>
    <id>http://oilbeater.com/2023/06/24/optimization-without-changing-code-pgo/</id>
    <published>2023-06-24T08:58:29.000Z</published>
    <updated>2025-04-14T17:16:50.277Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#pgo-in-general">PGO in General</a></li><li><a href="#pgo-in-golang">PGO in Golang</a><ul><li><a href="#%E6%94%B6%E9%9B%86-profile-%E4%BF%A1%E6%81%AF">收集 Profile 信息</a></li><li><a href="#%E7%BC%96%E8%AF%91%E4%BC%98%E5%8C%96">编译优化</a></li></ul></li><li><a href="#pgo-for-kernel">PGO for Kernel</a></li><li><a href="#%E6%80%BB%E7%BB%93">总结</a></li></ul><p>Golang 在 1.20 引入了 <a href="https://go.dev/doc/pgo">PGO(Profile-guided optimization)</a> 的优化，根据官方博客的介绍可以在不更改代码的情况下达到 2%-7% 的性能提升。在 1.21 的 <a href="https://tip.golang.org/doc/go1.21">Release Note</a> 中，Golang 将该功能升级到 GA 并在自己的构建中开启了 PGO，将 Golang 自身编译器的性能提升了 2%-4%。PGO 本身是个编译器的优化方法，并不和特定语言相关，Golang 目前也只是很初步的应用了 PGO，这篇文章将会以 Golang 为例介绍 PGO 的工作原理，可能的优化方向和一个应用 PGO 优化 Linux Kernel 的例子。</p><h1 id="PGO-in-General"><a href="#PGO-in-General" class="headerlink" title="PGO in General"></a>PGO in General</h1><p>通常我们认为静态编译型语言的运行性能要好于动态解释型的语言，但是随着 <a href="https://en.wikipedia.org/wiki/Just-in-time_compilation">JIT(Just In Time)</a> 技术的引入，动态语言性能有了大幅提升，因为 JIT 可以根据运行时的情况进行针对的强化。</p><p>例如：</p><ul><li>热点代码编译成机器码</li><li>函数内联</li><li>分支预测</li><li>循环展开</li><li>类型推断</li><li>内存分配优化</li><li>寄存器优化</li></ul><p>JIT 通过一系列运行时的优化可以达到和编译型语言接近的性能，在部分情况下由于可以动态根据情况做一些编译期间无法确认的优化，甚至会有比编译型语言更好的性能。例如 Golang 中的函数内联是写死的规则稍，主要看函数大小，而不是函数使用频率。分支预测和循环展开编译器由于不知道分支运行时的频率分布，也无法做专门的优化。Golang 中 Slice 和 Map 的初始化大小需要通过参数手动指定或者自动根据写死的规则进行扩容，无法根据运行时信息分配一个合适的初始化大小。</p><p>于是一个很自然的想法就是能否将类似 JIT 的技术应用到编译型语言，通过运行时的信息去优化代码性能，传统的做法是通过人类对代码运行时理解的经验，主动的在代码中加入编译提示信息，帮助编译器优化，例如 C++ 中的 inline 函数，C 语言中的宏，GCC 提供的 likely&#x2F;unlikely。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> likely(x) __builtin_expect(!!(x), 1) <span class="comment">//gcc内置函数, 帮助编译器分支优化</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> unlikely(x) __builtin_expect(!!(x), 0)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>* argv[])</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> x = <span class="number">0</span>;</span><br><span class="line">    x = <span class="built_in">atoi</span>(argv[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">unlikely</span>(x == <span class="number">3</span>))&#123;  <span class="comment">//告诉编译器这个分支非常不可能为true</span></span><br><span class="line">        x = x + <span class="number">9</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        x = x - <span class="number">8</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;x=%d\n&quot;</span>, x);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这种方式存在几个问题：</p><ol><li>需要对所有分支，函数进行考虑，工作量大，并且调整复杂。</li><li>需要依赖程序员凭借经验推测，不一定符合实际场景。</li><li>只能对自己编写的代码进行调整比较方便，一些依赖的库函数调整困难，比如一个应用开发者可能需要向下调整到 glibc 的代码。</li></ol><p>那么能不能有一种方法可以不调整代码，自动根据运行时的信息对代码的编译进行全局的优化呢，这个方法就是 PGO。通过收集运行时的 Profile 信息反过来优化编译过程。</p><p>一个典型的使用 PGO 的工作流分为下面几步：</p><ol><li>构建初始版本二进制，不带任何 PGO 的优化。</li><li>在生产环境收集 Profile 信息。</li><li>重新构建二进制，并使用 2 中收集的 Profile 信息进行构建优化。</li><li>回到 2，持续迭代。</li></ol><p>Google 通过 <a href="https://research.google/pubs/pub45290/">AutoFDO</a> 实现了持续的 PGO，同时也介绍了 PGO 存在的问题：</p><ol><li>需要配合 pprof 信息，生产环境完整的 Profile 会带来性能下降，通常使用 Sample 的方式牺牲一定精度，开销在 1%。</li><li>需要随着代码动态调整 Profile，不是个一次性的优化，Profile 信息可能会有泄密隐患，如何构建完整流程是个挑战。</li><li>整体性能提升有限，整体性能优化在 10% 左右。</li><li>二进制体积增加。</li></ol><h1 id="PGO-in-Golang"><a href="#PGO-in-Golang" class="headerlink" title="PGO in Golang"></a>PGO in Golang</h1><p>Golang 目前有两个主流的编译器，gc(go compiler) 和 <a href="https://github.com/golang/gofrontend">gccgo</a>。</p><table><thead><tr><th></th><th>gc</th><th>gccgo</th></tr></thead><tbody><tr><td>优点</td><td><ol>1. 官方支持</ol><ol>2. 兼容性好</ol><ol>3. 编译速度较快</ol></td><td><ol>1. 可以使用 GCC 更多优化能力，性能较好</ol><ol>2. 可以支持更多 CPU 架构和系统</ol></td></tr><tr><td>缺点</td><td><ol>1. 优化较为保守</ol></td><td><ol>1. 跟随 GCC 发版，不支持 Golang 新特性，潜在兼容性问题</ol><ol>2. 安装使用复杂</ol><ol>3. 较慢编译速度</ol></td></tr></tbody></table><p>gccgo 使用前后端分离架构，后端已经支持 PGO，这里主要讨论的是官方 gc 的 PGO 优化。</p><h2 id="收集-Profile-信息"><a href="#收集-Profile-信息" class="headerlink" title="收集 Profile 信息"></a>收集 Profile 信息</h2><p>Golang 的 PGO 目前只支持通过 CPU Profile 进行优化，我们可以通过 Golang 标准库里的 runtime&#x2F;pprof 或者 net&#x2F;http&#x2F;pprof 进行 CPU Profile 的采集，如果是其他的 Profile 采集器的数据如果能转换成 <a href="https://github.com/google/pprof/tree/main/proto">Google pprof</a> 的格式也可以兼容。</p><p>需要注意的是由于 PGO 使用的是类似 JIT 的优化方式，因此最好在真实的生产环境收集 Profile 信息，才能最贴合程序实际的运行条件，方便进行后期的优化。也可将多台机器上收集的 Profile 信息进行合并 <code>go tool pprof -proto a.pprof b.pprof &gt; merged.pprof</code>.</p><h2 id="编译优化"><a href="#编译优化" class="headerlink" title="编译优化"></a>编译优化</h2><p><code>go build -gpo=/tmp/foo.pprof</code> 即可在编译过程中通过 Profile 信息进行优化。Golang 目前针只实现了<a href="https://go-review.googlesource.com/c/proposal/+/430398/10/design/55022-pgo-implementation.md#208">函数内联优化</a>，尝试将调用比例大于 2% 的函数进行内联。更多的优化还在画大饼，目前画的大饼可以参考 <a href="https://github.com/golang/go/issues/55022#issuecomment-1245605666%E3%80%82">https://github.com/golang/go/issues/55022#issuecomment-1245605666。</a></p><p><img src="/../images/pgo-in-go.png" alt="Alt text"></p><p>在实践中经常碰到的场景是在跑完 profile 后不会直接重新编译，而是在下次代码变更后再编译发布。这样带来的问题就是 Profile 的信息和代码是存在差异的，Golang 目前通过一些启发式的规则，可以在代码和 Profile 不一致的情况下尽可能的工作。</p><h1 id="PGO-for-Kernel"><a href="#PGO-for-Kernel" class="headerlink" title="PGO for Kernel"></a>PGO for Kernel</h1><p>PGO 一般用于应用的性能优化，但是一些服务器上的应用有可能是系统调用密集型的，在这种情况下其实可以根据应用对内核进行 PGO，打造一个针对应用优化过后的内核。这里可以参考一下早些年我上学时实验室一个大神师弟的工作 <a href="http://coolypf.com/kpgo.htm">Profile-Guided Operating System Kernel Optimization</a>，我当时也是通过这个工作才了解到了 PGO。</p><p><img src="/../images/kernel-pgo.png" alt="Alt text"></p><p>图片里显示的是不同软件在优化后的吞吐量提升，可以看到在完全不用动应用代码，只是优化内核的编译，吞吐量就会有 2%~10% 的提升。在 Nginx 的例子里出现了性能的下降，我记的当时大神说过是 GCC 在 PGO 的一个 Bug，导致了错误的优化。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>PGO 是一个不需要改动代码就可以获得性能优化的方法，结合 Golang 的 pprof 可以做很好的配合，但是优化程度也相应有限，而且需要配合发布上线流程，可以作为一个性能优化的尝试。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#pgo-in-general&quot;&gt;PGO in General&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#pgo-in-golang&quot;&gt;PGO in Golang&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%94%B6%E9%9B%</summary>
      
    
    
    
    
    <category term="性能" scheme="http://oilbeater.com/tags/%E6%80%A7%E8%83%BD/"/>
    
    <category term="golang" scheme="http://oilbeater.com/tags/golang/"/>
    
    <category term="PGO" scheme="http://oilbeater.com/tags/PGO/"/>
    
    <category term="Linux" scheme="http://oilbeater.com/tags/Linux/"/>
    
    <category term="Kernel" scheme="http://oilbeater.com/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>我和耗子叔的回忆</title>
    <link href="http://oilbeater.com/2023/05/19/memory-with-haochen/"/>
    <id>http://oilbeater.com/2023/05/19/memory-with-haochen/</id>
    <published>2023-05-19T00:05:02.000Z</published>
    <updated>2025-04-14T17:16:50.277Z</updated>
    
    <content type="html"><![CDATA[<h1 id="我和耗子叔的回忆"><a href="#我和耗子叔的回忆" class="headerlink" title="我和耗子叔的回忆"></a>我和耗子叔的回忆</h1><h2 id="初始酷壳"><a href="#初始酷壳" class="headerlink" title="初始酷壳"></a>初始酷壳</h2><p>和很多人一样，第一次接触耗子叔是通过 coolshell.cn 博客。</p><p>当时印象最深，对我之后影响最大的应该是 <a href="https://coolshell.cn/articles/2105.html">分享：我的READER订阅</a> 。当年尽管我已经是大三的计算机专业的学生，但是课本外的专业知识还是相当差劲，也不知道哪里才能获得一手的信息和资料。让我一个象牙塔里的学生，接触到工业界最先进的技术就是从这个列表开始的。这个 RSS 的列表陪伴我度过了很多年，很长时间我的计算机技术相关的信息都是依赖这个列表。</p><h2 id="亚马逊的擦肩而过"><a href="#亚马逊的擦肩而过" class="headerlink" title="亚马逊的擦肩而过"></a>亚马逊的擦肩而过</h2><p>几年后在我实习的时候，碰巧去了亚马逊中国耗子叔在的那个部门，不过当时耗子叔已经去阿里了，没能在亚马逊碰到。实习结束后我专门写了篇博客记录当时在亚马逊实习的经历，抱着蹭流量的心态我在微博发博客的时候 @ 了一下耗子叔。本来是抱着蹭到就算赚的心态，没想到耗子叔很快就转发了，当时博客流量就上来了，我还拿出去和同学们炫耀了一段。开始写博客的初衷也是受耗子叔博客的影响，这个博客几乎是我早些年工作没啥成就时，唯一能拿得出手的东西了。</p><p><img src="/../images/image1.png" alt="image1.png"></p><h2 id="从阿里到-Docker"><a href="#从阿里到-Docker" class="headerlink" title="从阿里到 Docker"></a>从阿里到 Docker</h2><p>第一份工作去了阿里，总算是和耗子叔到了一个公司，但是部门差的太远，没有什么业务的交叉，起初也就没什么交流。当时 Docker 开始崭露头角了，阿里内部各个部门都有一拨人都看到了这个技术的突破性，想在阿里内部推广这个技术，几个不同部门的人组了个群聊这些事，我和耗子叔凑巧也在这个群里。当时我还是个应届生，肯定掀不起什么风浪，但其他高级别的人也都碰到了不少阻碍，所以不断有人跳出去单干，我们也戏称这个群变成了前橙会群。</p><p>在当时阿里内部大领导已经拍板不会推进 Docker 了，耗子叔在这个过程中也遇到了不少风波，里面涉及到很多复杂的人和事这里就不多说了。内部做不了，我当时其实也有了去找个创业公司做 Docker 的想法。</p><p>作为社恐的我那天还是鼓足勇气和耗子叔私聊了这件事，耗子叔还是出乎我意料的快速的就恢复我了。在谈到容器在阿里的时候耗子叔情绪还是挺激动的，觉得阿里做事不地道，在阿里搞 Docker 已经不可能了，说了很多的气话。然后我再提及了想去外面的创业公司做 Docker 的事情，本以为情绪都烘到这个地步了，应该会和我再吐槽几句阿里是垃圾，让我赶快去外面吧。结果耗子叔话头一转，开始劝我在阿里先稳住，有一定积累再去看外面的机会。</p><p><img src="/../images/image2.png" alt="image2.png"></p><p>我在那一刻突然意识到，耗子叔是那种我之前很少没见过的，并不是以公司立场甚至不是以个人的立场去考虑问题，而是真的完全出于为你好的立场去考虑问题。而我当时其实只是想找个大牛去肯定我的一个决策。</p><p>之后我还是去了那个创业公司，一直到现在，今天回过来看，如果从经济的角度那我当年还是应该听话的。鬼知道阿里转身就是 All in 云原生。</p><h2 id="多年后的合作"><a href="#多年后的合作" class="headerlink" title="多年后的合作"></a>多年后的合作</h2><p>之后很长一段时间和耗子叔都没什么联系，最近几年我开始做开源项目，一直不瘟不火，不知道该怎么去推广。这时候耗子叔也开始创业了，并且把公司内部的技术分享放到网上，我就又萌生了去蹭流量的想法。说来惭愧，这么多年都没帮过什么忙，每次一有事情还都想着去蹭。另一方面耗子叔创业也在云原生领域，所谓同行是冤家，了解这个圈子的应该知道这个圈子友商之间交情都挺差的，见面不打架就算客气了，我这还想明目张胆的去蹭流量。</p><p>本着蹭到就算赚我还是发了微信，想在他们的分享会上分享一下我们最近的技术进展。耗子叔依然出乎我意料快速的就答应了我，让然后就张罗起来了。</p><p><img src="/../images/image3.png" alt="image3.png"></p><p>尽管很多年都没联系，但耗子叔像老朋友一样就要和我语音聊聊，一聊就聊了一个半小时。从云原生这个领域来说，我们俩几乎是同时进入的，要说正儿八经当职业来做我还更早一些，我本来以为自己还算老兵了，尤其在网络这个领域我也还算可以，但耗子叔的很多观点还是让我觉得自己狭隘了。</p><p>我们聊到当时为了让 kube-ovn 这个项目能被更多人使用，我做了很多在云原生领域并不推荐的功能，让更多非云原生最佳实践也能跑起来，这个成为了我们早期吸引用户的最大卖点。耗子叔很直接的指出我的做法过于功利了，快速的获取用户变成了我最优先的目标，为了完成这个目标我其实在鼓励用户使用并不符合最佳实践的做法。虽然我的做法能吸引用户，但是并没有让用户发挥出云原生最大的价值，这种形式上的云原生最终并不会产生业务上的收益。如果是他的话，他会劝说用户尽可能使用最佳实践，而他所有产品功能也都是依托最佳实践基础来做的，尽管初期改造后会很痛苦，但是改造后能够确定性的有大幅度的改善，而不是为了云原生而云原生。</p><p>说实话，在之前我一直觉得云原生领域已经挺无聊的了，东西就是那些，每天都是各种和现实磨合，甚至都忘了当初是为啥要加入这个领域。大概是理想多次被现实扭曲，所以才变得这么功利吧。</p><p>耗子叔之后介绍了当时 MegaEase 的愿景，当时在外面能看到的还只有一个网关，但背后其实有个更大的梦想。耗子叔希望透过流量入口和云原生的技术把复杂的多云管理变的简单，通过云原生可以做一个跨公有云的系统，符合这个最佳实践的应用，存储、网络、计算都可以在多个云间快速的迁移。这样可以把多个公有云变成自己的资源池，把价格给打下来，给用户提供更优质低价的云服务。</p><p>当时听完后我是有点懵的，技术上我是听懂了的，但是还是被里面的一些有想象力的做法惊住了。耗子叔看气氛差不多了就问我，你们公司现在做的产品我看过了，挺没意思的，也不咋地，你要不要过来和我干这个？我内心 os 了一句你知道做这个不咋地的东西我有多努力嘛？嘴上说那等我把这个公司做黄了再去你们那吧。耗子叔听完哈哈一笑，说这倒也不至于，不至于。</p><p>那时候我们都还以为会有合作的机会，没想到之后再也不会有机会了。</p><h2 id="回想"><a href="#回想" class="headerlink" title="回想"></a>回想</h2><p>回想这些年来和耗子叔的交往，每次都是我有求于他，每次我多觉得这是不是太占用他宝贵的资源了。但是每次都会得到他毫不迟疑的，全情投入的帮助。他对技术理想的执着，对有着技术热情人的支持，只需要只言片语就深深的感染了我，让我在现实的阴霾中又看到了理想的光辉。如果我们每个人做事的时候想想耗子叔哲哲情况会怎么做，遇到有人需要帮助时想想如果是耗子叔会怎样帮我，那我想耗子叔不只有数字分身，也会在我们每个人的精神世界中有个分身。我想说他是个高尚的人，纯粹的人，在搜这句话出处是发现出自《纪念白求恩》，里面的一些话似乎正是再说耗子叔，就让这段引文作为这个纪念的结尾吧。</p><blockquote><p>从前线回来的人说到白求恩，没有一个不佩服，没有一个不为他的精神所感动。晋察冀边区的军民，凡亲身受过白求恩医生的治疗和亲眼看过白求恩医生的工作的，无不为之感动。每一个共产党员，一定要学习白求恩同志的这种真正共产主义者的精神。<br>白求恩同志是个医生，他以医疗为职业，对技术精益求精；在整个八路军医务系统中，他的医术是很高明的。这对于一班见异思迁的人，对于一班鄙薄技术工作以为不足道、以为无出路的人，也是一个极好的教训。<br>我和白求恩同志只见过一面。后来他给我来过许多信。可是因为忙，仅回过他一封信，还不知他收到没有。对于他的死，我是很悲痛的。现在大家纪念他，可见他的精神感人之深。我们大家要学习他毫无自私自利之心的精神。从这点出发，就可以变为大有利于人民的人。一个人能力有大小，但只要有这点精神，就是一个高尚的人，一个纯粹的人，一个有道德的人，一个脱离了低级趣味的人，一个有益于人民的人。 </p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;我和耗子叔的回忆&quot;&gt;&lt;a href=&quot;#我和耗子叔的回忆&quot; class=&quot;headerlink&quot; title=&quot;我和耗子叔的回忆&quot;&gt;&lt;/a&gt;我和耗子叔的回忆&lt;/h1&gt;&lt;h2 id=&quot;初始酷壳&quot;&gt;&lt;a href=&quot;#初始酷壳&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    
  </entry>
  
</feed>
