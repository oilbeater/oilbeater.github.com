<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Oilbeater 的自习室</title>
  <icon>http://oilbeater.com/icon.png</icon>
  
  <link href="http://oilbeater.com/atom.xml" rel="self"/>
  
  <link href="http://oilbeater.com/"/>
  <updated>2025-01-12T08:28:37.261Z</updated>
  <id>http://oilbeater.com/</id>
  
  <author>
    <name>Oilbeater</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>从 Network Binding Plugin 看 KubeVirt 的扩展方式</title>
    <link href="http://oilbeater.com/2025/01/12/kubevirt-network-binding/"/>
    <id>http://oilbeater.com/2025/01/12/kubevirt-network-binding/</id>
    <published>2025-01-12T08:16:07.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<p>在 KubeVirt v1.4 的新版本里将 <a href="https://kubevirt.io/user-guide/network/network_binding_plugins/">Network Binding Plugin</a> 提升到了 Beta，提供了一种新的扩展 KubeVirt 网络的方式。虽然名义上是为了扩展网络的能力，但实际上从实现上来看，这个机制能做的事情远不止网络，所有和 libvirt domain xml 相关的变更都可以通过这个机制来实现。</p><h2 id="KubeVirt-Network-Overview"><a href="#KubeVirt-Network-Overview" class="headerlink" title="KubeVirt Network Overview"></a>KubeVirt Network Overview</h2><p>先从网络的角度来看下 KubeVirt 之前的网络机制有什么问题，新的机制又是如何进行扩展的。</p><p>由于 KubeVirt 使用的是 Pod 里面跑 VM 的架构，所以复用了 CNI 的网络机制。这样的话就将网络分成了两个部分，一个是 Pod 网络由各个 CNI 提供。另一部分就是如何将 CNI 提供的网络接入 VM，在 libvirt 里这部分叫做 Domain 网络。</p><p>KubeVirt 之前的各种网络机制（Bridge， Masquerade， Passt， Slirp）所做的事情就是通过不同的技术方案将 Pod 里的 eth0 接入到 VM 的 tap0 网卡。例如 Bridge 将 tap0 和 eth0 接入到同一个网桥，Masquerade 将 tap0 的流量经过 iptables nat 规则导入 eth0，Passt 和 Slirp 通过用户态的网络栈做流量重定向。</p><p><img src="/../images/kubevirt-networking-tradition.png" alt="alt text"></p><p>这些方法在实现上都是类似的，在 Pod 内部做一些网络相关的配置，然后修改 libvirt 的启动参数接入对应的网络。但是现有的机制都是写死在 KubeVirt Core 里的，并没有扩展机制，想要新增一种机制或者修改已有的机制都需要修改 KubeVirt 的代码很不灵活，例如默认的 bridge 插件会劫持 DHCP 请求，但是又不支持 IP, 所以 bridge 模式下的双栈就很难实现，而 Kube-OVN 中已经实现的 DHCP 又被这个机制绕过去了，之前想做 bridge 的双栈就需要改 KubeVirt 的代码来关闭默认的 DHCP 十分麻烦。因此新版本中将这套机制抽象出来提供了一套通用的机制。</p><h2 id="Hook-Sidecar"><a href="#Hook-Sidecar" class="headerlink" title="Hook Sidecar"></a>Hook Sidecar</h2><p>先来看一种在 KubeVirt 中已经存在的扩展机制 <a href="https://kubevirt.io/user-guide/user_workloads/hook-sidecar/">Hook Sidecar</a>。</p><p>这套机制是在 VM 正式创建前，可以加载一个用户自定义的镜像，或者一段 ConfigMap 里保存的 Shell 或者 Python 脚本，来修改 VM 启动前 libvirt 的启动参数和 cloud-init 参数。</p><p>它的执行机制和 CNI 有些类似，virt-handler 在启动 VM 前会去对应目录寻找 <code>/usr/bin/onDefineDomain</code> 和 <code>/usr/bin/preCloudInitIso</code> 两个二进制文件，前者传入 virt-handler 生成的 libvirt XML 配置，返回修改后的配置；后者传入 cloudInit 配置，返回修改后的 cloudInit 配置。这样的话所有 KubeVirt 本身不支持的 libvirt 和 cloudInit 参数都可以通过这种机制来注入修改。并且由于 Sidecar 内实际可以执行任意代码，所能做的事情远不止修改这两个配置，所有初始化阶段 KubeVirt 没有实现的能力其实都可以在这里来实现。</p><h2 id="Network-Binding-Plugin"><a href="#Network-Binding-Plugin" class="headerlink" title="Network Binding Plugin"></a>Network Binding Plugin</h2><p>现在可以到 Network Binding Plugin 这个机制了，这个机制其实和 Hook Sidecar 基本上大同小异。主要区别是将二进制调用改成了 gRPC 调用，gRPC 里注册的方法还是  <code>onDefineDomain</code> 和 <code>preCloudInitIso</code> 参数传递从命令行参数改为了 gRPC Request 里的参数，其他都是一样的。</p><p>具体的例子可以参考目前还在 KubeVirt 代码里的 <a href="https://github.com/kubevirt/kubevirt/tree/main/cmd/sidecars/network-slirp-binding">Slirp Binding</a> 的实现。尽管在 Network Binding Plugin 的规范里还增加了<code>networkAttachmentDefinition</code> 字段可以选择一个 CNI，但这个其实使用之前的网卡选择机制也能实现，甚至由于 Sidecar 里可以执行任意代码，在里面再实现一个 CNI 覆盖 Pod 原先的网络也是可以的。</p><p>那么之后的网络架构就变成了下图这样：</p><p><img src="/../images/networking-binding.png" alt="alt text"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>虽然 Network Binding Plugin 的机制是为 Network 扩展准备的，但实际上几乎可以扩展所有 KubeVirt 在 virt-handler 侧的处理逻辑。甚至可以把 KubeVirt 也只当一个框架，所有的逻辑都通过 Sidecar 来处理，相信未来可以玩出不少花活来。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://kubevirt.io/user-guide/user_workloads/hook-sidecar/">https://kubevirt.io/user-guide/user_workloads/hook-sidecar/</a></li><li><a href="https://kubevirt.io/user-guide/network/network_binding_plugins/">https://kubevirt.io/user-guide/network/network_binding_plugins/</a></li><li><a href="https://github.com/kubevirt/kubevirt/blob/main/docs/network/network-binding-plugin.md">https://github.com/kubevirt/kubevirt/blob/main/docs/network/network-binding-plugin.md</a></li><li><a href="https://github.com/kubevirt/kubevirt/tree/main/cmd/sidecars/network-slirp-binding">https://github.com/kubevirt/kubevirt/tree/main/cmd/sidecars/network-slirp-binding</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在 KubeVirt v1.4 的新版本里将 &lt;a href=&quot;https://kubevirt.io/user-guide/network/network_binding_plugins/&quot;&gt;Network Binding Plugin&lt;/a&gt; 提升到了 Beta，提供了</summary>
      
    
    
    
    
    <category term="kubevirt" scheme="http://oilbeater.com/tags/kubevirt/"/>
    
    <category term="networking" scheme="http://oilbeater.com/tags/networking/"/>
    
  </entry>
  
  <entry>
    <title>加速容器镜像下载：从缓存到按需加载</title>
    <link href="http://oilbeater.com/2024/10/31/docker-pull/"/>
    <id>http://oilbeater.com/2024/10/31/docker-pull/</id>
    <published>2024-10-31T11:59:25.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<p>在容器的启动过程中，镜像下载速度往往是影响启动速度的最主要因素，通常占据了启动时间的 70% 以上。特别是对于体积庞大的 VM、AI 镜像，它们的大小可能达到数十 GB，导致下载和解压速度都成为启动的瓶颈。本文将探讨镜像下载的主要瓶颈、常见的优化方案以及最新的按需加载技术，以加速容器启动。</p><h3 id="镜像下载速度慢的原因"><a href="#镜像下载速度慢的原因" class="headerlink" title="镜像下载速度慢的原因"></a>镜像下载速度慢的原因</h3><p>容器镜像下载慢的原因主要有以下几点：</p><ul><li><strong>镜像体积过大</strong>：VM、AI 镜像体积通常较大，可能达到数十 GB，使得下载时间显著。</li><li><strong>gzip 解压耗时</strong>：特别是在内网环境中，解压时间往往远高于网络传输时间，导致解压成为新的瓶颈。</li></ul><h3 id="常见的镜像优化思路"><a href="#常见的镜像优化思路" class="headerlink" title="常见的镜像优化思路"></a>常见的镜像优化思路</h3><p>为了解决下载和解压的速度问题，业界提出了多种优化方案：</p><ol><li><p><strong>镜像缓存</strong><br>镜像缓存是提升镜像下载速度的一种方法，通过缓存镜像可以避免重复下载。然而，缓存无法解决冷启动问题，并且镜像频繁变更（如应用更新或安全更新）会导致缓存失效。要实现高效的缓存管理，还需要较复杂的机制来管理缓存更新。</p></li><li><p><strong>减小镜像体积</strong><br>减少镜像体积也有助于缩短下载时间，但在某些场景下，例如 VM、AI、CUDA 镜像，体积优化空间有限。它们通常需要使用超过 7 GB 的存储空间，难以进一步缩减。</p></li></ol><h3 id="按需加载：是否可行？"><a href="#按需加载：是否可行？" class="headerlink" title="按需加载：是否可行？"></a>按需加载：是否可行？</h3><p>目前，大多数容器在启动时并不需要完整的镜像内容。一些论文表明，启动期间仅需 6.4% 的镜像内容，因此理论上可以通过按需下载来优化启动速度。然而，现有的镜像格式存在以下问题，限制了按需下载的实现：</p><ul><li><strong>OverlayFS 的限制</strong>：需要所有镜像层下载完毕后才能得知最终文件结构。</li><li><strong>gzip 不支持随机访问</strong>：即使只需下载单个文件，也要下载并解压整个层。</li><li><strong>校验问题</strong>：镜像 digest 是按整个层计算的，无法针对单个文件校验。</li></ul><h3 id="eStargz：实现按需加载"><a href="#eStargz：实现按需加载" class="headerlink" title="eStargz：实现按需加载"></a>eStargz：实现按需加载</h3><p>为了解决上述问题，eStargz 提出了针对 gzip 层的优化方案，即每个文件单独压缩并增加文件级别索引。eStargz 引入了如下优化：</p><ol><li><strong>独立压缩</strong>：每个文件单独压缩并索引，解决了 gzip 无法随机访问的问题。</li><li><strong>文件校验</strong>：可以对单个文件进行校验，无需校验整个层。</li></ol><p>具体的存储格式如下图：</p><p><img src="/../images/estartgz.png" alt="alt text"></p><p>每个文件被单独压缩合并成一个大的 blob，在 blob 最后增加一个 TOC 的描述文件记录每个文件的偏移量和校验值，这样就实现了按文件的索引和校验。</p><p>以下是 eStargz 的 TOC 格式示例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;entries&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bin/&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;dir&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;modtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-08-20T10:30:43Z&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;mode&quot;</span><span class="punctuation">:</span> <span class="number">16877</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;NumLink&quot;</span><span class="punctuation">:</span> <span class="number">0</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bin/busybox&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;reg&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;size&quot;</span><span class="punctuation">:</span> <span class="number">833104</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;modtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-06-12T17:52:45Z&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;mode&quot;</span><span class="punctuation">:</span> <span class="number">33261</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;offset&quot;</span><span class="punctuation">:</span> <span class="number">126</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;NumLink&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;digest&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sha256:8b7c559b8cccca0d30d01bc4b5dc944766208a53d18a03aa8afe97252207521f&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;chunkDigest&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sha256:8b7c559b8cccca0d30d01bc4b5dc944766208a53d18a03aa8afe97252207521f&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>通过这种改进，eStargz 实现了对单个文件的按需加载，且可以实现文件级别校验。</p><h3 id="性能权衡：优先级加载"><a href="#性能权衡：优先级加载" class="headerlink" title="性能权衡：优先级加载"></a>性能权衡：优先级加载</h3><p>虽然按需加载大幅优化了下载性能，但也可能带来运行时性能的下降。为此，eStargz 采用特殊标识来实现优先级加载，将启动所需文件放置于 <code>prioritized zone</code> 中，确保这些文件优先下载，进而提升运行时性能。</p><p><img src="/../images/estargz-optimize.png" alt="alt text"></p><p>按照作者测试的性能表现如下：</p><p><img src="/../images/estargz-perf.png" alt="alt text"></p><h3 id="代价与挑战"><a href="#代价与挑战" class="headerlink" title="代价与挑战"></a>代价与挑战</h3><p>尽管 eStargz 带来了按需加载的性能提升，但也带来了以下代价：</p><ul><li><strong>存储空间增加</strong>：每个文件单独压缩会增加额外的 metadata，降低压缩率。</li><li><strong>额外插件支持</strong>：eStargz 需要插件支持，例如在容器镜像推送和拉取时需要特定处理插件。</li></ul><h3 id="如何使用-eStargz"><a href="#如何使用-eStargz" class="headerlink" title="如何使用 eStargz"></a>如何使用 eStargz</h3><p>以下是 eStargz 的使用方法，适用于 containerd 的子项目以及一些支持 eStargz 的工具：</p><ol><li><p><strong>Docker, kaniko, nerdctl 命令行参数</strong>：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker buildx build -t ghcr.io/ktock/hello:esgz \</span><br><span class="line">    -o <span class="built_in">type</span>=registry,oci-mediatypes=<span class="literal">true</span>,compression=estargz,force-compression=<span class="literal">true</span> \</span><br><span class="line">    /tmp/buildctx/</span><br><span class="line"></span><br><span class="line">nerdctl image convert --estargz --oci ghcr.io/ktock/hello:1 ghcr.io/ktock/hello:esgz</span><br></pre></td></tr></table></figure></li><li><p><strong>containerd 插件配置</strong>：</p> <figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version</span> = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="section">[proxy_plugins]</span></span><br><span class="line">  <span class="section">[proxy_plugins.stargz]</span></span><br><span class="line">    <span class="attr">type</span> = <span class="string">&quot;snapshot&quot;</span></span><br><span class="line">    <span class="attr">address</span> = <span class="string">&quot;/run/containerd-stargz-grpc/containerd-stargz-grpc.sock&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd]</span></span><br><span class="line">  <span class="attr">snapshotter</span> = <span class="string">&quot;stargz&quot;</span></span><br><span class="line">  <span class="attr">disable_snapshot_annotations</span> = <span class="literal">false</span></span><br></pre></td></tr></table></figure></li></ol><p>此外，GKE 等云平台的集群已默认启用类似方案，进一步加速了镜像启动速度。看阿里也发表了基于 block device 的按需加载，这类的实现看上去在云厂商都有了比较大规模的落地。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>从传统的镜像缓存、镜像体积优化，到按需加载，eStargz 提供了一种兼顾性能和灵活性的方案，使得容器可以在仅下载部分内容的情况下启动。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="https://github.com/containerd/stargz-snapshotter/blob/main/docs/estargz.md">eStargz: Standard-Compatible Extension to Container Image Layers for Lazy Pulling</a></li><li><a href="https://medium.com/nttlabs/startup-containers-in-lightning-speed-with-lazy-image-distribution-on-containerd-243d94522361">Startup Containers in Lightning Speed with Lazy Image Distribution on Containerd</a></li><li><a href="https://www.usenix.org/conference/fast16/technical-sessions/presentation/harter">Slacker: Fast Distribution with Lazy Docker Containers</a></li><li><a href="https://github.com/containerd/accelerated-container-image">Accelerated Container Image</a></li><li><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/image-streaming">Use Image streaming to pull container images</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在容器的启动过程中，镜像下载速度往往是影响启动速度的最主要因素，通常占据了启动时间的 70% 以上。特别是对于体积庞大的 VM、AI 镜像，它们的大小可能达到数十 GB，导致下载和解压速度都成为启动的瓶颈。本文将探讨镜像下载的主要瓶颈、常见的优化方案以及最新的按需加载技术，</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>AI Gateway 调研之 Kong, Gloo 和 Higress</title>
    <link href="http://oilbeater.com/2024/08/26/kong-gloo-higress/"/>
    <id>http://oilbeater.com/2024/08/26/kong-gloo-higress/</id>
    <published>2024-08-26T07:39:23.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<p>上篇<a href="2024-08-25-ai-gateway-cloudflare">博客</a>介绍了 Cloudflare AI Gateway，这篇集中介绍一下 Kong, Gloo 和 Higress 因为这三者有一定的相似性，都是从原有的 API 网关基础上进行扩展，通过插件的方式支持了一系列 AI 相关的功能，在交付上也是传统的软件部署方式。这几个算是传统 API Gateway 迎接 AI 浪潮的代表，其中 Higress 更是把产品 Slogan 直接从 Cloud Native API Gateway 变成了 AI Gateway，虽然打不过就加入，但这样变来变去不怕人说没根么：）</p><p>由于这三款产品都需要额外的部署开通，有的 AI 功能还是商业版才有，所以下面的分析都是根据看文档总结而来，可能存在着和实际不符的情况。</p><table><thead><tr><th>功能</th><th>Kong</th><th>Gloo</th><th>Higress</th><th>备注</th></tr></thead><tbody><tr><td>技术栈</td><td>Nginx + Lua</td><td>Envoy + Go</td><td>Envoy + WASM</td><td>虽然几家都提供了插件机制，但是和网关的耦合程度都比价高，非网关的开发者上手还是有一定难度</td></tr><tr><td>日志监控</td><td>每个 AI 插件会将元信息，如模型名、Token 开销费用等信息加入到 Audit Log 中，但是似乎没有自定义元信息的功能，需要通过其他插件来辅助完成</td><td>似乎没有在 AI 这块对日志有功能增强，还是通用的监控</td><td>日志和监控中增加了 Token 的用量，提供的信息和 Kong 类似，也不具备自定义元信息的功能</td><td>如果能增加一些自定义元信息，并支持记录 Request 和 Response 里 Message 信息就更好了</td></tr><tr><td>Proxy</td><td>Kong 提供了归一化的 API 能够用一套统一的 API 去调用不同的 LLM API，这对开发者还是比较友好的能够不需要大改应用代码就能用不同的 LLM</td><td>Gloo 没有提供归一化的 API，只是反向代理到上游 LLM API</td><td>Higress 支持将不同的 LLM API 统一转换成 OpenAI API，这对开发者来说也比较友好，毕竟目前生态里还是直接用 OpenAI API 的比较多</td><td>虽然我觉得是否提供归一化的 API 没那么重要，不过一定要归一化的话归一化成 OpenAI 格式的会好些</td></tr><tr><td>API Key 管理</td><td>客户端的 Key 可以和上游的 Key 不一样，相当于把 Key 在网关层做了一层屏蔽</td><td>客户端的 Key 可以和上游的 Key 不一样，相当于把 Key 在网关层做了一层屏蔽</td><td>直接从客户端透传 Key 给上游</td><td>个人感觉 Gloo 这个功能还比较实用，避免了在 LLM 那里真实的 Key 被过多业务方知道，安全和可控性会更好一些</td></tr><tr><td>Cache</td><td>当前版本没有提供 LLM Cache 相关能力，据说会在 3.8 版本提供</td><td>提供了语义 Cache，看配置是调用了 OpenAI 的 Embedding 和 Redis 的 Vector，不过没看到更细粒度的比如 TTL 相似度的配置</td><td>提供的还是文本匹配的缓存，相比全文本可以通过 JSON PATH 的语法选择部分 Message 做缓存，看配置也是利用 Redis，不过不支持语义 Cache</td><td>Gloo 提供的语义 Cache 看起来更高级一些</td></tr><tr><td>请求&#x2F;响应改写</td><td>可以在 Request 和 Response 阶段分别加 prompt 对 message 进行改写，相当于一个小型的 workflow</td><td>只提供了 prepend system prompt 的能力，感觉提升有限</td><td>和 Kong 类似提供了用 prompt 进行改写的能力，不过现在只支持通义千问的 LLM 感觉不够开放</td><td></td></tr><tr><td>RAG</td><td>目前没有相关功能的插件</td><td>可以对接一个 postgres 和 OpenAI 的 embedding Token 这样可以自己提供一些文本来做 RAG</td><td>和 Gloo 的功能类似，不过只支持阿里云的向量服务和通义千问，还是感觉不够开放</td><td>感觉 RAG 的配置参数都比较少没有相似度，或者爬取网页的接口，只能做比较简单的 RAG</td></tr></tbody></table><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这三款产品由于只是看文档没有真实使用过，里面的内容很可能有不准确的地方，希望了解的同学可以指正。</p><p>总体来看三款产品都是 LLM 爆发前就存在的，之前也不是专门为 AI 场景设计的，很多使用的配置可能懂 Kubernetes 的更能看懂。尤其是 Gloo 的文档全是 YAML 和 CRD 的配置，浓浓的 Cloud Native 味道，Higress 看上去也是各种 ConfigMap 脱离 Kubernetes 是否还能用好我是心里存在疑虑的。</p><p>Higress 虽然没根了，但是整体看 AI 功能做的还是最完整的，发力也比较明显。Kong 感觉还只是试探性的做了些功能，而 Gloo 是把所有 AI 相关功能都放到商业版里了。如果 Higress 能把开发性做好不是被通义千问和阿里云上各种服务绑定的话我觉得还是个不错的项目。</p><p>最后的依赖还是这三款产品的扩展性可能都存在一定难度，需要高度了解网关相关的逻辑并掌握 Lua 或者 WASM 这样非主流的语言。而 AI 应用现在的形态其实还存在很多变化的可能，对应的 API 和需要的通用能力可能也有比较大的变化，比如怎么做 RAG，怎么做 Cache，怎么编排 LLM 都没有确定下来。不知道现在的架构会不会对他们未来的功能灵活变化产生影响。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;上篇&lt;a href=&quot;2024-08-25-ai-gateway-cloudflare&quot;&gt;博客&lt;/a&gt;介绍了 Cloudflare AI Gateway，这篇集中介绍一下 Kong, Gloo 和 Higress 因为这三者有一定的相似性，都是从原有的 API 网关基础上进</summary>
      
    
    
    
    
    <category term="AI" scheme="http://oilbeater.com/tags/AI/"/>
    
    <category term="Gateway" scheme="http://oilbeater.com/tags/Gateway/"/>
    
    <category term="Kong" scheme="http://oilbeater.com/tags/Kong/"/>
    
    <category term="Gloo" scheme="http://oilbeater.com/tags/Gloo/"/>
    
    <category term="Higress" scheme="http://oilbeater.com/tags/Higress/"/>
    
  </entry>
  
  <entry>
    <title>AI Gateway 调研之 Cloudflare AI Gateway</title>
    <link href="http://oilbeater.com/2024/08/25/ai-gateway-cloudflare/"/>
    <id>http://oilbeater.com/2024/08/25/ai-gateway-cloudflare/</id>
    <published>2024-08-25T14:10:45.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<p>随着 AI 的火热，眼看着之前调研的各家竞品 API 网关产品纷纷把自己的介绍改为 AI Gateway，于是就想调研一下这些所谓的 AI Gateway 究竟做了些啥。这次调研的对象有一些之前靠 API 网管或者云原生 Ingress Controller 起家加入 AI 功能的，例如：<a href="https://konghq.com/products/kong-ai-gateway">Kong</a>，<a href="https://www.solo.io/products/gloo-ai-gateway/">Gloo</a> 和 <a href="https://higress.io/en/">Higress</a>。也包括一些第一天就是借着 AI 起来的我认为真正 AI 原生的网关，例如 <a href="https://portkey.ai/features/ai-gateway">Portkey</a> 和 <a href="https://github.com/songquanpeng/one-api">OneAPI</a>。以及这篇博客介绍的基于公有云 Serverless 的 <a href="https://developers.cloudflare.com/ai-gateway/">Cloudflare AI Gateway</a>。</p><p>大体来看目前的 AI Gateway 主要能力在三个方面：</p><p><strong>常规 API 网关功能在 AI API 上的应用</strong>，例如：监控，日志，限速，反向代理，请求或响应改写，集成用户系统等。这些功能其实和 AI 关系不大就是把 LLM 的 API 当成了一个普通的 API 进行接入。</p><p><strong>部分 API 网关功能针对 AI 进行优化</strong>，例如限速功能增加基于 Token 的限速，缓存功能增加基于 Prompt 的缓存，防火墙基于 prompt 和 LLM 返回进行过滤，多个 LLM API Key 之间的负载均衡，多个 LLM Provider 的 API 转换。这些功能在原有的 API 网关就存在类似的概念，不过在 AI 场景下又有了相应的扩展。</p><p><strong>基于 AI 应用的场景增加的新功能</strong>，例如部分 AI 网关增加了 Embedding 和 RAG 的功能，把向量数据库和文本数据库的功能通过 API 的形式提供出来。还有一些针对 token 用量的性能优化，比如 Prompt 简化，语义化 Cache 等。还有一些更偏应用层的功能，例如对 LLM Output 提供打分功能等。</p><p>这篇博客介绍 <a href="https://developers.cloudflare.com/ai-gateway/">Cloudflare AI Gateway</a> 这款 AI Gateway 的特点。</p><h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><p>Cloudflare 的这款 AI Gateway 主要功能其实就是一个反向代理，看完了我甚至觉得我用 Cloudflare Worker 捣鼓一阵也能做个功能类似的。如果你原来用的是 OpenAI 的 API 那么现在你要做的就是把 SDK 里的 baseURL 换成 <code>https://gateway.ai.cloudflare.com/v1/$&#123;accountId&#125;/$&#123;gatewayId&#125;/openai</code> 就可以了。在这个过程中由于流量进出都是过 Cloudflare 的，Cloudflare 平台上就可以提供对应的监控，日志，缓存等功能。</p><p>这个方案有下面几个优点：</p><ul><li>接入很简单，改一下 baseURL 就接入进来了，API 格式也没有任何变化。并且完全是 Serverless 的，不需要自己额外管理任何服务器，这个功能现在是免费的，直接就白嫖了监控数据。</li><li>借助 Cloudflare 的全球网络可以实现一定的用户接入加速，不过这个用户接入的加速相比 LLM 本身的延迟比重应该很小，顶多在首个 Token 的延迟会有明显变化。</li><li>通用借助 Cloudflare 的全球网络可以一定程度隐藏掉源 IP，对于一些 OpenAI API 访问受限的区域用这个可以绕过去。</li></ul><p>但对应的也有下面的缺点：</p><ul><li>所有请求信息包括 API Key 都要在 Cloudflare 上过一道，会有安全方面的一些隐患。</li><li>Gateway 本身没有什么插件机制，想扩展功能的话会比较麻烦，只能在外面再套一层。</li><li>同样是因为 Cloudflare 的全球网路欧，如果一个 Key 一直变换 IP 地址访问，不知道会不会触发 OpenAI 那边的拉黑。</li></ul><h1 id="主要能力"><a href="#主要能力" class="headerlink" title="主要能力"></a>主要能力</h1><h2 id="多个-Provider-支持"><a href="#多个-Provider-支持" class="headerlink" title="多个 Provider 支持"></a>多个 Provider 支持</h2><p>由于 Cloudflare AI Gateway 并没有对 LLM API 进行修改，只是做反向代理，所以几乎主流的 LLM API 它都可以支持，只需要把 baseURL 改成对应 Provider 如 <code>https://gateway.ai.cloudflare.com/v1/$&#123;accountId&#125;/$&#123;gatewayId&#125;/&#123;provider&#125;</code> 即可。</p><p>它唯一多提供的一个 API 叫做 <a href="https://developers.cloudflare.com/ai-gateway/providers/universal/">Universal Endpoint</a> 可以做简单的 fallback。用法是在一个 API 请求里可以填写多个 Provider 的<br>query，这样当前面的 Provider 请求失败时会自动调用下一个 Provider。</p><h2 id="可观测"><a href="#可观测" class="headerlink" title="可观测"></a>可观测</h2><p>监控层面除了基础的 QPS 和 Error Rate 这些监控面板，还针对 LLM 的场景提供了 Token，Cost 以及 Cache 命中率的面板。</p><p>日志方面和 Worker 的日志很类似，只有实时日志无法查询历史日志。这里感觉做的不太好，Worker 至少还有第三方的方案能保存日志，但是 Gateway 这里却没有了。虽然通过一些实时日志 API 再自己保存的方式也可以，但还是太麻烦了。分析 LLM 请求和响应日志应该是很多 AI 应用后续做优化甚至 fine-tuning 的一个重要环节，这里没有直接集成持久化的方案其实是个硬伤。</p><h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><p>缓存方面，Cloudflare 提供的还是基于文本内容完全匹配的缓存，目测是通过 <a href="https://developers.cloudflare.com/kv/">Cloudflare Workers KV</a> 来实现的。也可以通过 <code>cf-aig-cache-key</code> 来实现自定义 Cache Key，包括设置缓存的 TTL 以及忽略 Cache。但是整体看起来基于现在的功能是无法实现语义缓存的，官方文档的说法是语义缓存会在未来提供。</p><h2 id="Rate-Limiting"><a href="#Rate-Limiting" class="headerlink" title="Rate Limiting"></a>Rate Limiting</h2><p>限速方面，Cloudflare 提供的还是传统的基于 QPS 的限速，这块并没有基于 AI 的场景提供基于 Token 的限速，这里未来还有改善的空间。</p><h2 id="Custom-metadata"><a href="#Custom-metadata" class="headerlink" title="Custom metadata"></a>Custom metadata</h2><p>可以在请求的 Header 中增加一些自定义字段，比如用户信息。这些信息可以通过日志进行检索。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>整体来看 Cloudflare AI Gateway 胜在简单易用，对于之前没有使用 AI Gateway 的用户可以两三分钟就接进来，提供了基础的监控和缓存能力。而且 Cloudflare 还有一些其他配套的 AI 服务例如 Works AI 提供了大量的开源模型的 Serving 和 Worker 提供边缘计算，几个一结合就能搭一套完全 Serverless 的 AI 系统。</p><p>他的问题主要在于更深入的功能提供的比较少，而且功能扩展比较麻烦，只能在外围通过 Worker 再来包一层。与其这样 Cloudflare 还不如直接把 AI Gateway 开源出来变成一个模板，用户可以根据自己需求去更改代码或者写插件，没准还能形成一个新的生态。毕竟我高度怀疑现在的 AI Gateway 其实就是个 Worker 模板。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;随着 AI 的火热，眼看着之前调研的各家竞品 API 网关产品纷纷把自己的介绍改为 AI Gateway，于是就想调研一下这些所谓的 AI Gateway 究竟做了些啥。这次调研的对象有一些之前靠 API 网管或者云原生 Ingress Controller 起家加入 AI</summary>
      
    
    
    
    
    <category term="AI" scheme="http://oilbeater.com/tags/AI/"/>
    
    <category term="Gateway" scheme="http://oilbeater.com/tags/Gateway/"/>
    
    <category term="Cloudflare" scheme="http://oilbeater.com/tags/Cloudflare/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 多集群通信的五种方案</title>
    <link href="http://oilbeater.com/2024/05/24/5-way-kubernetes-multcluster-communication/"/>
    <id>http://oilbeater.com/2024/05/24/5-way-kubernetes-multcluster-communication/</id>
    <published>2024-05-24T08:25:30.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<p>随着企业的业务规模不断扩大，Kubernetes 的使用也从单集群逐步扩展到多集群部署。多集群环境下，集群之间的通信成为一个重要的研究课题。本文将介绍五种跨 Kubernetes 集群通信的方案的基本原理，优点及其局限性。</p><h2 id="1-Underlay-网络"><a href="#1-Underlay-网络" class="headerlink" title="1. Underlay 网络"></a>1. Underlay 网络</h2><p>这类网络插件包括 <a href="https://www.cni.dev/plugins/current/main/macvlan/">macvlan</a>&#x2F;<a href="https://www.cni.dev/plugins/current/main/ipvlan/">ipvlan</a>&#x2F;<a href="https://kubeovn.github.io/docs/stable/start/underlay/">Kube-OVN underlay</a> 以及各种云上的 VPC CNI 等。</p><p><strong>基本原理</strong>：</p><p>Underlay 网络从 CNI（容器网络接口）的角度来看是最简单的方式。这种方式依赖于底层基础设施在网络层面实现打通。例如，使用公有云上的 VPC Peering 或者物理网络配置路由和大二层。当底层网络实现了连通，跨集群的容器网络就自然连通了。</p><p><strong>优点</strong>：</p><ul><li>CNI 角度最为简单，无需额外操作。</li><li>架构上清晰，把跨集群通信的责任交给底层网络。</li></ul><p><strong>局限性</strong>：</p><ul><li>依赖于特定 CNI，Underlay 类型的网络场景使用范围有限，有些情况只能使用 Overlay 网络。</li><li>在异构的环境，比如多云之间、公有云和私有云之间打通存在困难。</li><li>只是做了基础的容器网络通信打通，更上层的服务发现，域名和网络策略等功能存在缺失。</li><li>一次性打通所有集群的容器网络，缺乏细粒度的控制。</li></ul><h2 id="2-提供跨集群通信能力的-Overlay-CNI"><a href="#2-提供跨集群通信能力的-Overlay-CNI" class="headerlink" title="2. 提供跨集群通信能力的 Overlay CNI"></a>2. 提供跨集群通信能力的 Overlay CNI</h2><p>在无法使用 Underlay 网络的情况下，一些特定的 CNI 在 Overlay 层面也实现了跨集群。例如 <a href="https://cilium.io/use-cases/cluster-mesh/">Cilium Cluster Mesh</a>, <a href="https://antrea.io/docs/v2.0.0/docs/multicluster/quick-start/">Antrea Multi-Cluster</a> 和 <a href="https://kubeovn.github.io/docs/stable/en/advance/with-ovn-ic/">Kube-OVN with ovn-ic</a>。</p><p><strong>基本原理</strong>：</p><p>这些 CNI 的实现方案其实大体一致，通过选择一组集群内的节点作为网关节点，然后网关节点之间建立隧道，跨集群流量通过网关节点转发。</p><p><strong>优点</strong>：</p><ul><li>CNI 自包含跨集群功能，无需额外组件支持。</li></ul><p><strong>局限性</strong>：</p><ul><li>依赖特定 CNI，无法实现不同 CNI 集群之间的通信。</li><li>无法处理 CIDR 重叠的情况，需要提前规划好网段。</li><li>除了 Cilium 实现的比较完整，把跨集群的服务发现和网络策略都实现了，其他的仅实现基础容器网络通信。</li><li>一次性打通所有集群的容器网络，缺乏细粒度的控制。</li></ul><h2 id="3-Submariner"><a href="#3-Submariner" class="headerlink" title="3. Submariner"></a>3. Submariner</h2><p>由于跨集群网络互通存在着通用的需求，在实现上也存在着类似，各个 CNI 其实存在着一些重复造轮子的工作。<a href="https://submariner.io/">Submariner</a> 作为一款 CNI 无关的跨集群网络插件提供了一种通用的能力，能将不同 CNI 的集群组成一个网络达到互通。Submariner 最早由 Rancher 的工程师创建，现在已经是 CNCF Sandbox 项目了，目前看红帽的工程师也在积极参与这个项目。</p><p><strong>基本原理</strong>：</p><p>Submariner 选择集群内的网关节点，网关节点通过 VXLAN 通信。跨集群流量通过 VXLAN 传输。Submariner 依赖 CNI 将 egress 流量先发送到宿主机的网络内，然后再进行转发。此外，Submariner 部署了一组 CoreDNS 实现跨集群服务发现，并使用 Globalnet Controller 解决 CIDR 重叠问题。</p><p><strong>优点</strong>：</p><ul><li>一定程度上 CNI 无关，可以连接不同 CNI 的集群。</li><li>实现了跨集群服务发现，支持 Service 和域名解析。</li><li>支持 CIDR 重叠的集群通信，避免了集群部署后想互联却发现 IP 冲突的尴尬。</li></ul><p><strong>局限性</strong>：</p><ul><li>并不是所有的 CNI 都可用，如果像 macvlan 或者 cilium 短路这种宿主机看不到流量的情况，就没有办法拦截流量到自己的隧道了。</li><li>Gateway 目前是主备模式，没法横向负载均衡，在大流量的场景下可能存在性能瓶颈。</li><li>一次性打通所有集群的容器网络，缺乏细粒度的控制。</li></ul><h2 id="4-Skupper"><a href="#4-Skupper" class="headerlink" title="4. Skupper"></a>4. Skupper</h2><p><a href="https://skupper.io/index.html">Skupper</a> 是我认为几个方案里最有意思的，它能够按需进行 Service 层面的网络打通，避免了一次完全打通的控制问题。而且很创新的使用了七层消息队列的方式来实现，可以说是对底层网络和 CNI 完全无依赖，上手也十分简单。目前 Skupper 看贡献者主要是红帽的工程师。</p><p><strong>基本原理</strong>：</p><p>和上述几个方案使用隧道将容器 IP 直接打通不同，Skupper 提出了一个 VAN（Virtual Application Networks）的概念，要在 7 层将网络打通。简单说就是不把 IP 直接打通而是把 Service 拉通，概念上和 ServiceExporter，ServiceImporter 类似，但是这个项目启动的比较早，当时还没有这些社区提出来的概念，在当时应该算个很创新的想法了</p><p>在实现上 Skupper 也另辟蹊径，用的是个消息队列的实现，多个集群之间组成了一个大的消息队列，跨集群通信的数据包发送到这个消息队列里。另一端再去消费这个数据包。思路上其实类似反向代理，但用消息队列来实现也是个很开脑洞的想法。把 Service 变成了一个消息的订阅点来提供消费，这样就可以按需的在服务端和客户端之间建立一个消息队列，通过消息队列的概念去管理和控制这个消息通路。</p><p><strong>优点</strong>：</p><ul><li>CNI 兼容性好，完全不依赖 CNI 的行为在应用层进行数据包的打通</li><li>上手简单，不需要太复杂的前期网络规划，也没有 CIDR 不重叠要求。提供了 CLI 方便临时测试和快速演示</li><li>并不是容器网络互通而是按需将 Service 打通，可以做到细粒度的控制，对底层要求也更低</li></ul><p><strong>局限性</strong>：</p><ul><li>文档介绍目前只支持 TCP 协议，UDP 和更底层的协议比如 ICMP 会有问题</li><li>由于是通过消息队列转发消息，IP 信息会丢失</li><li>通过消息队列转发的思路还是有些奇怪，在性能方面比如延迟和吞吐量上可能会有一些损失</li><li>而且 TCP 本身是有状态的，能否完全转换成消息队列的形式，兼容性如何是个疑问</li></ul><h2 id="5-KubeSlice"><a href="#5-KubeSlice" class="headerlink" title="5. KubeSlice"></a>5. KubeSlice</h2><p><a href="https://kubeslice.io/documentation/open-source/1.3.0">KubeSlice</a> 是刚刚进入 CNCF Sandbox 的一个项目。上述的方案基于隧道做的没法做到细粒度控制和 CNI 完全兼容，基于应用层做的又没法做到网络协议完全兼容。KubeSlice 提供了一个新的思路，尝试同时解决这两个问题。</p><p><strong>基本原理</strong>：</p><p>KubeSlice 最底层的思路十分简单粗暴，就是按需给 Pod 动态插入一块网卡，在这个网卡上面做跨集群的 Overlay，然后再在这个 Overlay 网络上面实现服务发现，和网络策略。用户可以按需的将跨集群的几个 Namespace 或者 某几个 Pod 组成一个网络，在可以使实现灵活的细粒度管控基础上由于走的是基于网卡的二层网络，也实现了网络协议的最大兼容性。</p><p><strong>优点</strong>：</p><ul><li>CNI 兼容性好，由于是额外插入了一块网卡完全不关心原先的 CNI 是什么，并且是额外的网络也不用担心原先网络地址是否有冲突的问题</li><li>网络协议兼容性好，同样是由于额外有了一块网卡做流量转发，所有的网络协议都是兼容的。</li><li>灵活度高，KubeSlice 提供了命令行工具可以动态的创建和加入网络，用户可以选择按应用的需求创建多个跨集群的虚拟网络</li><li>功能完善，按照文档的说法，该项目在额外的这个 Overlay 网络上实现了服务发现，DNS，QoS，Networkpolicy 和监控，可以说把各个方面都覆盖到了</li></ul><p><strong>局限性</strong>：</p><ul><li>由于本质上是双网卡，应用侧需要感知这个事情选择合适的网络，可能会涉及到应用改造</li><li>由于跨集群走的是另一个网卡不是 Pod 主网卡的 IP，对监控和追踪等外部系统可能涉及到改造</li><li>目前看文档应该是一个内部的项目进行开源，文档里很多使用方式和 API 介绍的不是特别清楚，需要对着 reference 慢慢猜每个参数到底是什么意思。功能上看上去确实比较全，但能让外部用户很好的使用的话文档还有很多需要完善的地方。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在 Kubernetes 多集群环境中实现高效通信有多种方案可供选择。每种方案都有其优点和局限性，用户可以根据具体需求和环境选择合适的方案。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;随着企业的业务规模不断扩大，Kubernetes 的使用也从单集群逐步扩展到多集群部署。多集群环境下，集群之间的通信成为一个重要的研究课题。本文将介绍五种跨 Kubernetes 集群通信的方案的基本原理，优点及其局限性。&lt;/p&gt;
&lt;h2 id=&quot;1-Underlay-网络</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/tags/kubernetes/"/>
    
    <category term="network" scheme="http://oilbeater.com/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>k8gb: 云原生最佳开源 GSLB 方案</title>
    <link href="http://oilbeater.com/2024/04/18/k8gb-best-cloudnative-gslb/"/>
    <id>http://oilbeater.com/2024/04/18/k8gb-best-cloudnative-gslb/</id>
    <published>2024-04-18T10:01:10.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<p>如何将流量在多个 Kubernetes 集群之间进行负载均衡，并做到自动的灾备切换一直是一个让人头疼的问题。我们之前调研了公有云，<a href="https://github.com/karmada-io/multi-cluster-ingress-nginx">Karmada Ingress</a>，自己也做了一些手动 DNS 的方案，但这些方案在成本，通用性，灵活度以及自动化程度上都有所欠缺。直到调研到了 <a href="https://www.k8gb.io/">k8gb</a> 这个由南非银行 <a href="https://www.absa.africa/">Absa Group</a> 为了做金融级别的多活而启动的一个项目。k8gb 巧妙的利用了 DNS 的各种协议完成了一个通用且高度自动化的 GSLB 方案，看完后我就再也不想用其他的方案了。这篇博客会简单介绍一下其他几个方案各自存在的问题，以及 k8gb 究竟是如何巧妙的利用 DNS 来实现的 GSLB。</p><h2 id="什么是-GSLB"><a href="#什么是-GSLB" class="headerlink" title="什么是 GSLB"></a>什么是 GSLB</h2><p>GSLB（Global Service Load Balancer）是相对于单集群负载均衡的一个概念，单集群负载均衡主要作为一个集群的入口将流量分发到集群内部，而 GSLB 通常作为再上一层多个集群的流量入口，进行流量负载均衡和故障处理。一方面 GSLB 可以设置一些地理亲和的规则达到流量就近转发提升整体的性能，另一方面当某个集群出现问题后可以自动将流量切换到正常集群，减少单个集群故障对用户的影响。</p><h2 id="其他方案的问题"><a href="#其他方案的问题" class="headerlink" title="其他方案的问题"></a>其他方案的问题</h2><h3 id="商用负载均衡"><a href="#商用负载均衡" class="headerlink" title="商用负载均衡"></a>商用负载均衡</h3><p>GSLB 并不是一个新出的概念，所以不少商业公司都在这方面有很成熟的产品，例如 <a href="https://www.f5.com/solutions/use-cases/global-server-load-balancing-gslb">F5 GSLB</a>。这类产品通常有以下几个缺点：</p><ol><li>没有很好的和云原生对接，通常需要在 Kubernetes 集群外独立部署专有的软硬件，无法做到统一管理。</li><li>成本高，且有厂商锁定的风险。</li></ol><h3 id="公有云全局负载均衡"><a href="#公有云全局负载均衡" class="headerlink" title="公有云全局负载均衡"></a>公有云全局负载均衡</h3><p>公有云为了解决流量多地域分发会提供多集群负载均衡的产品，例如 AWS 的 <a href="https://aws.amazon.com/global-accelerator/">Global Accelerator</a> 和 GCP 的 <a href="https://cloud.google.com/load-balancing/docs/https">External Application Load Balancer</a>。GCP 上甚至还有一组自定义的 <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress">Multi Cluster Ingress</a> 资源，可以很好的和 Kubernetes 里的 Ingress 进行对接。但是他们也有一下几个问题：</p><ol><li>虽然是多集群负载均衡，但是多个集群必须是同一个公有云，不能在多云间进行流量调度。</li><li>私有云无法使用这个方案。</li></ol><h3 id="Karmada-Multi-Cluster-Ingress"><a href="#Karmada-Multi-Cluster-Ingress" class="headerlink" title="Karmada Multi  Cluster Ingress"></a>Karmada Multi  Cluster Ingress</h3><p>Karmada 是一个多集群的编排工具，也提供了自己的多集群流量调度方案 <a href="https://karmada.io/docs/userguide/service/multi-cluster-ingress/">Karmada Multi Cluster Ingress</a>。该方案通过在某个集群内部署一个由 Karmada 社区提供的 ingress-nginx 以及定义 <code>MultiClusterIngress</code> 来完成多集群流量调度，但这个方案有以下几个问题：</p><ol><li>依赖多集群间容器网络打通，和 ServiceImporter ServiceExporter 等 CRD，整体要求比较高。</li><li>需要额外再去管理这个提供 GSLB 服务的 ingress-nginx 实例，该部署在哪，该部署多少，怎么分配都是运维期间需要考虑的问题。</li><li>这个社区改造后的 <a href="https://github.com/karmada-io/multi-cluster-ingress-nginx">multi-cluster-ingress-nginx</a> 近两年基本没有什么代码提交，是否可用会让人有担忧。</li></ol><h3 id="简单的-DNS-方案"><a href="#简单的-DNS-方案" class="headerlink" title="简单的 DNS 方案"></a>简单的 DNS 方案</h3><p>如果手动攒出来一个简单的基于 DNS 的方案其实也是可以的，大部分 DNS 厂商都提供了健康检查的功能，因此我们可以将多个集群的出口 IP 地址加入到 DNS 的解析记录里，同时配置健康检查来做故障切换。但是这个简单的方案在规模扩大后就会有一些明显的限制：</p><ol><li>无法很好的自动化，一个集群下可能有多个不同的域名和多个不同的 Ingress IP 的组合，手动去管理他们的映射关系会随着规模增加变得难以维护。</li><li>DNS 厂商的健康检查通常是基于 TCP 和 ICMP 的，因此如果一个集群的出口完全挂掉这种故障是可以检测到并进行切换的。但是如果是局部故障就无法探测，例如多个 Ingress 复用一个 ingress-controller 并通过域名进行流量转发的情况下，如果其中一个服务的后端实例全部异常了，但是 ingress-controller 上的其他服务正常，那么健康检查还是会正常通过，并没有办法把流量切换到另一个集群。</li><li>DNS 本身存在各级缓存，更新时间可能较长。</li><li>DNS 健康检查本身也是个厂商提供的能力，并不能保证所有厂商都能提供这个能力，尤其是在私有云的场景。</li></ol><h2 id="k8gb-的解决方案"><a href="#k8gb-的解决方案" class="headerlink" title="k8gb 的解决方案"></a>k8gb 的解决方案</h2><p>k8gb 的解决方案其实也是用 DNS，但是通过自己的一系列巧妙的设计，解决了上面提到的简单 DNS 方案的一系列缺陷。</p><p>简单 DNS 方案的问题本质是没有和 Kubernetes 进行很好的对接，Kubernetes 内的一些动态信息，例如新增的 Ingress，新增的域名，服务的健康状态没法很好的同步到上游 DNS 服务器，而上游 DNS 服务器简单的健康检查也没办法应付 Kubernetes 里这种复杂的变化。因此 k8gb 最核心的一个变化就是上游的 DNS 记录不再是通过 A 记录或者 AAAA 记录指向集群的一个出口地址，而是 forward 到集群内的一个自己配置的 CoreDNS 进行 DNS 解析，将真正复杂的 DNS 逻辑下沉到集群里来自己控制。这样上游 DNS 只需要做简单的代理，不再需要配置健康检查，也不再需要动态的调整多个地址映射。</p><p>调整后用户请求 DNS 的流程如下图所示：</p><ul><li><img src="https://www.k8gb.io/docs/images/gslb-basic-multi-cluster.svg" alt="K8GB multi-cluster interoperability"></li></ul><ol><li>用户向外部 DNS 服务商请求一个域名的 IP 记录。</li><li>外部 DNS 将这个请求代理发送给集群内由 k8gb 管控的一个 CoreDNS。</li><li>k8gb 根据 Ingress Spec 里的域名，Ingress Status 里的 Ingress IP 以及集群内对应后端 Pod 的健康状态，负载均衡策略等信息分析出一个可用的 Ingress IP 返回给用户。</li><li>用户通过这个 IP 就可以直接访问到对应的 Ingress Controller。</li><li>当某一个集群的 k8gb 管控的 CoreDNS 出问题时由于上游 DNS 会同时将 DNS 请求代理到多个集群，另一个集群也可以返回自己的 Ingress IP，用户端可以通过多个返回的可用 IP 选择一个进行访问。</li></ol><p>这种方式管理员只需要在上游 DNS 注册几个域名后缀并代理到每个集群的 CoreDNS 就可以了，k8gb 本身也提供了自动化的能力，只要配好证书可以自动分析 Ingress 内使用的域名自动注册给上游 DNS，大幅简化了管理员的操作。</p><p>整个流程中还有一个特殊的点要注意，就是每个集群自己的 CoreDNS 不能只记录本集群 Ingress IP 的地址，还需要记录其他集群同一个 Ingress 的 Ingress IP 地址。因为如果只记录本集群的，当本集群对应服务的 Pod 都 Not Ready 时，CoreDNS 会返回 NXDomain 如果客户端收到了这个返回就会按照域名无法解析处理，此时另一个集群服务其实是可以正常提供服务的。因此 k8gb 还需要同步所有集群同一个域名 Ingress 对应的 Ingress IP 和它的健康状态。</p><p>众所周知多集群之间的数据同步也是一个世界难题，但是 k8gb 同样通过 DNS 巧妙的实现了数据的同步。</p><p>同步的流程图如下所示：</p><p><img src="https://www.k8gb.io/docs/images/k8gb-multi-cluster-interoperabililty.svg" alt="k8gb multi-cluster-interoperability"></p><p>大致的思路是每个集群的 k8gb 会把自己的 CoreDNS 的 Ingress IP 同样注册到上游 CoreDNS，这样每个集群就可以直接访问另一个集群的 CoreDNS 了。然后每个集群内的 CoreDNS 再按照一个特殊的域名格式比如 <code>localtargets-app.cloud.example.com</code> 来保存本集群内 <code>app.cloud.example.com</code> Ingress 的 Ingress IP 并维护其健康状态。这样每个集群就都可以通过这个特殊的域名来或者其他集群这个域名对应的 Ingress IP 然后加入到自己的返回结果里，实现了域名解析的多集群同步。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>k8gb 作为一款开源的 GSLB 实现了和 Kubernetes 的无缝对接，能够很好的对跨集群的域名和流量进行自动化管理，并且对外部的依赖降到了只需要一个添加 DNS 记录的 API，真正实现了一套可以多云统一的 GSLB 方案。尽管目前这个项目还没有那么火热，但在我心里它已经是这个领域内的最佳方案了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;如何将流量在多个 Kubernetes 集群之间进行负载均衡，并做到自动的灾备切换一直是一个让人头疼的问题。我们之前调研了公有云，&lt;a href=&quot;https://github.com/karmada-io/multi-cluster-ingress-nginx&quot;&gt;Karm</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/tags/kubernetes/"/>
    
    <category term="network" scheme="http://oilbeater.com/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>如何轻松将 AI 生成内容整合入 Logseq 笔记</title>
    <link href="http://oilbeater.com/2024/03/21/sentence-to-logseq/"/>
    <id>http://oilbeater.com/2024/03/21/sentence-to-logseq/</id>
    <published>2024-03-21T10:39:37.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<p>我的日常生活已经融入了很多 AI 的协助，比如翻译一段文字、解释某个名词、回答一个问题等等。但是频繁使用后发现了另一个问题就是这些 AI 的产出对我的知识来说是个有益的补充，可是使用的时候通常是一次性的解决问题，并没有把这些内容沉淀下来。当需要把他们收集到笔记里的时候还要再回去整理，很是不方便。想象一下，如果这些临时查询的宝贵信息可以自动保存到自己的笔记系统中，就再也不用担心丢失重要的知识片段了。于是就萌生了做个工作流把这些 AI 生成的内容自动存到我的笔记系统里，然后再定期回顾。这里会介绍我调研的一些工具的局限，以及如何用了一个简单的框架把 AI 生成的内容自动导入到 logseq，并做成 flashcard。</p><h1 id="问题分解和初步调研"><a href="#问题分解和初步调研" class="headerlink" title="问题分解和初步调研"></a>问题分解和初步调研</h1><p>这里以我一个日常翻译的场景为例：当遇到读不懂的句子是我希望能够划词翻译，然后让 AI 帮我分析这个句子里疑难的单词，最后把这些内容以 flashcard 的格式保存到 logseq 里，这样借助手机端的同步我就可以在碎片时间来复习这个句子了。所以问题大致就是三个，1. 取出划线的文本 2. 编写一个 prompt 生成我想要的内容 3. 保存到 logseq。</p><p>前两步其实很多工具都已经集成了包括 Raycast AI 和 OpenAI-translator，但是第他们都没提供把前两步的结果导出到第三方的扩展接口。Raycast 理论上可以通过自己的插件体系来完成这件事，但是我这边不太熟悉 JS，并且也不想太依赖 Raycast AI，毕竟后面不打算续费了。于是决定自己写个 python 脚本做这些事，然后通过 Raycast 的 script command 和快捷键直接调用脚本，这样理论上也可以一个快捷键完成整个工作流。</p><h1 id="如何获取划选的文本"><a href="#如何获取划选的文本" class="headerlink" title="如何获取划选的文本"></a>如何获取划选的文本</h1><p>第一个难题就是获取划选的文本，看上去是个操作系统级别的操作，并不是很好实现。直到我看到了一个查词软件的实现我才恍然大悟：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/raycast/script-commands/blob/master/commands/apps/dictionary/look-up-in-dictionary.applescript</span></span><br><span class="line">tell application <span class="string">&quot;System Events&quot;</span></span><br><span class="line">    keystroke <span class="string">&quot;c&quot;</span> using &#123;<span class="built_in">command</span> down&#125;</span><br><span class="line">    delay 0.1</span><br><span class="line">    <span class="keyword">do</span> shell script <span class="string">&quot;open dict://&quot;</span> &amp; the clipboard</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>这个脚本本质上是模拟键盘摁下了 <code>cmd + v</code>，然后直接读剪切板去了，就这么绕过了获取选中文本的问题。考虑到我在用 Raycast 的时候经常碰到划选的文本识别错误，已经养成了 <code>cmd + v</code> 的习惯，那就不用费劲心思找获取划选的文本的方法了，直接读剪贴板就完了。</p><h1 id="调用-AI"><a href="#调用-AI" class="headerlink" title="调用 AI"></a>调用 AI</h1><p>这块基本经轻车熟路了，关键的是设置一个合适的 prompt，在翻译的同时生成一个句子的教学指南，并尽可能的按照 logseq 的格式来输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">message_text = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a university English teacher, below is a paragraph in English. &quot;</span> +</span><br><span class="line">        <span class="string">&quot;Please first translate it into Chinese. Then extract difficult words and phrases from the source paragraph, sort them in descending order of importance choose only the top 3 output them with explain of their usage to me in detail from a linguistic perspective.&quot;</span> +</span><br><span class="line">        <span class="string">&quot;The overall output should look like this: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- &#123;The Chinese Translation&#125; \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- Explanation: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;  - &#123;word or phrase&#125;: &#123;explanation&#125;\n&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>:<span class="string">&quot;user&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: content</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h1 id="输出到-logseq"><a href="#输出到-logseq" class="headerlink" title="输出到 logseq"></a>输出到 logseq</h1><p>在输出到 logseq 这一步的时候本来是打算调用 logseq 的 API，logseq 的开发者模式提供了一个本地的 HTTP 服务可以通过 HTTP 去调用。但是当我看了他们的 API 文档后差点给整崩溃了，所有的操作都要用 id，page id 又没办法去直接索引，要 getAll 后自己过滤。appendBlock 也不允许插入带层级的 Block，要串好几个 API 才能完成一个简单的操作。</p><p>崩溃的时候转念一想，logseq 不就是一堆 markdown 的渲染器么，既然 API 那么难用我直接去写文件不就好了，于是一组复杂的 API 调用变成了轻松愉快的文件 append 操作。这样既绕开了 logseq API 的限制，甚至有可能接入其他基于 markdown 的笔系统。</p><p>最后完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyperclip</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> AzureOpenAI</span><br><span class="line"></span><br><span class="line">content = pyperclip.paste()</span><br><span class="line"></span><br><span class="line">azure_endpoint = <span class="string">&quot;YOUR AZURE ENDPOINT&quot;</span></span><br><span class="line">api_key = <span class="string">&quot;YOUR API KEY&quot;</span></span><br><span class="line">model = <span class="string">&quot;YOUR MODEL NAME&quot;</span></span><br><span class="line">logseq_path = <span class="string">&quot;YOUR LOGSEQ PAGE FILE PATH&quot;</span></span><br><span class="line"></span><br><span class="line">client = AzureOpenAI(</span><br><span class="line">  azure_endpoint = azure_endpoint,</span><br><span class="line">  api_key=api_key,</span><br><span class="line">  api_version=<span class="string">&quot;2024-02-15-preview&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">message_text = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a university English teacher, below is a paragraph in English. &quot;</span> +</span><br><span class="line">        <span class="string">&quot;Please first translate it into Chinese. Then extract difficult words and phrases from the source paragraph, sort them in descending order of importance choose only the top 3 output them with explain of their usage to me in detail from a linguistic perspective.&quot;</span> +</span><br><span class="line">        <span class="string">&quot;The overall output should look like this: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- &#123;The Chinese Translation&#125; \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- Explanation: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;  - &#123;word or phrase&#125;: &#123;explanation&#125;\n&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>:<span class="string">&quot;user&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: content</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">completion = client.chat.completions.create(</span><br><span class="line">  model=model, <span class="comment"># model = &quot;deployment_name&quot;</span></span><br><span class="line">  messages = message_text,</span><br><span class="line">  temperature=<span class="number">0.7</span>,</span><br><span class="line">  max_tokens=<span class="number">500</span>,</span><br><span class="line">  top_p=<span class="number">0.95</span>,</span><br><span class="line">  frequency_penalty=<span class="number">0</span>,</span><br><span class="line">  presence_penalty=<span class="number">0</span>,</span><br><span class="line">  stop=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> completion.choices[<span class="number">0</span>].message.content == <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;No response&quot;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">response = completion.choices[<span class="number">0</span>].message.content</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(logseq_path, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&quot;\n- &quot;</span> + content.rstrip() + <span class="string">&quot; #card #English&quot;</span> + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> response.splitlines():</span><br><span class="line">        <span class="keyword">if</span> line.startswith(<span class="string">&quot;```&quot;</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> line.rstrip() == <span class="string">&quot;&quot;</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line.startswith(<span class="string">&quot;- &quot;</span>):</span><br><span class="line">            line = <span class="string">&quot;- &quot;</span> + line.rstrip()</span><br><span class="line">        f.write(<span class="string">&quot;  &quot;</span> + line + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure><p>后需要做的就是在 Raycast 里给这个脚本设置一个快捷键，这样下次碰到不会的句子，选中复制后就可以通过快捷键完成翻译，难点提取，和生成 flashcard 的整个工作流。</p><p>最后生成的 flashcard 效果大概如下：</p><p><img src="/../images/logseq-card.png" alt="alt text"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过将 AI 生成的内容自动化集成到 Logseq 笔记中，我们不仅提高了信息管理的效率，还优化了学习和工作流程。通过一个简单的脚本每个人都可以轻松地将这种强大的技术整合到日常生活中，实现知识的积累和复习。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我的日常生活已经融入了很多 AI 的协助，比如翻译一段文字、解释某个名词、回答一个问题等等。但是频繁使用后发现了另一个问题就是这些 AI 的产出对我的知识来说是个有益的补充，可是使用的时候通常是一次性的解决问题，并没有把这些内容沉淀下来。当需要把他们收集到笔记里的时候还要再</summary>
      
    
    
    
    
    <category term="logseq" scheme="http://oilbeater.com/tags/logseq/"/>
    
    <category term="ai" scheme="http://oilbeater.com/tags/ai/"/>
    
    <category term="learning" scheme="http://oilbeater.com/tags/learning/"/>
    
  </entry>
  
  <entry>
    <title>实现 VS Code Remote SSH 下的自动关机</title>
    <link href="http://oilbeater.com/2024/03/13/shutdown-remote-ssh-server/"/>
    <id>http://oilbeater.com/2024/03/13/shutdown-remote-ssh-server/</id>
    <published>2024-03-13T10:09:26.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<p>我现在所有的开发环境都转移到了 GCP 的 Spot Instance 实例，然后用 VS Code Remote SSH 插件进行连接。这种方式的好处是 GCP 可以按秒计费，即使开了很高规格的机器只要及时关机费用也是可控的，缺点是如果中间忘了关机赶上过节那费用就烧开了。再被烧掉了 10 多美元后，痛定思痛决定找个能在 VS Code 空闲时自动关机的方法。过程中为了能够在 Docker 容器内执行 shutdown 命令还搞了一些黑魔法。</p><h1 id="如何判断空闲"><a href="#如何判断空闲" class="headerlink" title="如何判断空闲"></a>如何判断空闲</h1><p>这个功能其实类似 CodeSpace 里的 idle timeout，但是 Remote SSH 这个插件并没有暴露这个功能，所以只能自己实现。其实主要的难点在于如何在 Server 端判断空闲，找了一圈没在 VS Code 里看到有暴露的接口，于是就想能不能跳出 VS Code 看看有什么方法能够简洁的判断空闲发生。</p><p>最直接的想法就是去看 SSH 的连接，因为 Remote SSH 也是通过 SSH 连接上来的，如果当前机器没有存活的 SSH 连接，那么就可以认为是空闲直接关机了。但是问题是 Remote SSH 的连接超时时间会特别长，搜索了一些 Issue 有说是 4 个小时的，我也尝试了直接关闭 VS Code 的客户端，发现 Server 端的 SSH 连接也一直没有消失。如果在 Server 端设置 SSH 超时，Client 那边很快就会重连，连接数量也不会减少。</p><p>既然没法通过直接看 SSH 连接数量来判断，那么就进一步去看能不能判断已有的 SSH 连接是不是已经没有流量了。用 <code>tcpdump</code> 抓包看了一下，即使客户端没有任何交互，还是会有 1s 一次的 TCP 心跳数据包，所以也不能有流量为 0 来判断。不过观察下来心跳数据包的大小都是固定的，都是 44 字节，正好可以根据这个特征来判断，如果一段时间内 SSH 端口没有大于 44 字节的数据包就可以判断空闲了。</p><p>于是第一版本的脚本就出来了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span></span><br><span class="line">  <span class="keyword">if</span> [ -f /root/dump ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">timeout</span> 1m tcpdump -nn -i ens4 tcp and port 22 and greater 50 -w /root/dump</span><br><span class="line"></span><br><span class="line">  line_count=$(<span class="built_in">wc</span> -l &lt; /root/dump)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> [ <span class="variable">$line_count</span> -gt 0 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">    shutdown -h now</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>这样就实现了 1 分钟内 SSH 端口没有非心跳数据包就关机的功能，下一步就是要让这个脚本自动运行了。</p><h1 id="Docker-黑魔法"><a href="#Docker-黑魔法" class="headerlink" title="Docker 黑魔法"></a>Docker 黑魔法</h1><p>在把脚本打包成 Docker 镜像时发现了一个有趣的问题，那就是所有的 Base Image 里都没有 <code>shutdown</code> 命令，<code>shutdown</code> 命令也没法很容易的安装。为了能够执行主机上的 <code>shutdown</code> 命令，就需要在 Docker 容器里切换到主机的命名空间，再去关机。所以需要把之前的脚本稍微修改一下，包装一下 <code>shutdown</code> 命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsenter -a -t 1 shutdown -h now</span><br></pre></td></tr></table></figure><p>这个 <code>nsenter</code> 命令的作用是选定 Pid 为 1 的进程，然后进入这个进程的所有(pid, mount, network) Namespaces，这样当 Docker 运行在共享主机 Pid 模式下我们相当于就进入了主机 1 号进程的所有 Namespaces，看上去就和 SSH 到主机上一样可以执行操作了。这样只要再用下面的命令启动容器，就可以不担心忘记随时关机了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name=close --pid=host --network=host --privileged --restart=always -d close:v0.0.1</span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过监控 SSH 的流量情况可以一定程度上猜测 VS Code 已经空闲了，然后再用 Docker 的一些黑魔法就可以实现自动关机了。不过整个链路的黑魔法都太多了，有没有什么简单的方式呢？</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我现在所有的开发环境都转移到了 GCP 的 Spot Instance 实例，然后用 VS Code Remote SSH 插件进行连接。这种方式的好处是 GCP 可以按秒计费，即使开了很高规格的机器只要及时关机费用也是可控的，缺点是如果中间忘了关机赶上过节那费用就烧开了。</summary>
      
    
    
    
    
    <category term="docker" scheme="http://oilbeater.com/tags/docker/"/>
    
    <category term="vscode" scheme="http://oilbeater.com/tags/vscode/"/>
    
    <category term="tools" scheme="http://oilbeater.com/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>minikube，kind 和 k3d 大比拼</title>
    <link href="http://oilbeater.com/2024/02/22/minikube-vs-kind-vs-k3d/"/>
    <id>http://oilbeater.com/2024/02/22/minikube-vs-kind-vs-k3d/</id>
    <published>2024-02-22T14:42:22.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<p>作为云原生生态的一个开发者，开发中经常碰到的一个需求是要频繁测试应用在 Kubernetes 环境下的运行状态，在 CI 中可能还要快速测试多个不同 Kubernetes 集群的配置，例如单点，高可用，双栈，多集群等等。因此能够低成本的在本地单机环境快速创建管理 Kubernetes 集群就成了一个刚需。本文将介绍几个常见的单机 Kubernetes 管理工具 minikube, kind 和 k3d 各自的特点、使用场景以及可能的坑。</p><blockquote><p>TL;DR<br>如果你只关心快不快，那么 k3d 是最好的选择。如果你关心的是兼容性以及测试尽可能模拟真实场景，那么 minikube 是最稳妥的选择。kind 算是在这两个之间的一个平衡。</p></blockquote><ul><li><a href="#%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF%E6%AF%94%E8%BE%83">技术路线比较</a><ul><li><a href="#minikube">minikube</a></li><li><a href="#kind">kind</a></li><li><a href="#k3d">k3d</a></li></ul></li><li><a href="#%E5%90%AF%E5%8A%A8%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83">启动性能比较</a><ul><li><a href="#%E6%B5%8B%E8%AF%95%E6%96%B9%E6%B3%95">测试方法</a></li><li><a href="#%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C">测试结果</a></li></ul></li><li><a href="#%E6%80%BB%E7%BB%93">总结</a></li></ul><h1 id="技术路线比较"><a href="#技术路线比较" class="headerlink" title="技术路线比较"></a>技术路线比较</h1><p>这三者大体功能是类似的，都能够完成单机管理 Kubernetes 的任务，但是由于一些历史原因和技术选项导致了一些细节和使用场景的差异。</p><h2 id="minikube"><a href="#minikube" class="headerlink" title="minikube"></a>minikube</h2><p><a href="https://minikube.sigs.k8s.io/docs/">minikube</a> 是 Kubernetes 社区最早的一款快速在本地创建 Kubernetes 的软件，也是很多老人第一次上手 Kubernetes 的工具。早期版本是通过在本机创建 VM 来模拟多节点集群，这个方案的好处是能够最大程度还原真实场景，一些操作系统级别的变化，例如不同 OS，不同内核模块都可以覆盖到。缺点就是资源消耗太大，而且在一些虚拟化环境如果没有嵌套虚拟化的支持是没办法运行的，并且启动的速度也比较慢。不过社区最近也推出了 Docker 的 Driver 这些问题都得到了比较好的解决，不过对应代价就是一些虚拟机级别的模拟就不好做了。此外 minikube 还提供了不少的 addon，比如 dashboard，nginx-ingress 等常见的社区组件都能快速的安装使用。</p><h2 id="kind"><a href="#kind" class="headerlink" title="kind"></a>kind</h2><p><a href="https://kind.sigs.k8s.io/">kind</a> 是近几年流行起来的一个本地部署 Kubernetes 的工具，他的主要特点就是用 Docker 容器模拟节点，并且基本只专注在 Kubernetes 标准部署这一个事情上，其他社区组件都需要额外自己去安装。目前 Kubernetes 本身的 CI 也是通过 kind 来跑的。优点就是启动速度很快，熟悉 Docker 的人用起来也很顺手。缺点是用了容器模拟缺乏操作系统级别的隔离，而且和宿主机共享内核，一些操作系统相关的测试就不好测试了。我之前在测一个内核模块的时候就因为宿主机加了一些 netfilter 功能，结果 kind 里的 Kubernetes 集群挂了。</p><h2 id="k3d"><a href="#k3d" class="headerlink" title="k3d"></a>k3d</h2><p><a href="https://k3d.io/stable/">k3d</a> 是一个超轻量的本地部署 Kubernetes 工具，他的大体思路和 kind 类似，都是通过 Docker 来模拟节点，主要区别在于部署的不是个标准 Kubernetes 而是一个轻量级的 <a href="https://k3s.io/">k3s</a>，所以他的大部分优缺点也来自于下面这个 k3s。优点就是安装极致的快，你先别管对不对，你就问快不快吧。缺点主要来自于为了速度做出的一些牺牲，比如镜像用的是个超精简的操作系统，连 glibc 都没有，因此一些要在操作系统层面的操作都会无比困难。还有就是他的安装方式和常见的 kubeadm 也不一样，Kubernetes 组件都不是容器启动的，如果依赖标准部署的一些特性可能都会比较困难。</p><h1 id="启动性能比较"><a href="#启动性能比较" class="headerlink" title="启动性能比较"></a>启动性能比较</h1><p>minikube 社区有一些<a href="https://minikube.sigs.k8s.io/docs/benchmarks/timetok8s/v1.32.0/">性能测试报告</a>，正好对比的就是本文关注的三款软件的启动速度，不过我更关注的是其他的一些方面，比如镜像大小，内存占用以及最小化安装的启动时间，所以还是再做了一组测试。</p><h2 id="测试方法"><a href="#测试方法" class="headerlink" title="测试方法"></a>测试方法</h2><p>由于三款软件都是一行命令就可以启动，测试还是比较方便的，主要注意以下几个点：</p><ol><li>minikube 采用 Docker Driver，因为真要测启动速度还用虚拟机的 Driver 就没什么意义了。</li><li>所有测试都是镜像已经下载到本地的结果，不会涉及网络下载时间。</li><li>测试的每个软件都是当前的最新版，但是他们支持的 Kubernetes 版本不一致，不是很严谨，但是定性分析应该够了。</li><li>都只启动最基本的组件，不安装其他插件，但是基础 CNI 和 CoreDNS 以及 CSI 都是有的，保证应用的基本运行。</li><li>使用 <code>docker image</code> 命令查看镜像大小，使用 <code>docker stat</code> 查看内存用量。</li></ol><p>测试的命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#minikube</span></span><br><span class="line">time minikube start --driver=docker --force</span><br><span class="line"></span><br><span class="line"><span class="comment">#kind</span></span><br><span class="line">time kind create cluster</span><br><span class="line"></span><br><span class="line"><span class="comment">#k3d</span></span><br><span class="line">time k3d cluster create mycluster --k3s-arg <span class="string">&#x27;--disable=traefik,metrics-server@server:*&#x27;</span> --no-lb</span><br></pre></td></tr></table></figure><h2 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h2><table><thead><tr><th>名称</th><th>软件版本</th><th>Kubernetes 版本</th><th>镜像大小</th><th>启动时间</th><th>内存消耗</th></tr></thead><tbody><tr><td>minikube</td><td>v1.32.0</td><td>v1.28.3</td><td>1.2GB</td><td>29s</td><td>536MiB</td></tr><tr><td>kind</td><td>v0.22</td><td>v1.29.2</td><td>956MB</td><td>20s</td><td>463MiB</td></tr><tr><td>k3d</td><td>v5.6.0</td><td>v1.27.4</td><td>263MB</td><td>7s</td><td>423MiB</td></tr></tbody></table><p>可见单从启动性能这个指标，k3d 在镜像大小，启动时间和内存消耗几个方面都有比较大的优势，对于用 Github 免费 Action 跑 CI 的穷人还是很有吸引力的。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>如果快和更少的资源占用是最重要的目标，那么 k3d 相当合适，如果要测试需要操作系统级别隔离的功能，那么 minikube 的虚拟机 Driver 是唯一的选择，其他场景下 kind 会在兼容和性能之间比较平衡。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;作为云原生生态的一个开发者，开发中经常碰到的一个需求是要频繁测试应用在 Kubernetes 环境下的运行状态，在 CI 中可能还要快速测试多个不同 Kubernetes 集群的配置，例如单点，高可用，双栈，多集群等等。因此能够低成本的在本地单机环境快速创建管理 Kuber</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/tags/kubernetes/"/>
    
    <category term="ci" scheme="http://oilbeater.com/tags/ci/"/>
    
  </entry>
  
  <entry>
    <title>Golang 中预分配 slice 内存对性能的影响（续）</title>
    <link href="http://oilbeater.com/2024/01/09/alloc-slice-for-golang-2-md/"/>
    <id>http://oilbeater.com/2024/01/09/alloc-slice-for-golang-2-md/</id>
    <published>2024-01-09T08:16:22.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#%E5%9F%BA%E7%A1%80%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95">基础性能测试</a></li><li><a href="#%E6%95%B4%E4%B8%AA-slice-append">整个 Slice Append</a></li><li><a href="#%E5%A4%8D%E7%94%A8-slice">复用 Slice</a></li><li><a href="#syncpool">sync.Pool</a></li><li><a href="#bytebufferpool">bytebufferpool</a></li><li><a href="#%E6%80%BB%E7%BB%93">总结</a></li></ul><p>之前写了一篇 <a href="https://oilbeater.com/2023/07/19/pre-alloc-slice-for-golang/">Golang 中预分配 slice 内存对性能的影响</a>，探讨了一下在 Slice 中预分配内存对性能的影响，之前考虑的场景比较简单，最近又做了一些其他测试，补充一下进一步的信息。包括整个 Slice append，sync.Pool 对性能的影响。</p><h1 id="基础性能测试"><a href="#基础性能测试" class="headerlink" title="基础性能测试"></a>基础性能测试</h1><p>最初的 BenchMark 代码，只考虑了 Slice 是否初始化分配空间的情况，具体的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> prealloc_test</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;sync&quot;</span></span><br><span class="line">    <span class="string">&quot;testing&quot;</span></span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">var</span> length = <span class="number">1024</span></span><br><span class="line"><span class="keyword">var</span> testtext = <span class="built_in">make</span>([]<span class="type">byte</span>, length, length)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocateByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">        <span class="keyword">var</span> init []<span class="type">byte</span></span><br><span class="line">        <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">            init = <span class="built_in">append</span>(init, testtext[j])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocateByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">        <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">            init = <span class="built_in">append</span>(init, testtext[j])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        569978              2151 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          804807              1304 ns/op            1024 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>可见没有预分配的情况下多了 8 次内存分配，两个相对比可以粗略的认为 40% 的时间消耗在了这额外的 8 次内存分配。</p><p>这两个测试用例使用的是循环里逐个 append 元素，但是 Slice 还支持整个 Slice 进行 append 在这种情况下的性能差距是没有体现出来的。而且在这两个测试用例里我们其实无法知道内存分配所占的时间消耗占整个时间的占比。</p><h1 id="整个-Slice-Append"><a href="#整个-Slice-Append" class="headerlink" title="整个 Slice Append"></a>整个 Slice Append</h1><p>因此加入两个整个 Slice Append 的测试用例，观察预分配内存对性能还有没有这么大的影响。新增的用例代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">        <span class="keyword">var</span> init []<span class="type">byte</span></span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        569978              2151 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          804807              1304 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                3829890               311.5 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3968048               306.7 ns/op          1024 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>可见两个用例都只用了一次内存分配，消耗时间几乎相同，而且大幅低于逐个元素进行 append 的情况。一方面整个 Slice append，在 Slice 扩容时就知道了最终的大小没必要进行动态内存分配，降低了内存分配的开销。另一方面整个 Slice append 在实现上会进行整段复制，降低了循环的开销，性能会提升很多。</p><p>但在这里每次还是会有一次内存分配，我们依然无法确定这一次内存分配所占的整体时间比例。</p><h1 id="复用-Slice"><a href="#复用-Slice" class="headerlink" title="复用 Slice"></a>复用 Slice</h1><p>为了计算一次内存分配的消耗，我们设计一个新的测试用例，将 Slice 的创建放到循环外，循环内每次最后将 Slice 的 length 设为 0，给下次进行复用。这样在大量的测试下只会进行一次内存分配，平均下来就可以忽略不计了。具体的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate2</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">        init = init[:<span class="number">0</span>]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        514904              2171 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          761772              1333 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                4041459               320.9 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3854649               320.1 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate2-12                63147178                18.63 ns/op            0 B/op          0 allocs/op</span><br></pre></td></tr></table></figure><p>可见这次测试统计上没有内存分配了，整体消耗时间也降为了之前的 5%。因此大致可以计算出在之前的测试用例里每一次内存分配会消耗 95% 的时间，这个占比还是很惊人的。因此对于性能敏感的场景还是需要尽可能的复用对象，避免反复的对象创建的内存开销。</p><h1 id="sync-Pool"><a href="#sync-Pool" class="headerlink" title="sync.Pool"></a>sync.Pool</h1><p>简单的场景下可以像上个测试用例里一样手动的清空 Slice 在循环内进行复用，但是真实场景里对象的创建通常会发生在代码的各个地方，就需要统一的进行管理和复用了，Golang 里的 <code>sync.Pool</code> 就是做这个事情的，而且使用起来也很简单。但是内部实现还是比较复杂的，为了性能进行了大量无锁化的设计，具体实现可以参考<a href="https://unskilled.blog/posts/lets-dive-a-tour-of-sync.pool-internals/">Let’s dive: a tour of sync.Pool internals</a>。</p><p>使用 <code>sync.Pool</code> 重新设计的测试用例如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> sPool = &amp;sync.Pool&#123; </span><br><span class="line">        New: <span class="function"><span class="keyword">func</span><span class="params">()</span></span> any &#123;</span><br><span class="line">                b := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">                <span class="keyword">return</span> &amp;b</span><br><span class="line">        &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPoolByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">        b.ResetTimer()</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">                <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">                b := sPool.Get().(*[]<span class="type">byte</span>)</span><br><span class="line">                buf := *b</span><br><span class="line">                <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">                        buf = <span class="built_in">append</span>(buf, testtext[j])</span><br><span class="line">                &#125;</span><br><span class="line">                buf = buf[:<span class="number">0</span>]</span><br><span class="line">                sPool.Put(b)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPool</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">        b.ResetTimer()</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">                <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">                bufPtr := sPool.Get().(*[]<span class="type">byte</span>)</span><br><span class="line">                buf := * bufPtr</span><br><span class="line">                buf = <span class="built_in">append</span>(buf, testtext...)</span><br><span class="line">                buf = buf[:<span class="number">0</span>]</span><br><span class="line">                sPool.Put(bufPtr)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中 <code>New</code> 用来给 <code>sync.Pool</code> 一个在没有可用对象时创建对象的构造函数，使用的时候使用 <code>Get</code> 方法从 Pool 里获取一个对象，用完了再用 <code>Put</code> 方法把对象还给 <code>sync.Pool</code>。这里主要注意一下对象的生命周期，以及放回到 <code>sync.Pool</code> 时需要清空对象，避免脏数据。测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        469431              2313 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          802392              1339 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPoolByElement-12                1212828               961.5 ns/op             0 B/op          0 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                3249004               370.2 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3268851               368.2 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate2-12                62596077                18.63 ns/op            0 B/op          0 allocs/op</span><br><span class="line">BenchmarkPool-12                        32707296                35.59 ns/op            0 B/op          0 allocs/op</span><br></pre></td></tr></table></figure><p>可见使用 <code>sync.Pool</code> 也可以避免内存分配，由于 <code>sync.Pool</code> 还有一些额外的处理性能消耗会比手动复用 Slice 稍高一些，不过考虑到使用的便利性以及相比不使用还是有明显的性能提升，还是一个不错的方案。</p><p>但是直接使用 <code>sync.Pool</code> 也有下面两个问题：</p><ol><li>对于 Slice 的情况 <code>New</code> 分配的初始内存是固定的，运行时使用空间超出的话，可能还会有大量动态的内存分配调整。</li><li>另一个极端是 Slice 被动态扩容很大后放回到 <code>sync.Pool</code> 中，可能会造成内存的泄漏和浪费。</li></ol><h1 id="bytebufferpool"><a href="#bytebufferpool" class="headerlink" title="bytebufferpool"></a>bytebufferpool</h1><p>为了达到实际运行时更优的性能，<a href="https://github.com/valyala/bytebufferpool">bytebufferpool</a> 这个项目在 <code>sync.Pool</code> 的基础上运用了一些简单的统计规律，尽可能的减少了上面提到的两个问题在运行时的影响。（该项目的作者是俄罗斯人，手下还有 fasthttp, quicktemplate 和 VictoriaMetrics 几个项目，个顶个都是性能优化的优秀案例，战斗民族经常会搞这种性能推极限的项目。</p><p>代码里主要的结构如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// bytebufferpool/pool.go</span></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">  minBitSize = <span class="number">6</span> <span class="comment">// 2**6=64 is a CPU cache line size</span></span><br><span class="line">  steps      = <span class="number">20</span></span><br><span class="line"> </span><br><span class="line">  minSize = <span class="number">1</span> &lt;&lt; minBitSize</span><br><span class="line">  maxSize = <span class="number">1</span> &lt;&lt; (minBitSize + steps - <span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">  calibrateCallsThreshold = <span class="number">42000</span></span><br><span class="line">  maxPercentile           = <span class="number">0.95</span></span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">type</span> Pool <span class="keyword">struct</span> &#123;</span><br><span class="line">  calls       [steps]<span class="type">uint64</span></span><br><span class="line">  calibrating <span class="type">uint64</span></span><br><span class="line"> </span><br><span class="line">  defaultSize <span class="type">uint64</span></span><br><span class="line">  maxSize     <span class="type">uint64</span></span><br><span class="line"> </span><br><span class="line">  pool sync.Pool</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中 <code>defaultSize</code> 的作用是 <code>New</code> 的时候给 Slice 分配的大小，<code>maxSize</code> 的作用是超过这个大小的 Slice <code>Put</code> 时会拒绝。核心的算法其实就是在运行时根据统计到的 Slice 使用大小信息动态的去调整 <code>defaultSize</code> 和 <code>maxSize</code> ，避免额外的内存分配同时还要避免内存泄漏。</p><p>这个动态统计的过程也比较简单，就是将 <code>Put</code> 到 Pool 里的 Slice 大小划分了 20 个区间范围进行统计，当 <code>Put</code> 次数达到 <code>calibrating</code> 后就进行一次排序，将这个时间段内使用最为频繁的区间大小作为 <code>defaultSize</code> 这样在统计上就可以避免不少额外的内存分配。然后按大小排序，将 95% 分位大小设置为  <code>maxSize</code>，这样就避免了在统计上长尾大的对象进入 Pool。就靠着这样动态调整这两个值，在统计上可以在运行时获得更优的性能。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>Slice 初始化尽可能指定 capacity</li><li>避免在循环中初始化 Slice</li><li>性能敏感路径考虑使用 <code>sync.Pool</code></li><li>内存分配的性能开销可能远大于业务逻辑</li><li>bytebuffer 的复用可以考虑看下 bytebufferpool</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%9F%BA%E7%A1%80%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95&quot;&gt;基础性能测试&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%95%B4%E4%B8%AA-slice-append&quot;&gt;整个</summary>
      
    
    
    
    
    <category term="性能" scheme="http://oilbeater.com/tags/%E6%80%A7%E8%83%BD/"/>
    
    <category term="golang" scheme="http://oilbeater.com/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>不需要封装的 Overlay 容器网络</title>
    <link href="http://oilbeater.com/2023/07/30/xmasq-overlay-without-encap/"/>
    <id>http://oilbeater.com/2023/07/30/xmasq-overlay-without-encap/</id>
    <published>2023-07-30T05:24:33.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8-overlay-%E7%BD%91%E7%BB%9C">为什么要用 Overlay 网络</a></li><li><a href="#%E5%AE%B9%E5%99%A8-overlay-%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BC%80%E9%94%80%E6%9D%A5%E6%BA%90">容器 Overlay 网络的开销来源</a></li><li><a href="#%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3-overlay-%E5%B0%81%E8%A3%85%E7%9A%84%E5%BC%80%E9%94%80">如何解决 Overlay 封装的开销</a></li><li><a href="#%E6%88%91%E7%9A%84%E6%83%B3%E6%B3%95">我的想法</a></li></ul><p>上周在看 <a href="https://antrea.io/">Antrea</a> 的<a href="https://github.com/antrea-io/antrea/wiki/Community-Meetings#july-17-2023">会议纪要</a>时，发现了一篇上海交大和 VMware 合作的论文 —— <a href="https://arxiv.org/pdf/2305.05455.pdf">XMasq: Low-Overhead Container Overlay Network Based on eBPF</a>。论文里介绍不少通过 eBPF 做容器网络性能优化的方法，不过最令人印象深刻的是他们最终无需给容器数据包封装额外的隧道头就可以实现 Overlay 网络，这里就简单介绍下作者们是如何做到这一点的。</p><h2 id="为什么要用-Overlay-网络"><a href="#为什么要用-Overlay-网络" class="headerlink" title="为什么要用 Overlay 网络"></a>为什么要用 Overlay 网络</h2><p>Overlay 网络相比 Underlay 网络可以完全解耦应用层和底层物理网络,确保两者的灵活性。应用层不需要关心底层物理网络的路由规则等细节,物理网络也不需要针对容器 IP 进行专门路由配置。因此 Overlay 网络作为一种不调底层网络实现的容器组网技术，快速的在容器领域铺开了，主流的开源网络插件处于安装兼容的考虑，基本上都会将 Overlay 作为默认的安装选项。</p><h2 id="容器-Overlay-网络的开销来源"><a href="#容器-Overlay-网络的开销来源" class="headerlink" title="容器 Overlay 网络的开销来源"></a>容器 Overlay 网络的开销来源</h2><p>但是在灵活性的便利下，带来的是性能方面的开销。根据论文里的测量，主流的 Overlay 网络插件，相比于 HOST 网络，吞吐量会下降 25%，CPU 消耗会增加 34%~44%，延迟会上升 45%~58%。</p><p>当然，这里面的性能开销并不完全是 Overlay 的封装带来的，由于容器网络本身的流量路径和 HOST 网络就有比较大的区别，Overlay 封装其实只是其中的一环。如下图所示：<br><img src="/../images/container-network-overhead.png" alt="Alt text"></p><p>其中 1~4，7~10 都是容器网络相比宿主机网络额外带来的链路，Overlay 的封装主要在 4 和 7，分别占到了 egress 和 ingress 开销的 24% 和 29%。</p><p>其他几个链路上的优化在 Cilium 和其他开源项目上其实已经见到过一些了，主要思路是通过 eBPF bypaas 掉部分链路，将数据包能够从容器网络栈直通到物理网卡，下面主要介绍下这篇论文是如何解决 Overlay 封装的问题。</p><h2 id="如何解决-Overlay-封装的开销"><a href="#如何解决-Overlay-封装的开销" class="headerlink" title="如何解决 Overlay 封装的开销"></a>如何解决 Overlay 封装的开销</h2><p>常见的 Overlay 隧道如 VXlan 和 Geneve 都是通过给容器网络的数据包封装一个 UDP 的 Header 实现的 Overlay。外层的 UDP 头部记录宿主机实际的 IP 和 Mac，内层数据包的 Header 里记录容器网络的 IP 和 Mac，外层作为真实网络的通信标识，内层作为容器网络的通信标识。那么有没有可能把两层的信息通过一层全带走，从而实现不需要额外的 UDP Header 呢？这就是这篇论文作者的一个很有意思的工作。</p><p>外层的 IP 和 Mac 作为真实通信的标识是不可能省略的，但是 IP 的 Header 中有一部分字段比如 DSCP 和 ID 是很少被使用的。如果能将这两个字段利用起来，编码内层的信息，那么我们就可以不用内层的容器 IP 和 Mac 这一层了。这里可以想象一下用 iptables 来做 nat 其实是将源 IP 和端口映射成目标 IP 和端口，也可以理解为一种编码的映射关系。</p><p>作者在内核中通过 eBPF 的 Map 缓存了容器网络的 Mac 和 IP 信息，并生成了一个 key 来对应每一组 IP 和 Mac。这样，当容器再进行跨主机网络通信时，可以直接将目标地址修改为对应宿主机的地址，同时将这个 key 写入 IP 中的指定的保留字段。等数据包到达对端后，对端就可以通过这个 key 查询本地的缓存将数据包的地址再还原为容器的地址。这样并不需要额外的封装，通过 IP 和 Mac 头的直接替换，就完成了跨主机的 Overlay 网络。</p><p>由于论文中还应用了很多其他 bypass 的优化，没有单独衡量 Overlay 改造带来的性能提升，从整体效果来看性能优化后的效果较普通 Overlay 网络有很大的提升基本接近 HOST 网络。</p><h2 id="我的想法"><a href="#我的想法" class="headerlink" title="我的想法"></a>我的想法</h2><p>按照我之前的经验使用 UDP 进行 Overlay 封装还有一个很大的性能问题，那就是如果内部数据包是 TCP 的时候，TCP 其实有很多网卡的 Offload 优化，能大幅提升吞吐量，但如果是封装后这种 TCP in UDP 的形式，很多网卡 Offload 的能力就失效了。我们之前在一些虚拟化的平台，使用 UDP 的封装吞吐量只有使用 STT 这种 TCP 进行封装的十分之一。不过这个差别也和网卡的能力，操作系统内核相关，在作者测试的网卡上并没有表现出来明显的差异，有可能是物理网卡支持这种 TCP in UDP 的 Offload。</p><p>这篇论文的目的是希望把这套性能优化做成一个 CNI 无关的插件，所有的容器网络都可以使用，但是论文里大量的 eBPF bypaas 其实也会跳过 CNI Datapath 的处理，会导致原有 CNI 功能的缺失。所以做一个通用插件的意义可能并不大。直接做一个新的 CNI ，不要求其他功能，只追求 Overlay 的通用性和极致的性能其实更合适一些，没准能拿下不少的细分领域市场。反而是 Overlay 封装这块如果能单独拿出来，做成和 VXlan，Geneve 并列的一种封装格式，倒是能真正做到 CNI 无关的一个通用模块。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8-overlay-%E7%BD%91%E7%BB%9C&quot;&gt;为什么要用 Overlay 网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%AE%</summary>
      
    
    
    
    
    <category term="network" scheme="http://oilbeater.com/tags/network/"/>
    
    <category term="performance" scheme="http://oilbeater.com/tags/performance/"/>
    
  </entry>
  
  <entry>
    <title>如何在本地用 CPU 跑大模型</title>
    <link href="http://oilbeater.com/2023/07/27/local-cpu-llm/"/>
    <id>http://oilbeater.com/2023/07/27/local-cpu-llm/</id>
    <published>2023-07-27T10:51:07.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<p>之前觉得大模型在本地运行是个不可能的事情，我甚至没有一块 GPU。但是尝试了一下 llama.cpp 发现几分钟就把 Meta 最新开源的 Llama 2 在 CPU 上运行起来了，速度和质量都还可以接受，很多想法突然就变得可行了。记录一下搭建的过程，希望对感兴趣的人有帮助。</p><h1 id="需要的配置"><a href="#需要的配置" class="headerlink" title="需要的配置"></a>需要的配置</h1><p>尽管不需要 GPU 和加速卡，但是大模型对磁盘和内存还是有需求的，我运行的是 Llama 2 7B 的模型，经过了 GGML 处理，选择了 int8 的精度。这个模型跑起推理来大概需要 7G 的磁盘空间，运行起来也需要 7G 左右的内存，长期运行的话内存有 10G 会比较保险。CPU 的话 1 核也能跑，但给到 6 核会比较流畅。</p><h1 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h1><p>下载模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin</span><br></pre></td></tr></table></figure><p>这是一个处理过的模型，减小了体积和精度，专门为端侧运行做了优化，也不需要申请 Meta 的授权直接就能下载。</p><p>下载并安装 <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ggerganov/llama.cpp.git</span><br><span class="line"><span class="built_in">cd</span> llama.cpp</span><br><span class="line">make</span><br></pre></td></tr></table></figure><p>这是一个高性能的 Llama 推理工具集，可以方便的做 chat，api 等等服务。</p><p>移动模型文件到对应目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> ../llama-2-7b-chat.ggmlv3.q8_0.bin models/</span><br></pre></td></tr></table></figure><p>好了现在你就可以在本地运行一个 chat 了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./main -m ./models/llama-2-7b-chat.ggmlv3.q8_0.bin -c 512 -b </span><br><span class="line">1024 -n 256 --keep 48 \</span><br><span class="line">    --repeat_penalty 1.0 --color -i -t 4 \</span><br><span class="line">    -r <span class="string">&quot;User:&quot;</span> -f prompts/chat-with-bob.txt</span><br></pre></td></tr></table></figure><h1 id="更多配置"><a href="#更多配置" class="headerlink" title="更多配置"></a>更多配置</h1><p>在 examples 目录下还可以看到 llama.cpp 的其他用法，比如提供 API 服务，提供 embedding 和 fine-tune，甚至还有一个兼容 OpenAI API 的转换器。</p><p>如果你的机器有 GPU 或者用的是 M 系列芯片的 Mac 那么可以通过 make 参数提高推理的性能。</p><p>如果你的机器再厉害一些可以考虑去 huggingface 上去下载更大参数量的模型。</p><h1 id="为什么要本地运行大模型"><a href="#为什么要本地运行大模型" class="headerlink" title="为什么要本地运行大模型"></a>为什么要本地运行大模型</h1><p>其实我手头 OpenAI 的 GPT3&#x2F;GPT3，Google Bard，Claude 2 的 API 使用权都有，那为啥还要费心思在本地用 CPU 跑质量并不如他们的大模型呢？</p><p>第一个原因是穷，和我想做的事情相关。我想用大模型做代码分析和信息摘要，一个项目可能有上万个 commit 跑一遍就破产了。</p><p>第二个原因是穷，买不起 GPU 和 M 系列的 Mac，只能找穷人的方法。</p><p>第三个原因是我觉得未来端侧的大模型会更实用，也更能贴近个人的场景进行垂直方向的定制化，想借着这个机会看看这个领域到啥地步了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;之前觉得大模型在本地运行是个不可能的事情，我甚至没有一块 GPU。但是尝试了一下 llama.cpp 发现几分钟就把 Meta 最新开源的 Llama 2 在 CPU 上运行起来了，速度和质量都还可以接受，很多想法突然就变得可行了。记录一下搭建的过程，希望对感兴趣的人有帮助</summary>
      
    
    
    
    
    <category term="llama" scheme="http://oilbeater.com/tags/llama/"/>
    
    <category term="AI" scheme="http://oilbeater.com/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>新的网络性能优化技术 —— BIG TCP</title>
    <link href="http://oilbeater.com/2023/07/24/big-tcp/"/>
    <id>http://oilbeater.com/2023/07/24/big-tcp/</id>
    <published>2023-07-24T15:21:15.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<p>在翻 Cilium Release Blog 时发现了一个叫 Big TCP 的内核技术，这个技术今年 2 月左右刚被合并到内核。看介绍是专门为 100Gbit 以上的高速网络设计的，在将吞吐量提升 50% 的时候还能大幅降低延迟，于是就去了解了一下这个技术。</p><h1 id="高速网络的性能的瓶颈和误区"><a href="#高速网络的性能的瓶颈和误区" class="headerlink" title="高速网络的性能的瓶颈和误区"></a>高速网络的性能的瓶颈和误区</h1><p>提到网络性能，很多人会很自然的联想到网卡的性能，但是做过网络性能优化的人会知道瓶颈更多其实是在 CPU 上。一般使用 iperf3、qperf 这类压测软件进行测试，尤其在小包情况下基本是无法打满网卡带宽的，这时候 CPU 会先于网卡到达瓶颈。</p><p>因此做网络性能的优化通常做的都是 CPUU 相关的优化，不管是 DPDK，Offload，XDP 加速的原理要么是绕过内核栈，要么是卸载部分工作给网卡，本质上都是为了节省 CPU 资源，来处理更多的数据包。</p><p>以 100G 的网卡为例，以太网的 MTU 是 1500 字节，那么在不做任何优化的情况下 CPU 想要跑满网卡，每秒要处理将近 800 万个数据包，如果每个数据包都要走完整的网络栈，现代 CPU 单核还远远处理不了这个量级的数据包。</p><p>为了能让单核跑出尽可能好的网络性能，就需要内核和网卡驱动共同协作，将一部分工作 Offload 给网卡，这也就是 GRO(Generic Receive Offload) 和 TSO(TCP Segment Offload) 相关的技术。</p><h1 id="GRO-和-TSO"><a href="#GRO-和-TSO" class="headerlink" title="GRO 和 TSO"></a>GRO 和 TSO</h1><p>CPU 处理不了那么多数据的一个原因是需要给每个数据包封装协议头，计算校验和等等会浪费大量的资源，而在 100G 这种带宽下 1500 的 MTU 又实在太小了，不得不拆分出这么多小的数据包。如果将数据包的拆分和组装交给网卡来做，那么内核需要处理的数据包的规模就降下来了，这就是 GRO 和 TSO 提升性能的基本原理。</p><p><img src="/../images/tso-gro.png" alt="Alt text"></p><p>如上图所示，在发送数据包的时候内核可以按照 64K 来处理数据包，网卡通过 TSO 拆分成 1.5K 的小包进行传输，接收端网卡通过 GRO 将小包聚合成 64K 的大包在发送给内核处理。这样一来，内核需要处理包的数量就降为了原来的四十分之一。因此理论上开了这两个 Offload 网络的吞吐量在大包的情况下会有数十倍的提升，在我们之前的测试中这个性能大概会差十倍左右。</p><p>而且在主流内核和网卡驱动里只支持 TCP 的 Offload，所以经常会看到的一个不太符合直觉的现象就是 iperf3 的 TCP 性能要远远好于 UDP 的性能，差距大概也在十倍左右。</p><h1 id="Big-TCP-又做了什么？"><a href="#Big-TCP-又做了什么？" class="headerlink" title="Big TCP 又做了什么？"></a>Big TCP 又做了什么？</h1><p>既然已经有了这个 Offload 那么 Big TCP 又做了什么呢？Big TCP 要做的事情就是让内核里处理的这个数据包变的更大，这样整体要处理的数据包就会进一步下降来提升性能。之前内核处理的数据包大小为 64K 的主要限制在于 IP 包的头部有个长度字段，这个字段长度为 16bit，因此理论上一个 IP 包最大长度就是 64K。</p><p>怎样才能突破这个长度限制呢，内核的作者在这里用了一些很 hack 的方法，在 IPv6 的数据包中有一个 hop-by-hop 的 32 位字段可以存储一些附加信息，那么内核里就可以把 IP 包的长度设置为 0，然后从 hop-by-hop 字段中获取真实的数据包长度，这样一个数据包最大就可以到 4GB 的容量。但是处于稳妥的考虑，目前最大只能设置为 512K，即使这样要处理的数据包也变为了原来的八分之一，相比没有卸载的情况就是将近三百分之一。</p><p>根据开发者的<a href="https://lwn.net/Articles/883713/">测试</a>，吞吐量有将近 50% 的提升，延迟也有将近 30% 的下降，效果可以说相当显著了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;Standard&#x27;</span> performance with current (74KB) limits.</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..10&#125;; <span class="keyword">do</span> ./netperf -t TCP_RR -H iroa23  -- -r80000,80000 -O MIN_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT|<span class="built_in">tail</span> -1; <span class="keyword">done</span></span><br><span class="line">77           138          183          8542.19    </span><br><span class="line">79           143          178          8215.28    </span><br><span class="line">70           117          164          9543.39    </span><br><span class="line">80           144          176          8183.71    </span><br><span class="line">78           126          155          9108.47    </span><br><span class="line">80           146          184          8115.19    </span><br><span class="line">71           113          165          9510.96    </span><br><span class="line">74           113          164          9518.74    </span><br><span class="line">79           137          178          8575.04    </span><br><span class="line">73           111          171          9561.73    </span><br><span class="line"></span><br><span class="line">Now <span class="built_in">enable</span> BIG TCP on both hosts.</span><br><span class="line"></span><br><span class="line">ip <span class="built_in">link</span> <span class="built_in">set</span> dev eth0 gro_ipv6_max_size 185000 gso_ipv6_max_size 185000</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..10&#125;; <span class="keyword">do</span> ./netperf -t TCP_RR -H iroa23  -- -r80000,80000 -O MIN_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT|<span class="built_in">tail</span> -1; <span class="keyword">done</span></span><br><span class="line">57           83           117          13871.38   </span><br><span class="line">64           118          155          11432.94   </span><br><span class="line">65           116          148          11507.62   </span><br><span class="line">60           105          136          12645.15   </span><br><span class="line">60           103          135          12760.34   </span><br><span class="line">60           102          134          12832.64   </span><br><span class="line">62           109          132          10877.68   </span><br><span class="line">58           82           115          14052.93   </span><br><span class="line">57           83           124          14212.58   </span><br><span class="line">57           82           119          14196.01 </span><br></pre></td></tr></table></figure><p>而 <a href="https://lwn.net/Articles/920017/">Big TCP 的 IPv4 支持</a>要晚一些，主要原因是 IPv4 内并没有类似 IPv6 中 hop-by-hop 的可选信息字段，因此也就不能在 IP 头中保存真实长度信息。不过作者另辟蹊径，直接从内核的 skb-&gt;len 中计算数据包的真实长度信息，反正这个数据包只要在送出去之后长度是正确的就可以，在机器内部其实可以完全依赖 skb 保存状态信息，这样实现了超过 64K 的 TCP 数据包。理论上 IPv4 的做法会更通用，不过 IPv6 已经先实现了，所以就存在了两种不同实现 Big TCP 的方案。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这个技术对内核和硬件驱动都有一定的要求，内核需要 6.3 才正式支持，而网卡驱动的支持可能还需要联系硬件厂商。但 Big TCP 还是一个值得令人期待的技术，能够在不需要应用程序调整的情况下显著提升网络的性能，在特定场景下的收益还是很大的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在翻 Cilium Release Blog 时发现了一个叫 Big TCP 的内核技术，这个技术今年 2 月左右刚被合并到内核。看介绍是专门为 100Gbit 以上的高速网络设计的，在将吞吐量提升 50% 的时候还能大幅降低延迟，于是就去了解了一下这个技术。&lt;/p&gt;
&lt;h</summary>
      
    
    
    
    
    <category term="network" scheme="http://oilbeater.com/tags/network/"/>
    
    <category term="performance" scheme="http://oilbeater.com/tags/performance/"/>
    
    <category term="kernel" scheme="http://oilbeater.com/tags/kernel/"/>
    
  </entry>
  
  <entry>
    <title>Golang 中预分配 slice 内存对性能的影响</title>
    <link href="http://oilbeater.com/2023/07/19/pre-alloc-slice-for-golang/"/>
    <id>http://oilbeater.com/2023/07/19/pre-alloc-slice-for-golang/</id>
    <published>2023-07-19T10:48:09.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#slice-%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80">Slice 内存分配理论基础</a></li><li><a href="#%E5%AE%9A%E9%87%8F%E6%B5%8B%E9%87%8F">定量测量</a></li><li><a href="#lint-%E5%B7%A5%E5%85%B7-prealloc">Lint 工具 prealloc</a></li><li><a href="#%E6%80%BB%E7%BB%93">总结</a></li></ul><p>在我代码 review 的过程中经常会关注代码里 slice 的初始化是否分配了预期的内存空间，也就是凡是 <code>var init []int64</code> 的我都会要求尽可能改成 <code>init := make([]int64, 0, length)</code> 格式。但是这个改进对性能究竟有多少影响并没有什么定量的概念，只是教条的去要求。这篇博客会介绍一下预分配内存提升性能的理论基础，定量测量，和自动化检测发现的工具。</p><h1 id="Slice-内存分配理论基础"><a href="#Slice-内存分配理论基础" class="headerlink" title="Slice 内存分配理论基础"></a>Slice 内存分配理论基础</h1><p>Golang Slice 扩容的代码在<a href="https://github.com/golang/go/blob/go1.20.6/src/runtime/slice.go#L157">slice.go 下的 growslice</a>。大体思路是在 Slice 容量小于 256 时<br>每次扩容会创建一个容量翻倍的新 slice；当容量大于 256 后，每次扩容会创建一个容量为原先的 1.25 倍的新 slice。之后会将旧 slice 的数据复制到新的 slice，最终返回新的 slice。</p><p>扩容的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">newcap := oldCap</span><br><span class="line">doublecap := newcap + newcap</span><br><span class="line"><span class="keyword">if</span> newLen &gt; doublecap &#123;</span><br><span class="line">newcap = newLen</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">const</span> threshold = <span class="number">256</span></span><br><span class="line"><span class="keyword">if</span> oldCap &lt; threshold &#123;</span><br><span class="line">newcap = doublecap</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// Check 0 &lt; newcap to detect overflow</span></span><br><span class="line"><span class="comment">// and prevent an infinite loop.</span></span><br><span class="line"><span class="keyword">for</span> <span class="number">0</span> &lt; newcap &amp;&amp; newcap &lt; newLen &#123;</span><br><span class="line"><span class="comment">// Transition from growing 2x for small slices</span></span><br><span class="line"><span class="comment">// to growing 1.25x for large slices. This formula</span></span><br><span class="line"><span class="comment">// gives a smooth-ish transition between the two.</span></span><br><span class="line">newcap += (newcap + <span class="number">3</span>*threshold) / <span class="number">4</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Set newcap to the requested cap when</span></span><br><span class="line"><span class="comment">// the newcap calculation overflowed.</span></span><br><span class="line"><span class="keyword">if</span> newcap &lt;= <span class="number">0</span> &#123;</span><br><span class="line">newcap = newLen</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因此理论上如果预分配好 slice 的容量，不需要动态扩张我们可以在好几个地方有性能的提升：</p><ol><li>内存只需要一次分配，不需要反复分配。</li><li>不需要反复进行数据复制。</li><li>不需要反复对旧的 slice 进行垃圾回收。</li><li>内存准确分配，不存在动态分配导致的容量浪费。</li></ol><p>理论上来看，预分配 slice 容量相比动态分配会带来性能提升，但具体提升有多少就需要定量测量了。</p><h1 id="定量测量"><a href="#定量测量" class="headerlink" title="定量测量"></a>定量测量</h1><p>我们参考 <a href="https://github.com/alexkohler/prealloc/blob/master/prealloc_test.go">prealloc</a> 的代码进行简单修改来测量不同容量的 slice 预分配和动态分配对性能的影响。</p><p>测试代码如下，通过修改 <code>length</code> 可以观察不同情况下的性能数据：</p><figure class="highlight go"><figcaption><span>title</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> prealloc_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;testing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> length = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">b.ResetTimer()</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line"><span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line"><span class="keyword">var</span> init []<span class="type">int64</span></span><br><span class="line"><span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">init = <span class="built_in">append</span>(init, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">b.ResetTimer()</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line"><span class="comment">// Preallocate our initial slice</span></span><br><span class="line">init := <span class="built_in">make</span>([]<span class="type">int64</span>, <span class="number">0</span>, length)</span><br><span class="line"><span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">init = <span class="built_in">append</span>(init, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一个函数测试动态分配的性能，第二个函数测试预分配的性能。通过下面的命令可以执行测试：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go <span class="built_in">test</span> -bench=. -benchmem prealloc_test.go</span><br></pre></td></tr></table></figure><p>在 <code>length = 1</code> 情况下的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocate-12       40228154                27.36 ns/op            8 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12         55662463                19.97 ns/op            8 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>在 <code>length</code> 为 1 的情况下，理论上动态分配和静态分配都要进行一次初始化的内存分配，性能不应该有差异，但是实测下来，预分配的耗时为动态分配的 70%，即使在两者内存分配次数一直的情况下，预分配依然有 1.4x 的性能优势。目测性能提升和变量的连续分配相关。</p><p>在 <code>length = 10</code> 情况下的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocate-12        5402014               228.3 ns/op           248 B/op          5 allocs/op</span><br><span class="line">BenchmarkPreallocate-12         21908133                50.46 ns/op           80 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>在 &#96;length&#96;&#96; 为 10 的情况下，预分配依然只进行了一次性能分配，动态分配进行了 5 次性能分配，预分配的性能是动态分配性能的 4 倍。可见即使在 slice 规模较小的时候，预分配依然会有比较明显的性能提升。</p><p>下面是在 <code>length</code> 分别为 129,1025 和 10000 情况下的测试结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># length = 129</span></span><br><span class="line">BenchmarkNoPreallocate-12         743293              1393 ns/op            4088 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocate-12          3124831               386.1 ns/op          1152 B/op          1 allocs/op</span><br><span class="line"></span><br><span class="line"><span class="comment"># length = 1025</span></span><br><span class="line">BenchmarkNoPreallocate-12         169700              6571 ns/op           25208 B/op         12 allocs/op</span><br><span class="line">BenchmarkPreallocate-12           468880              2495 ns/op            9472 B/op          1 allocs/op</span><br><span class="line"></span><br><span class="line"><span class="comment"># length = 10000</span></span><br><span class="line">BenchmarkNoPreallocate-12          14430             86427 ns/op          357625 B/op         19 allocs/op</span><br><span class="line">BenchmarkPreallocate-12            56220             20693 ns/op           81920 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>在更大容量下，静态分配依然只做一次内存分配，但是性能提升并没有相应成倍增长，整体性能会是动态分配的 2 到 4 倍。应该是在这个过程中有一些其他的消耗，或者 golang 对大容量的复制有特殊的优化，因此性能差距并没有拉大。</p><p>当把 slice 的内容换成更复杂的 struct 时，原以为复制会带来更大的性能开销，但实测复杂 struct 预分配和动态分配的性能差距反而更小，看上去还是有很多内部的优化，表现和直觉并不一致。</p><h1 id="Lint-工具-prealloc"><a href="#Lint-工具-prealloc" class="headerlink" title="Lint 工具 prealloc"></a>Lint 工具 prealloc</h1><p>尽管预分配内存可以带来一定的性能提升，但是在比较大的项目中完全依赖人工去 review 这个问题很容易出现纰漏。这时候就需要用到一些 lint 工具来自动做代码扫描了。<a href="https://github.com/alexkohler/prealloc">prealloc</a> 就是这样一个工具可以扫描潜在的能够预分配但却没有预分配的 slice，并且可以整合到 <a href="https://golangci-lint.run/usage/linters/#prealloc">golangci-lint</a> 中。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>整体来看 slice 的内存预分配是个比较简单但却能有比较好优化效果的方法，即使在 slice 容量很小的情况下，预分配依然能有比较明显的性能提升。通过 prealloc 这种静态代码扫描工具，可以比较方便的实现这类潜在优化的检测并集成到 CI 中简化日后的操作。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#slice-%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80&quot;&gt;Slice 内存分配理论基础&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5</summary>
      
    
    
    
    
    <category term="性能" scheme="http://oilbeater.com/tags/%E6%80%A7%E8%83%BD/"/>
    
    <category term="golang" scheme="http://oilbeater.com/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>无需改动代码的性能优化方法 —— PGO</title>
    <link href="http://oilbeater.com/2023/06/24/optimization-without-changing-code-pgo/"/>
    <id>http://oilbeater.com/2023/06/24/optimization-without-changing-code-pgo/</id>
    <published>2023-06-24T08:58:29.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#pgo-in-general">PGO in General</a></li><li><a href="#pgo-in-golang">PGO in Golang</a><ul><li><a href="#%E6%94%B6%E9%9B%86-profile-%E4%BF%A1%E6%81%AF">收集 Profile 信息</a></li><li><a href="#%E7%BC%96%E8%AF%91%E4%BC%98%E5%8C%96">编译优化</a></li></ul></li><li><a href="#pgo-for-kernel">PGO for Kernel</a></li><li><a href="#%E6%80%BB%E7%BB%93">总结</a></li></ul><p>Golang 在 1.20 引入了 <a href="https://go.dev/doc/pgo">PGO(Profile-guided optimization)</a> 的优化，根据官方博客的介绍可以在不更改代码的情况下达到 2%-7% 的性能提升。在 1.21 的 <a href="https://tip.golang.org/doc/go1.21">Release Note</a> 中，Golang 将该功能升级到 GA 并在自己的构建中开启了 PGO，将 Golang 自身编译器的性能提升了 2%-4%。PGO 本身是个编译器的优化方法，并不和特定语言相关，Golang 目前也只是很初步的应用了 PGO，这篇文章将会以 Golang 为例介绍 PGO 的工作原理，可能的优化方向和一个应用 PGO 优化 Linux Kernel 的例子。</p><h1 id="PGO-in-General"><a href="#PGO-in-General" class="headerlink" title="PGO in General"></a>PGO in General</h1><p>通常我们认为静态编译型语言的运行性能要好于动态解释型的语言，但是随着 <a href="https://en.wikipedia.org/wiki/Just-in-time_compilation">JIT(Just In Time)</a> 技术的引入，动态语言性能有了大幅提升，因为 JIT 可以根据运行时的情况进行针对的强化。</p><p>例如：</p><ul><li>热点代码编译成机器码</li><li>函数内联</li><li>分支预测</li><li>循环展开</li><li>类型推断</li><li>内存分配优化</li><li>寄存器优化</li></ul><p>JIT 通过一系列运行时的优化可以达到和编译型语言接近的性能，在部分情况下由于可以动态根据情况做一些编译期间无法确认的优化，甚至会有比编译型语言更好的性能。例如 Golang 中的函数内联是写死的规则稍，主要看函数大小，而不是函数使用频率。分支预测和循环展开编译器由于不知道分支运行时的频率分布，也无法做专门的优化。Golang 中 Slice 和 Map 的初始化大小需要通过参数手动指定或者自动根据写死的规则进行扩容，无法根据运行时信息分配一个合适的初始化大小。</p><p>于是一个很自然的想法就是能否将类似 JIT 的技术应用到编译型语言，通过运行时的信息去优化代码性能，传统的做法是通过人类对代码运行时理解的经验，主动的在代码中加入编译提示信息，帮助编译器优化，例如 C++ 中的 inline 函数，C 语言中的宏，GCC 提供的 likely&#x2F;unlikely。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> likely(x) __builtin_expect(!!(x), 1) <span class="comment">//gcc内置函数, 帮助编译器分支优化</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> unlikely(x) __builtin_expect(!!(x), 0)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>* argv[])</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> x = <span class="number">0</span>;</span><br><span class="line">    x = <span class="built_in">atoi</span>(argv[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">unlikely</span>(x == <span class="number">3</span>))&#123;  <span class="comment">//告诉编译器这个分支非常不可能为true</span></span><br><span class="line">        x = x + <span class="number">9</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        x = x - <span class="number">8</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;x=%d\n&quot;</span>, x);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这种方式存在几个问题：</p><ol><li>需要对所有分支，函数进行考虑，工作量大，并且调整复杂。</li><li>需要依赖程序员凭借经验推测，不一定符合实际场景。</li><li>只能对自己编写的代码进行调整比较方便，一些依赖的库函数调整困难，比如一个应用开发者可能需要向下调整到 glibc 的代码。</li></ol><p>那么能不能有一种方法可以不调整代码，自动根据运行时的信息对代码的编译进行全局的优化呢，这个方法就是 PGO。通过收集运行时的 Profile 信息反过来优化编译过程。</p><p>一个典型的使用 PGO 的工作流分为下面几步：</p><ol><li>构建初始版本二进制，不带任何 PGO 的优化。</li><li>在生产环境收集 Profile 信息。</li><li>重新构建二进制，并使用 2 中收集的 Profile 信息进行构建优化。</li><li>回到 2，持续迭代。</li></ol><p>Google 通过 <a href="https://research.google/pubs/pub45290/">AutoFDO</a> 实现了持续的 PGO，同时也介绍了 PGO 存在的问题：</p><ol><li>需要配合 pprof 信息，生产环境完整的 Profile 会带来性能下降，通常使用 Sample 的方式牺牲一定精度，开销在 1%。</li><li>需要随着代码动态调整 Profile，不是个一次性的优化，Profile 信息可能会有泄密隐患，如何构建完整流程是个挑战。</li><li>整体性能提升有限，整体性能优化在 10% 左右。</li><li>二进制体积增加。</li></ol><h1 id="PGO-in-Golang"><a href="#PGO-in-Golang" class="headerlink" title="PGO in Golang"></a>PGO in Golang</h1><p>Golang 目前有两个主流的编译器，gc(go compiler) 和 <a href="https://github.com/golang/gofrontend">gccgo</a>。</p><table><thead><tr><th></th><th>gc</th><th>gccgo</th></tr></thead><tbody><tr><td>优点</td><td><ol>1. 官方支持</ol><ol>2. 兼容性好</ol><ol>3. 编译速度较快</ol></td><td><ol>1. 可以使用 GCC 更多优化能力，性能较好</ol><ol>2. 可以支持更多 CPU 架构和系统</ol></td></tr><tr><td>缺点</td><td><ol>1. 优化较为保守</ol></td><td><ol>1. 跟随 GCC 发版，不支持 Golang 新特性，潜在兼容性问题</ol><ol>2. 安装使用复杂</ol><ol>3. 较慢编译速度</ol></td></tr></tbody></table><p>gccgo 使用前后端分离架构，后端已经支持 PGO，这里主要讨论的是官方 gc 的 PGO 优化。</p><h2 id="收集-Profile-信息"><a href="#收集-Profile-信息" class="headerlink" title="收集 Profile 信息"></a>收集 Profile 信息</h2><p>Golang 的 PGO 目前只支持通过 CPU Profile 进行优化，我们可以通过 Golang 标准库里的 runtime&#x2F;pprof 或者 net&#x2F;http&#x2F;pprof 进行 CPU Profile 的采集，如果是其他的 Profile 采集器的数据如果能转换成 <a href="https://github.com/google/pprof/tree/main/proto">Google pprof</a> 的格式也可以兼容。</p><p>需要注意的是由于 PGO 使用的是类似 JIT 的优化方式，因此最好在真实的生产环境收集 Profile 信息，才能最贴合程序实际的运行条件，方便进行后期的优化。也可将多台机器上收集的 Profile 信息进行合并 <code>go tool pprof -proto a.pprof b.pprof &gt; merged.pprof</code>.</p><h2 id="编译优化"><a href="#编译优化" class="headerlink" title="编译优化"></a>编译优化</h2><p><code>go build -gpo=/tmp/foo.pprof</code> 即可在编译过程中通过 Profile 信息进行优化。Golang 目前针只实现了<a href="https://go-review.googlesource.com/c/proposal/+/430398/10/design/55022-pgo-implementation.md#208">函数内联优化</a>，尝试将调用比例大于 2% 的函数进行内联。更多的优化还在画大饼，目前画的大饼可以参考 <a href="https://github.com/golang/go/issues/55022#issuecomment-1245605666%E3%80%82">https://github.com/golang/go/issues/55022#issuecomment-1245605666。</a></p><p><img src="/../images/pgo-in-go.png" alt="Alt text"></p><p>在实践中经常碰到的场景是在跑完 profile 后不会直接重新编译，而是在下次代码变更后再编译发布。这样带来的问题就是 Profile 的信息和代码是存在差异的，Golang 目前通过一些启发式的规则，可以在代码和 Profile 不一致的情况下尽可能的工作。</p><h1 id="PGO-for-Kernel"><a href="#PGO-for-Kernel" class="headerlink" title="PGO for Kernel"></a>PGO for Kernel</h1><p>PGO 一般用于应用的性能优化，但是一些服务器上的应用有可能是系统调用密集型的，在这种情况下其实可以根据应用对内核进行 PGO，打造一个针对应用优化过后的内核。这里可以参考一下早些年我上学时实验室一个大神师弟的工作 <a href="http://coolypf.com/kpgo.htm">Profile-Guided Operating System Kernel Optimization</a>，我当时也是通过这个工作才了解到了 PGO。</p><p><img src="/../images/kernel-pgo.png" alt="Alt text"></p><p>图片里显示的是不同软件在优化后的吞吐量提升，可以看到在完全不用动应用代码，只是优化内核的编译，吞吐量就会有 2%~10% 的提升。在 Nginx 的例子里出现了性能的下降，我记的当时大神说过是 GCC 在 PGO 的一个 Bug，导致了错误的优化。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>PGO 是一个不需要改动代码就可以获得性能优化的方法，结合 Golang 的 pprof 可以做很好的配合，但是优化程度也相应有限，而且需要配合发布上线流程，可以作为一个性能优化的尝试。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#pgo-in-general&quot;&gt;PGO in General&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#pgo-in-golang&quot;&gt;PGO in Golang&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%94%B6%E9%9B%</summary>
      
    
    
    
    
    <category term="性能" scheme="http://oilbeater.com/tags/%E6%80%A7%E8%83%BD/"/>
    
    <category term="golang" scheme="http://oilbeater.com/tags/golang/"/>
    
    <category term="PGO" scheme="http://oilbeater.com/tags/PGO/"/>
    
    <category term="Linux" scheme="http://oilbeater.com/tags/Linux/"/>
    
    <category term="Kernel" scheme="http://oilbeater.com/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>我和耗子叔的回忆</title>
    <link href="http://oilbeater.com/2023/05/19/memory-with-haochen/"/>
    <id>http://oilbeater.com/2023/05/19/memory-with-haochen/</id>
    <published>2023-05-19T00:05:02.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<h1 id="我和耗子叔的回忆"><a href="#我和耗子叔的回忆" class="headerlink" title="我和耗子叔的回忆"></a>我和耗子叔的回忆</h1><h2 id="初始酷壳"><a href="#初始酷壳" class="headerlink" title="初始酷壳"></a>初始酷壳</h2><p>和很多人一样，第一次接触耗子叔是通过 coolshell.cn 博客。</p><p>当时印象最深，对我之后影响最大的应该是 <a href="https://coolshell.cn/articles/2105.html">分享：我的READER订阅</a> 。当年尽管我已经是大三的计算机专业的学生，但是课本外的专业知识还是相当差劲，也不知道哪里才能获得一手的信息和资料。让我一个象牙塔里的学生，接触到工业界最先进的技术就是从这个列表开始的。这个 RSS 的列表陪伴我度过了很多年，很长时间我的计算机技术相关的信息都是依赖这个列表。</p><h2 id="亚马逊的擦肩而过"><a href="#亚马逊的擦肩而过" class="headerlink" title="亚马逊的擦肩而过"></a>亚马逊的擦肩而过</h2><p>几年后在我实习的时候，碰巧去了亚马逊中国耗子叔在的那个部门，不过当时耗子叔已经去阿里了，没能在亚马逊碰到。实习结束后我专门写了篇博客记录当时在亚马逊实习的经历，抱着蹭流量的心态我在微博发博客的时候 @ 了一下耗子叔。本来是抱着蹭到就算赚的心态，没想到耗子叔很快就转发了，当时博客流量就上来了，我还拿出去和同学们炫耀了一段。开始写博客的初衷也是受耗子叔博客的影响，这个博客几乎是我早些年工作没啥成就时，唯一能拿得出手的东西了。</p><p><img src="/../images/image1.png" alt="image1.png"></p><h2 id="从阿里到-Docker"><a href="#从阿里到-Docker" class="headerlink" title="从阿里到 Docker"></a>从阿里到 Docker</h2><p>第一份工作去了阿里，总算是和耗子叔到了一个公司，但是部门差的太远，没有什么业务的交叉，起初也就没什么交流。当时 Docker 开始崭露头角了，阿里内部各个部门都有一拨人都看到了这个技术的突破性，想在阿里内部推广这个技术，几个不同部门的人组了个群聊这些事，我和耗子叔凑巧也在这个群里。当时我还是个应届生，肯定掀不起什么风浪，但其他高级别的人也都碰到了不少阻碍，所以不断有人跳出去单干，我们也戏称这个群变成了前橙会群。</p><p>在当时阿里内部大领导已经拍板不会推进 Docker 了，耗子叔在这个过程中也遇到了不少风波，里面涉及到很多复杂的人和事这里就不多说了。内部做不了，我当时其实也有了去找个创业公司做 Docker 的想法。</p><p>作为社恐的我那天还是鼓足勇气和耗子叔私聊了这件事，耗子叔还是出乎我意料的快速的就恢复我了。在谈到容器在阿里的时候耗子叔情绪还是挺激动的，觉得阿里做事不地道，在阿里搞 Docker 已经不可能了，说了很多的气话。然后我再提及了想去外面的创业公司做 Docker 的事情，本以为情绪都烘到这个地步了，应该会和我再吐槽几句阿里是垃圾，让我赶快去外面吧。结果耗子叔话头一转，开始劝我在阿里先稳住，有一定积累再去看外面的机会。</p><p><img src="/../images/image2.png" alt="image2.png"></p><p>我在那一刻突然意识到，耗子叔是那种我之前很少没见过的，并不是以公司立场甚至不是以个人的立场去考虑问题，而是真的完全出于为你好的立场去考虑问题。而我当时其实只是想找个大牛去肯定我的一个决策。</p><p>之后我还是去了那个创业公司，一直到现在，今天回过来看，如果从经济的角度那我当年还是应该听话的。鬼知道阿里转身就是 All in 云原生。</p><h2 id="多年后的合作"><a href="#多年后的合作" class="headerlink" title="多年后的合作"></a>多年后的合作</h2><p>之后很长一段时间和耗子叔都没什么联系，最近几年我开始做开源项目，一直不瘟不火，不知道该怎么去推广。这时候耗子叔也开始创业了，并且把公司内部的技术分享放到网上，我就又萌生了去蹭流量的想法。说来惭愧，这么多年都没帮过什么忙，每次一有事情还都想着去蹭。另一方面耗子叔创业也在云原生领域，所谓同行是冤家，了解这个圈子的应该知道这个圈子友商之间交情都挺差的，见面不打架就算客气了，我这还想明目张胆的去蹭流量。</p><p>本着蹭到就算赚我还是发了微信，想在他们的分享会上分享一下我们最近的技术进展。耗子叔依然出乎我意料快速的就答应了我，让然后就张罗起来了。</p><p><img src="/../images/image3.png" alt="image3.png"></p><p>尽管很多年都没联系，但耗子叔像老朋友一样就要和我语音聊聊，一聊就聊了一个半小时。从云原生这个领域来说，我们俩几乎是同时进入的，要说正儿八经当职业来做我还更早一些，我本来以为自己还算老兵了，尤其在网络这个领域我也还算可以，但耗子叔的很多观点还是让我觉得自己狭隘了。</p><p>我们聊到当时为了让 kube-ovn 这个项目能被更多人使用，我做了很多在云原生领域并不推荐的功能，让更多非云原生最佳实践也能跑起来，这个成为了我们早期吸引用户的最大卖点。耗子叔很直接的指出我的做法过于功利了，快速的获取用户变成了我最优先的目标，为了完成这个目标我其实在鼓励用户使用并不符合最佳实践的做法。虽然我的做法能吸引用户，但是并没有让用户发挥出云原生最大的价值，这种形式上的云原生最终并不会产生业务上的收益。如果是他的话，他会劝说用户尽可能使用最佳实践，而他所有产品功能也都是依托最佳实践基础来做的，尽管初期改造后会很痛苦，但是改造后能够确定性的有大幅度的改善，而不是为了云原生而云原生。</p><p>说实话，在之前我一直觉得云原生领域已经挺无聊的了，东西就是那些，每天都是各种和现实磨合，甚至都忘了当初是为啥要加入这个领域。大概是理想多次被现实扭曲，所以才变得这么功利吧。</p><p>耗子叔之后介绍了当时 MegaEase 的愿景，当时在外面能看到的还只有一个网关，但背后其实有个更大的梦想。耗子叔希望透过流量入口和云原生的技术把复杂的多云管理变的简单，通过云原生可以做一个跨公有云的系统，符合这个最佳实践的应用，存储、网络、计算都可以在多个云间快速的迁移。这样可以把多个公有云变成自己的资源池，把价格给打下来，给用户提供更优质低价的云服务。</p><p>当时听完后我是有点懵的，技术上我是听懂了的，但是还是被里面的一些有想象力的做法惊住了。耗子叔看气氛差不多了就问我，你们公司现在做的产品我看过了，挺没意思的，也不咋地，你要不要过来和我干这个？我内心 os 了一句你知道做这个不咋地的东西我有多努力嘛？嘴上说那等我把这个公司做黄了再去你们那吧。耗子叔听完哈哈一笑，说这倒也不至于，不至于。</p><p>那时候我们都还以为会有合作的机会，没想到之后再也不会有机会了。</p><h2 id="回想"><a href="#回想" class="headerlink" title="回想"></a>回想</h2><p>回想这些年来和耗子叔的交往，每次都是我有求于他，每次我多觉得这是不是太占用他宝贵的资源了。但是每次都会得到他毫不迟疑的，全情投入的帮助。他对技术理想的执着，对有着技术热情人的支持，只需要只言片语就深深的感染了我，让我在现实的阴霾中又看到了理想的光辉。如果我们每个人做事的时候想想耗子叔哲哲情况会怎么做，遇到有人需要帮助时想想如果是耗子叔会怎样帮我，那我想耗子叔不只有数字分身，也会在我们每个人的精神世界中有个分身。我想说他是个高尚的人，纯粹的人，在搜这句话出处是发现出自《纪念白求恩》，里面的一些话似乎正是再说耗子叔，就让这段引文作为这个纪念的结尾吧。</p><blockquote><p>从前线回来的人说到白求恩，没有一个不佩服，没有一个不为他的精神所感动。晋察冀边区的军民，凡亲身受过白求恩医生的治疗和亲眼看过白求恩医生的工作的，无不为之感动。每一个共产党员，一定要学习白求恩同志的这种真正共产主义者的精神。<br>白求恩同志是个医生，他以医疗为职业，对技术精益求精；在整个八路军医务系统中，他的医术是很高明的。这对于一班见异思迁的人，对于一班鄙薄技术工作以为不足道、以为无出路的人，也是一个极好的教训。<br>我和白求恩同志只见过一面。后来他给我来过许多信。可是因为忙，仅回过他一封信，还不知他收到没有。对于他的死，我是很悲痛的。现在大家纪念他，可见他的精神感人之深。我们大家要学习他毫无自私自利之心的精神。从这点出发，就可以变为大有利于人民的人。一个人能力有大小，但只要有这点精神，就是一个高尚的人，一个纯粹的人，一个有道德的人，一个脱离了低级趣味的人，一个有益于人民的人。 </p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;我和耗子叔的回忆&quot;&gt;&lt;a href=&quot;#我和耗子叔的回忆&quot; class=&quot;headerlink&quot; title=&quot;我和耗子叔的回忆&quot;&gt;&lt;/a&gt;我和耗子叔的回忆&lt;/h1&gt;&lt;h2 id=&quot;初始酷壳&quot;&gt;&lt;a href=&quot;#初始酷壳&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>2023 周报 #2：Github Action 优化和 ChatGPT</title>
    <link href="http://oilbeater.com/2023/02/26/2023-Weekly-2/"/>
    <id>http://oilbeater.com/2023/02/26/2023-Weekly-2/</id>
    <published>2023-02-26T13:49:45.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Github-Action-优化"><a href="#Github-Action-优化" class="headerlink" title="Github Action 优化"></a>Github Action 优化</h1><p>Kube-OVN 前段时间做了 E2E 测试的重构，增加了大量的测试，导致 Github Action 经常被占满，PR 经常好久才能合并，这周花时间和小伙伴优化了 Action 的执行时间，如果你也在 Github 上运行 Kubernetes 相关的 workflow 下面的优化方法可能会对你有帮助。 </p><ul><li>根据代码变动选择运行的测试和构建。比如在 <code>Kube-OVN</code> 中 <code>NetworkPolicy</code> 兼容性测试花费时间较长，那就可以根据代码目录判断，如果变动代码和 NetworkPolicy 无关，这部分测试就不运行了。</li><li>Github public repo 最大并发数是 20，如果 workflow 拆分的太细，超过 20 个并发，那么准备阶段的开销可能会大于并发节省的时间，需要在并发数量和每次任务的准备时间之间做权衡。</li><li>Cache 的 restore 和 save 都需要花时间，1G 内容 restore 大概 45s，save 要 3min，要考虑流水线的情况选择使哪些使用 cache，哪些就不用了。</li><li><code>kind</code> 起一个 <code>Kubernetes</code> 集群大概要 1m，<code>k3d</code> 差不多 10s，我们的环境有些兼容性问题，如果没有兼容性问题 CI 测试可以优先使用 <code>k3d</code>。</li><li><code>Ginkgo</code> 有并发测试的选项，很多 <code>Kubernetes</code> 的测试需要等待状态就绪，CPU 消耗并不大，可以适度增加并发度。</li><li>同样是因为 <code>Kubernetes</code> 中有很多等待状态的 <code>sleep</code>，可以把 <code>sleep</code> 间隔调小。比如 <code>sleep</code> 从 5s 调到 1s，那平均下来每次运行会节省 2s。</li><li><code>kubectl delete</code> 会默认等待删除完全结束，在删除 Pod 时可能会花较长时间，尤其是批量删除会一个个等，可以用 <code>--wait=false</code> 避免等待。</li><li>同样是和等待状态相关，设了 <code>readinessProbe</code> 的工作负载会等待 <code>initialDelaySeconds</code> 之后再探测，状态才会变成 <code>ready</code>，可以把 <code>initialDelaySeconds</code> 删掉，这样能加速启动。</li></ul><h1 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h1><p>这周菲律宾的 globe 手机卡到手了，总算能成功注册一个 openAI 的账号，不需要关各种代理的限制，能比较充分的去使用了。这个星期主要用 chatGPT 做了如下几个事情：</p><ul><li>写了封拖了很久的英文邮件，输入邮件的目的和一些背景信息让它写，第一次输出的有些过于客气和啰嗦，让它自己简化了一次后又太干脆了，结合两个版本自己微调一下就发出去了，前后不到 5 分钟就搞定了。</li><li>优化了给 Cilium 提的一个 Feature Request，这次是我自己先写了个初版，中途有些概念问题和一些背景知识问了一下，最后把文本交给他优化，一下子感觉 native 了不少，也是再微调一下就直接发了。</li><li>看的一些英文内容，不太理解的地方直接把整段辅助过去让他给我解释，不仅有翻译，还有背景知识，比一般的词典都好用。比如这句 <code>No provider or user of an interactive computer service shall be treated as the publisher or speaker of any information provided by another information content provider.</code></li><li>学 JavaScript 的过程中临时遇到的一些疑问，想到了就去问，概念和入门性的东西讲的相当详细，感觉就算找个老师都不会那么生动耐心的去讲。</li><li>直接帮我写了几个 JavaScript 的函数，发现 chatGPT 超级喜欢正则，还会详细讲解正则表达式，最后还给了几个测试用例。不过有一个函数正则写错了，看测试用例看出问题，告诉他哪里有问题，他就改正了。</li><li>帮我爸写了篇退休感言，帮我爸同事要写的书列了个大纲，编了个猪八戒和林黛玉的故事。我爸思考了一天后和我说文字工作者不存在了，现在的人想要学东西太容易了，编这个程序的人不简单，这个东西会不会卡脖子，小爱什么的不是一个层次的东西。</li><li>问 <code>logseq</code> 一个不存在的分享发布功能，他非说有，还告诉我是怎么实现的。页面交互，后台逻辑，可能用到的技术都编出来了，其中一个用 gist 来实现的方案看上去还不错。</li><li>让他帮我列了一下今年的 Roadmap，高度吻合后又让他列了下竞品的 Roadmap。</li></ul><p>给我的感受是使用 chatGPT 后，很多任务的难度从创作级别降到了 review 级别，不管是文本生成，代码生成甚至是 Roadmap 生成，原本需要从 0 开始创作，现在变成了基于一个还不错的输出做质量检查和微调。由于任务完全变成了另一个类型，从产出的角度来讲效率绝对是大幅提升了。但是我隐隐有些担心如果形成了路径依赖，跳过了中间过程直接拿到结果，创造的能力会不会受到影响？</p><p>对于用 chatGPT 辅助学习，实际效果也很不错，相比搜索引擎需要在很多信息中做过滤，chatGPT 能一下子提供直接相关的信息，可以减少很多注意力的转移。不过胡说八道的情况存在的比例也不低，还是要有一定的判断。当感觉出有问题的时候还是要追问几句，再去搜索引擎交叉验证一下。</p><p>我希望未来所有软硬件的用户交互方式都能提供一个 chat 的接口，在经历了鼠标、键盘、触屏后又能以一种更高效的形式回归到语音交互。想一想这不就是老罗当年的 TNT 么。  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Github-Action-优化&quot;&gt;&lt;a href=&quot;#Github-Action-优化&quot; class=&quot;headerlink&quot; title=&quot;Github Action 优化&quot;&gt;&lt;/a&gt;Github Action 优化&lt;/h1&gt;&lt;p&gt;Kube-OVN 前段时间做</summary>
      
    
    
    
    
    <category term="周报" scheme="http://oilbeater.com/tags/%E5%91%A8%E6%8A%A5/"/>
    
  </entry>
  
  <entry>
    <title>2023 周报 #1：Twitter，第二大脑和数字移民</title>
    <link href="http://oilbeater.com/2023/02/19/2023-weekly-1/"/>
    <id>http://oilbeater.com/2023/02/19/2023-weekly-1/</id>
    <published>2023-02-19T15:27:45.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<p>最近在 Twitter 上关注了很多写博客的小伙伴，发现很多都有写周报的习惯，DNA 又动了。给自己写周报是个有意思的事情，能在忙碌中让自己找到生活中的闪光点，于是就跟一下风，看看自己能不能重拾写作的乐趣。</p><h1 id="重回-Twitter"><a href="#重回-Twitter" class="headerlink" title="重回 Twitter"></a>重回 Twitter</h1><p>我的 Twitter 账号 13 年前就注册了，但是最近为了能更好的学一下英语表达才开始使用。意外发现里面的中文内容质量很高，而且发现了很多有意思的小伙伴。</p><p>和国内的很多社交平台相比，Twitter 限制更少的环境更能激发思考和表达的欲望。尽管在国内平台有些话题的讨论并不涉及审核的红线，但是审核的存在还是会潜移默化的影响表达的质量，并且压制了表达。当然在 Twitter 上也得设置重重的过滤规则才能让自己的 Timeline 不是那么乌烟瘴气。</p><p>后续我的一些观点和想法主要都会在 Twitter 上发布，感兴趣的小伙伴也可以关注一下<a href="https://twitter.com/liumengxinfly">我的 Twitter</a>。</p><p>在 Twitter 上发现了个奇怪的现象，每当国内有个地方出新闻，想去搜索的时候发现全是些奇奇怪怪的推文排在最前，难道这是另一种奇怪的信息管制？</p><p>总体感觉下来 Twitter 经过过滤后还是个不错的信息源，但是收集会存在一定的困难。feedly 等订阅工具需要付费，<a href="https://twitter.com/SaveToNotion">SaveToNotion</a> 和 <a href="https://twitter.com/readwise">readwise</a> 会产生大量无用的垃圾回复，影响 Timeline。目前看起来 logseq 的 Twitter block 可能是个相对轻量的收集方式了。</p><p>此外推荐一下 <a href="https://ponyexpress.notion.site/Twemex-Manual-71b320f4d95f48d38ca68fe6eaa3c49e">Twemex</a>， 是个不错的浏览器插件，可以展示关注用户历史上被 like 最多的 Tweet，快速查看精华内容。</p><h1 id="第二大脑"><a href="#第二大脑" class="headerlink" title="第二大脑"></a>第二大脑</h1><p>如果不了解第二大脑概念，可以参考一篇最近发现的博客 <a href="https://justgoidea.com/blog/post-024">在 Heptabase 中构建第二大脑</a>。第二大脑简单来说就是一个笔记，把日常学习、工作和生活中的思考记录下来，每个人多多少少其实都做过。但是能不能作为一个系统整体运转起来，在笔记背后其实还是要有一套工作流程和方法论去支撑的。</p><p>其实这个事情零零散散搭建了一段时间了，中途用过很多工具组合，也放弃过很多次，不过最近找到了一些感觉，开始持续的去运行这个系统。目前我这套系统里，主要是 feedly + 微信阅读（可以订阅公众号）做信息收集。初筛的信息（一般就是过一下标题和第一段）打 tag 后发到 Cubox 作为知识库。Cubox 里的内容精读后再把笔记和摘要输出到 logseq。</p><p>后续计划把部分 logseq 整理后的内容，用 <a href="https://github.com/pengx17/logseq-publish">logseq-publish</a> 发布出来，看看能不能和大家有更多的互动。</p><h1 id="数字移民"><a href="#数字移民" class="headerlink" title="数字移民"></a>数字移民</h1><p>事情的起因和最近大火的 chatGPT 有关，当想体验是发现遇到了重重阻碍。VPN、海外手机卡和海外信用卡其实绕路或者用第三方平台也能搞定，但用第三方平台尤其是手机号有比较大的安全隐患，注册的账号也容易出问题，并不是彻底的解决方案。于是在找合法途径办理国外的一套东西时关注到了数字移民。</p><p>数字移民这个事情甚至成为了一些政府的生意，感兴趣的可以看下 <a href="https://www.frontlinefellowship.io/blog/youyou?categoryId=320164">愛沙尼亞的中國「數字居民」：通往新身份之路</a> 如果想办理美国的数字身份可以参考<a href="https://twitter.com/madawei2699/status/1624564508168683527">这个推文</a>。</p><p>不过整体来看成本都不低，而且风险未知，我先试着搞套能用的海外手机号码和银行卡，后续有进展再来分享。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近在 Twitter 上关注了很多写博客的小伙伴，发现很多都有写周报的习惯，DNA 又动了。给自己写周报是个有意思的事情，能在忙碌中让自己找到生活中的闪光点，于是就跟一下风，看看自己能不能重拾写作的乐趣。&lt;/p&gt;
&lt;h1 id=&quot;重回-Twitter&quot;&gt;&lt;a href=&quot;</summary>
      
    
    
    
    
    <category term="周报" scheme="http://oilbeater.com/tags/%E5%91%A8%E6%8A%A5/"/>
    
  </entry>
  
  <entry>
    <title>烧钱工作法 —— 远程开发是怎样提升我的效率？</title>
    <link href="http://oilbeater.com/2023/01/15/how-remote-coding-improve-productivity/"/>
    <id>http://oilbeater.com/2023/01/15/how-remote-coding-improve-productivity/</id>
    <published>2023-01-15T04:20:45.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<p>这段时间为了解决本地开发环境资源和网络的问题尝试了下远程开发(IDE 前端在本地，代码后端再远端服务器的模式)，实践了一段时间发现远程开发带来的意外好处是可以通过实时烧钱大幅提升任务的紧迫度和开发效率，这个带来的效率提升甚至比之前预想的能从计算资源和网络里抠出来的还多。分享一下这段时间的经验，希望对考虑远程开发的人有帮助。</p><h2 id="为什么要远程开发"><a href="#为什么要远程开发" class="headerlink" title="为什么要远程开发"></a>为什么要远程开发</h2><p>最开始考虑远程开发的动机主要有下面几个：</p><ul><li>我这边的本地开发和测试可能需要启动多个 Kubernetes 集群，对 CPU 和内存消耗都比较大，17 年的 MacBook Pro 启动和运行速度都比较慢。</li><li>生活工作 App 在一个机器磁盘很容易莫秒奇妙的不够，尝试新项目下镜像经常磁盘不足，需要来回扣空间。</li><li>网络问题，需要各种切代理，叠加 Docker 环境会变得更复杂。</li><li>Mac 下的 Linux 开发还是存在工具和用法的不兼容，并不是那么平滑，很多时候还要起 Linux VM。</li><li>换新 Mac 貌似只能解决部分 CPU 性能问题（存疑，因为 Docker 需要虚拟化，ARM 在没有特定加速指令下可能性能会更差），其他问题依然存在，而且新 Mac 实在是在抢钱。</li><li>看了下 GCP 的 Spot 实例，可以按秒计费，算下来买电脑的钱购买好几年的实例了，而且理论上可以解决上面所有的问题。</li></ul><h2 id="选择过程"><a href="#选择过程" class="headerlink" title="选择过程"></a>选择过程</h2><p>最早开始调研的是 Github 的 CodeSpaces，因为和 Github 在一块的开发体验还是很流畅的，不过我用了多年 Goland 切 VS Code 不太习惯。而且貌似 CodeSpaces 是起的容器开发环境，而我这边有很多再跑容器的测试需求，有时还需要测 eBPF 需要完整的 Linux 环境就只能作罢。</p><p>回到 Goland 其实 JetBrains 自己也有类似 CodeSpaces 的 Space 提供全托管的服务，但问题也是类似的感觉提供的是个容器开发环境。AWS 和 GCP 上也提供了 Jetbrains 的半托管服务，会根据你的需求自动启停 VM 运行 IDE 的 Backend。但是看上去用的不是 Spot 实例，而且默认是两小时无交互才会关机。考虑到我这边如果流畅开发可能需要的资源比较多，这个浪费还是比较严重的。</p><p>最后我这边选择的方案是用最基础的 GCP 香港的 Spot 实例，几次调整后配置扩到了 8C16G，配合 EIP 固定公网地址，再通过脚本控制开关机来节省费用。Goland 这边下载个 Gateway 的插件，就可以引导你去通过 SSH（要给这个 SSH 设置代理，不然还是有网络不稳定情况）连接到自己的开发机器并做相应配置。这样就完成了远程开发环境的搭建，并且有个完整的 Linux 环境整体的配置都很灵活，还可以根据自己的需求继续扩充配置。</p><h2 id="使用体验"><a href="#使用体验" class="headerlink" title="使用体验"></a>使用体验</h2><p>该说不说，用上远程开发后，最初的几个问题都解决了，下载资源网络基本无感知，扩容到 8C16G 后代码的编译，起 kind 集群都飞快。</p><p>不过最直接的感受并不是以上那些，当启动机器的脚本启动后我这边就感觉点起了一个烧钱的火炉，由于 GCP 的计算资源是按秒计费的，那种感觉真的就是实时烧钱。所以基本上火炉一开就什么都顾不上了，注意里全部集中到了当前的任务上，根本不想分心。这段时间别人找我就根本不想理；要是有什么操作或者思路的错误导致耽误时间了，就想抽自己；之前懒得弄的一些脚本自动化，和一些工具的高效使用技巧，快捷键什么的也都开始研究了。由于这种被烧着工作的感觉过于刺激，有时候一些不想干又必须干的工作，并不是必须远程开发，比如文档什么的，我也会先把钱烧起来，然后就很刺激的完成了。</p><p>现在想起来其实想要提高自己的效率，并不一定要很强的自制力去痛苦的逼自己去做不想做的事情，而是要有不断的反馈让自己能尽快的看到效果。这种烧钱的负反馈效果可能比正反馈效果更好，GCP 这种实时烧钱的实时负反馈会刺激的完全不想别的事情了。</p><p>有感兴趣的可以也来试下，分享下体验，看看烧钱工作法是不是能提升你的效率。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这段时间为了解决本地开发环境资源和网络的问题尝试了下远程开发(IDE 前端在本地，代码后端再远端服务器的模式)，实践了一段时间发现远程开发带来的意外好处是可以通过实时烧钱大幅提升任务的紧迫度和开发效率，这个带来的效率提升甚至比之前预想的能从计算资源和网络里抠出来的还多。分享</summary>
      
    
    
    
    
    <category term="技术" scheme="http://oilbeater.com/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="效率" scheme="http://oilbeater.com/tags/%E6%95%88%E7%8E%87/"/>
    
    <category term="云计算" scheme="http://oilbeater.com/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    <category term="远程开发" scheme="http://oilbeater.com/tags/%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>2022 年终总结</title>
    <link href="http://oilbeater.com/2022/12/30/2022-summary/"/>
    <id>http://oilbeater.com/2022/12/30/2022-summary/</id>
    <published>2022-12-30T06:21:07.000Z</published>
    <updated>2025-01-12T08:28:37.261Z</updated>
    
    <content type="html"><![CDATA[<p>作为六年没有更新博客的僵尸博主，在看了 <a href="https://twitter.com/gaocegege">Ce Gao</a> 连续九年的<a href="http://gaocegege.com/Blog/%E9%9A%8F%E7%AC%94/newyear2022">年终总结</a>后 DNA 动了。收拾收拾博客，就有了这篇六年更的年度总结。在这里先祝不离不弃的各位过年好！</p><h2 id="晋升"><a href="#晋升" class="headerlink" title="晋升"></a>晋升</h2><p>今年做了工作八年来第一次晋升答辩，甚至之前都没做过转正答辩，然后一下就干到在这个公司的天花板了，拿下了名义上的技术合伙人。加上来的比较早还可以自称是初创工程师(Founding Engineer)，基本上把纯技术线能拿到的 Title 都收了。</p><p>回想一下这个过程，其实运气占了很大的成分，并不是个常规的流程。由于来的比较早，当时也算国内第一批参与容器的人，吃了不少技术的红利，现在再看看当初一块讨论技术的小伙伴，只要还在坚持做这块的，基本都飞黄腾达了。所谓一个人的命运，当然要靠自我奋斗，但是也要考虑到历史的进程。</p><p>尽管是走的技术线，但是这些年来纯技术的时间可能都不到一半。 反而是借着初创公司机制不完善和各个职位都缺人的机会，从市场、商务、售前、实施、售后的所有工作都接触了。甚至一直不善言谈的我，曾经是这个公司第二个售前，第一波去驻场的研发，以至于我有很长时间都是售前和驻场的面试官。</p><p>我自己来看，单论技术能力和产品能力，拿这个 Title 其实还是有段距离，但是这种非常规的一条龙经历，可以让我跳出常规技术维度的考察。</p><h2 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h2><p>落到具体的工作层面，今年明显碰到了墙。一方面今年正式有了团队要管理，技术相关的工作需要分配到团队其他人，同时其他岗位的人也在不断完善，原来我一个人就可以全包下来的一条线的职能需要不断分解给其他人。这对于之前单打独斗的我其实会很别扭，总感觉事情落不到实处，原来手拿把攥的事突然就变的虚无缥缈了，现在反而比自己之前亲力亲为更焦虑。今年来尝试着尽可能的从一线的动手处理上抽出来，把尽可能多的挑战和机会给到团队里的人，去帮助他们成长。但是过程中沟通，跨团队的协作，进度和质量的把控等方面等碰到了很多问题，收和放的度把握的也不是太好。希望新的一年里能和团队进一步成长，减少过程中的摩擦，能让事情自然而然的发生，也减少一些自己的焦虑。</p><p>另一方面本来今年希望在 Kube-OVN 的国际化上进行一些发力，但是语言上的墙还是结结实实的装上了。在一些 Issue 和 Slack 的异步文字沟通还好，但是当机会进一步发展需要对话的时候，几次线上交流的效果都很差。此外今年也收到了一些海外会议邀请我们去做分享，但同样又有语言能力的问题最终没有成形。最近也在从头看一些英语发音，表达和基础语法的内容，希望新的一年里可以完成一次英语的分享。</p><p>尽管之前我的直接技术工作比例也不是那么高，但是今年几乎就只做辅助相关的技术工作了，不再做直接产出相关的技术工作，又被大量非技术的事情占据精力，一度对技术产生了些迷茫。后来在 Twitter 上偶然发现了 <a href="https://twitter.com/bboreham">Bryan Boreham</a> 这个化石级别的工程师，在我出生前他就开始写代码了。现在还在疯狂的给 Prometheus，Grafana 相关项目写代码，而且随便打开一段代码都透露着优雅和功力。看完老爷子的代码和经历，又激发了我最初对技术单纯的热情，甚至自己头脑风暴出了几个新的项目 idea。最近自己也开始做一些调整了，希望新的一年里能 Get 到一些新的技术，参与一些新的项目。</p><h2 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h2><p>今年基本上把从公司离开后的全部时间都花在了孩子上，那种不计成本付出的感觉和看到孩子对你毫无保留的互动的感觉是无与伦比的。但是对时间和精力的挤占也是显而易见的，再叠加工作的调整和疫情的不确定性，整个一年都有种在失控边缘的感觉。今年下来基本上锻炼，读书和学习的时间都被压缩没了，生活的节奏和之前发生了很大的变化。由于各种失控的情况对情绪也产生了大量的消耗，其实陪伴孩子的效果也不是很好，很多时候都很被动。希望新的一年里能调整好生活的节奏，更好的关心身边的人。另外一个悲伤的事情，就是这个熊孩子现在对足球没兴趣了，我已经不知道该怎么拯救中国足球了，要不我还是改行去当足球教练吧。</p><h2 id="疫情"><a href="#疫情" class="headerlink" title="疫情"></a>疫情</h2><p>今年疫情的形式可谓急转直下，我的感觉是到 21 年底整体的疫情控制都还不错，北京除了长假也不需要核酸。甚至政府还很乐观的同时对房地产，互联网和教培开刀。结果 22 年从西安、上海、郑州到年底的北京，一次次指望大力出奇迹后疫情最终还是失控，社会面上一片动荡，政策也是急转弯。客观来说我们家到今年 11 月中旬才第一次碰到封小区，之前只要记着做核酸日常影响也不大，然后这最后一个半月就发生了仿佛好几年的事情，各种世界观被冲击。我一度想去美团当配送去解决运力不足的问题，结果自己就趴窝了，我趴窝期间运力就爬升回来了。</p><p>我觉得之前的动态清零在实践上存在很多问题，但现在的放开从实践来看显然也不是什么理想的做法。我对之前将近八成的无症状数据很愤怒，这种数据和现实如此离谱的分离不知道谁能解释一下。从理性角度来看现在各种政策的空间其实都不大，指望着回到 19 年前的状态是不现实的，我们必然要和这个病毒长期搏斗了。希望科研的人员不要躺平，在疫苗、检测和特效药方面都能有新的科技突破，能把精力集中在对抗病毒上而不是在对抗人上。</p><h2 id="2023"><a href="#2023" class="headerlink" title="2023"></a>2023</h2><p>无论如何 2022 都已经过去了，希望 2023 年我们都可以保持心中的热爱，去做更好的自己。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;作为六年没有更新博客的僵尸博主，在看了 &lt;a href=&quot;https://twitter.com/gaocegege&quot;&gt;Ce Gao&lt;/a&gt; 连续九年的&lt;a href=&quot;http://gaocegege.com/Blog/%E9%9A%8F%E7%AC%94/newyear</summary>
      
    
    
    
    
    <category term="生活" scheme="http://oilbeater.com/tags/%E7%94%9F%E6%B4%BB/"/>
    
    <category term="工作" scheme="http://oilbeater.com/tags/%E5%B7%A5%E4%BD%9C/"/>
    
    <category term="总结" scheme="http://oilbeater.com/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
</feed>
