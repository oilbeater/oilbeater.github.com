<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Oilbeater 的自习室</title>
  <icon>http://oilbeater.com/icon.png</icon>
  
  <link href="http://oilbeater.com/atom.xml" rel="self"/>
  
  <link href="http://oilbeater.com/"/>
  <updated>2024-03-13T11:25:11.952Z</updated>
  <id>http://oilbeater.com/</id>
  
  <author>
    <name>Oilbeater</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>实现 VS Code Remote SSH 下的自动关机</title>
    <link href="http://oilbeater.com/2024/03/13/shutdown-remote-ssh-server/"/>
    <id>http://oilbeater.com/2024/03/13/shutdown-remote-ssh-server/</id>
    <published>2024-03-13T10:09:26.000Z</published>
    <updated>2024-03-13T11:25:11.952Z</updated>
    
    <content type="html"><![CDATA[<p>我现在所有的开发环境都转移到了 GCP 的 Spot Instance 实例，然后用 VS Code Remote SSH 插件进行连接。这种方式的好处是 GCP 可以按秒计费，即使开了很高规格的机器只要及时关机费用也是可控的，缺点是如果中间忘了关机赶上过节那费用就烧开了。再被烧掉了 10 多美元后，痛定思痛决定找个能在 VS Code 空闲时自动关机的方法。过程中为了能够在 Docker 容器内执行 <code>shutdown</code> 命令还搞了一些黑魔法。</p><h1 id="如何判断空闲"><a href="#如何判断空闲" class="headerlink" title="如何判断空闲"></a>如何判断空闲</h1><p>这个功能其实类似 CodeSpace 里的 idle timeout，但是 Remote SSH 这个插件并没有暴露这个功能，所以只能自己实现。其实主要的难点在于如何在 Server 端判断空闲，找了一圈没在 VS Code 里看到有暴露的接口，于是就想能不能跳出 VS Code 看看有什么方法能够简洁的判断空闲发生。</p><p>最直接的想法就是去看 SSH 的连接，因为 Remote SSH 也是通过 SSH 连接上来的，如果当前机器没有存活的 SSH 连接，那么就可以认为是空闲直接关机了。但是问题是 Remote SSH 的连接超时时间会特别长，搜索了一些 Issue 有说是 4 个小时的，我也尝试了直接关闭 VS Code 的客户端，发现 Server 端的 SSH 连接也一直没有消失。如果在 Server 端设置 SSH 超时，Client 那边很快就会重连，连接数量也不会减少。</p><p>既然没法通过直接看 SSH 连接数量来判断，那么就进一步去看能不能判断已有的 SSH 连接是不是已经没有流量了。用 <code>tcpdump</code> 抓包看了一下，即使客户端没有任何交互，还是会有 1s 一次的 TCP 心跳数据包，所以也不能有流量为 0 来判断。不过观察下来心跳数据包的大小都是固定的，都是 44 字节，正好可以根据这个特征来判断，如果一段时间内 SSH 端口没有大于 44 字节的数据包就可以判断空闲了。</p><p>于是第一版本的脚本就出来了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span></span><br><span class="line">  <span class="keyword">if</span> [ -f /root/dump ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">timeout</span> 1m tcpdump -nn -i ens4 tcp and port 22 and greater 50 -w /root/dump</span><br><span class="line"></span><br><span class="line">  line_count=$(<span class="built_in">wc</span> -l &lt; /root/dump)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> [ <span class="variable">$line_count</span> -gt 0 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">    shutdown -h now</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>这样就实现了 1 分钟内 SSH 端口没有非心跳数据包就关机的功能，下一步就是要让这个脚本自动运行了。</p><h1 id="Docker-黑魔法"><a href="#Docker-黑魔法" class="headerlink" title="Docker 黑魔法"></a>Docker 黑魔法</h1><p>在把脚本打包成 Docker 镜像时发现了一个有趣的问题，那就是所有的 Base Image 里都没有 <code>shutdown</code> 命令，<code>shutdown</code> 命令也没法很容易的安装。为了能够执行主机上的 <code>shutdown</code> 命令，就需要在 Docker 容器里切换到主机的命名空间，再去关机。所以需要把之前的脚本稍微修改一下，包装一下 <code>shutdown</code> 命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsenter -a -t 1 shutdown -h now</span><br></pre></td></tr></table></figure><p>这个 <code>nsenter</code> 命令的作用是选定 Pid 为 1 的进程，然后进入这个进程的所有(pid, mount, network) Namespaces，这样当 Docker 运行在共享主机 Pid 模式下我们相当于就进入了主机 1 号进程的所有 Namespaces，看上去就和 SSH 到主机上一样可以执行操作了。这样只要再用下面的命令启动容器，就可以不担心忘记随时关机了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name=close --pid=host --network=host --privileged --restart=always -d close:v0.0.1</span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过监控 SSH 的流量情况可以一定程度上猜测 VS Code 已经空闲了，然后再用 Docker 的一些黑魔法就可以实现自动关机了。不过整个链路的黑魔法都太多了，有没有什么简单的方式呢？</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我现在所有的开发环境都转移到了 GCP 的 Spot Instance 实例，然后用 VS Code Remote SSH 插件进行连接。这种方式的好处是 GCP 可以按秒计费，即使开了很高规格的机器只要及时关机费用也是可控的，缺点是如果中间忘了关机赶上过节那费用就烧开了。</summary>
      
    
    
    
    
    <category term="docker" scheme="http://oilbeater.com/tags/docker/"/>
    
    <category term="vscode" scheme="http://oilbeater.com/tags/vscode/"/>
    
    <category term="tools" scheme="http://oilbeater.com/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>minikube，kind 和 k3d 大比拼</title>
    <link href="http://oilbeater.com/2024/02/22/minikube-vs-kind-vs-k3d/"/>
    <id>http://oilbeater.com/2024/02/22/minikube-vs-kind-vs-k3d/</id>
    <published>2024-02-22T14:42:22.000Z</published>
    <updated>2024-02-23T04:39:34.655Z</updated>
    
    <content type="html"><![CDATA[<p>作为云原生生态的一个开发者，开发中经常碰到的一个需求是要频繁测试应用在 Kubernetes 环境下的运行状态，在 CI 中可能还要快速测试多个不同 Kubernetes 集群的配置，例如单点，高可用，双栈，多集群等等。因此能够低成本的在本地单机环境快速创建管理 Kubernetes 集群就成了一个刚需。本文将介绍几个常见的单机 Kubernetes 管理工具 minikube, kind 和 k3d 各自的特点、使用场景以及可能的坑。</p><blockquote><p>TL;DR<br>如果你只关心快不快，那么 k3d 是最好的选择。如果你关心的是兼容性以及测试尽可能模拟真实场景，那么 minikube 是最稳妥的选择。kind 算是在这两个之间的一个平衡。</p></blockquote><ul><li><a href="#%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF%E6%AF%94%E8%BE%83">技术路线比较</a><ul><li><a href="#minikube">minikube</a></li><li><a href="#kind">kind</a></li><li><a href="#k3d">k3d</a></li></ul></li><li><a href="#%E5%90%AF%E5%8A%A8%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83">启动性能比较</a><ul><li><a href="#%E6%B5%8B%E8%AF%95%E6%96%B9%E6%B3%95">测试方法</a></li><li><a href="#%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C">测试结果</a></li></ul></li><li><a href="#%E6%80%BB%E7%BB%93">总结</a></li></ul><h1 id="技术路线比较"><a href="#技术路线比较" class="headerlink" title="技术路线比较"></a>技术路线比较</h1><p>这三者大体功能是类似的，都能够完成单机管理 Kubernetes 的任务，但是由于一些历史原因和技术选项导致了一些细节和使用场景的差异。</p><h2 id="minikube"><a href="#minikube" class="headerlink" title="minikube"></a>minikube</h2><p><a href="https://minikube.sigs.k8s.io/docs/">minikube</a> 是 Kubernetes 社区最早的一款快速在本地创建 Kubernetes 的软件，也是很多老人第一次上手 Kubernetes 的工具。早期版本是通过在本机创建 VM 来模拟多节点集群，这个方案的好处是能够最大程度还原真实场景，一些操作系统级别的变化，例如不同 OS，不同内核模块都可以覆盖到。缺点就是资源消耗太大，而且在一些虚拟化环境如果没有嵌套虚拟化的支持是没办法运行的，并且启动的速度也比较慢。不过社区最近也推出了 Docker 的 Driver 这些问题都得到了比较好的解决，不过对应代价就是一些虚拟机级别的模拟就不好做了。此外 minikube 还提供了不少的 addon，比如 dashboard，nginx-ingress 等常见的社区组件都能快速的安装使用。</p><h2 id="kind"><a href="#kind" class="headerlink" title="kind"></a>kind</h2><p><a href="https://kind.sigs.k8s.io/">kind</a> 是近几年流行起来的一个本地部署 Kubernetes 的工具，他的主要特点就是用 Docker 容器模拟节点，并且基本只专注在 Kubernetes 标准部署这一个事情上，其他社区组件都需要额外自己去安装。目前 Kubernetes 本身的 CI 也是通过 kind 来跑的。优点就是启动速度很快，熟悉 Docker 的人用起来也很顺手。缺点是用了容器模拟缺乏操作系统级别的隔离，而且和宿主机共享内核，一些操作系统相关的测试就不好测试了。我之前在测一个内核模块的时候就因为宿主机加了一些 netfilter 功能，结果 kind 里的 Kubernetes 集群挂了。</p><h2 id="k3d"><a href="#k3d" class="headerlink" title="k3d"></a>k3d</h2><p><a href="https://k3d.io/stable/">k3d</a> 是一个超轻量的本地部署 Kubernetes 工具，他的大体思路和 kind 类似，都是通过 Docker 来模拟节点，主要区别在于部署的不是个标准 Kubernetes 而是一个轻量级的 <a href="https://k3s.io/">k3s</a>，所以他的大部分优缺点也来自于下面这个 k3s。优点就是安装极致的快，你先别管对不对，你就问快不快吧。缺点主要来自于为了速度做出的一些牺牲，比如镜像用的是个超精简的操作系统，连 glibc 都没有，因此一些要在操作系统层面的操作都会无比困难。还有就是他的安装方式和常见的 kubeadm 也不一样，Kubernetes 组件都不是容器启动的，如果依赖标准部署的一些特性可能都会比较困难。</p><h1 id="启动性能比较"><a href="#启动性能比较" class="headerlink" title="启动性能比较"></a>启动性能比较</h1><p>minikube 社区有一些<a href="https://minikube.sigs.k8s.io/docs/benchmarks/timetok8s/v1.32.0/">性能测试报告</a>，正好对比的就是本文关注的三款软件的启动速度，不过我更关注的是其他的一些方面，比如镜像大小，内存占用以及最小化安装的启动时间，所以还是再做了一组测试。</p><h2 id="测试方法"><a href="#测试方法" class="headerlink" title="测试方法"></a>测试方法</h2><p>由于三款软件都是一行命令就可以启动，测试还是比较方便的，主要注意以下几个点：</p><ol><li>minikube 采用 Docker Driver，因为真要测启动速度还用虚拟机的 Driver 就没什么意义了。</li><li>所有测试都是镜像已经下载到本地的结果，不会涉及网络下载时间。</li><li>测试的每个软件都是当前的最新版，但是他们支持的 Kubernetes 版本不一致，不是很严谨，但是定性分析应该够了。</li><li>都只启动最基本的组件，不安装其他插件，但是基础 CNI 和 CoreDNS 以及 CSI 都是有的，保证应用的基本运行。</li><li>使用 <code>docker image</code> 命令查看镜像大小，使用 <code>docker stat</code> 查看内存用量。</li></ol><p>测试的命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#minikube</span></span><br><span class="line">time minikube start --driver=docker --force</span><br><span class="line"></span><br><span class="line"><span class="comment">#kind</span></span><br><span class="line">time kind create cluster</span><br><span class="line"></span><br><span class="line"><span class="comment">#k3d</span></span><br><span class="line">time k3d cluster create mycluster --k3s-arg <span class="string">&#x27;--disable=traefik,metrics-server@server:*&#x27;</span> --no-lb</span><br></pre></td></tr></table></figure><h2 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h2><table><thead><tr><th>名称</th><th>软件版本</th><th>Kubernetes 版本</th><th>镜像大小</th><th>启动时间</th><th>内存消耗</th></tr></thead><tbody><tr><td>minikube</td><td>v1.32.0</td><td>v1.28.3</td><td>1.2GB</td><td>29s</td><td>536MiB</td></tr><tr><td>kind</td><td>v0.22</td><td>v1.29.2</td><td>956MB</td><td>20s</td><td>463MiB</td></tr><tr><td>k3d</td><td>v5.6.0</td><td>v1.27.4</td><td>263MB</td><td>7s</td><td>423MiB</td></tr></tbody></table><p>可见单从启动性能这个指标，k3d 在镜像大小，启动时间和内存消耗几个方面都有比较大的优势，对于用 Github 免费 Action 跑 CI 的穷人还是很有吸引力的。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>如果快和更少的资源占用是最重要的目标，那么 k3d 相当合适，如果要测试需要操作系统级别隔离的功能，那么 minikube 的虚拟机 Driver 是唯一的选择，其他场景下 kind 会在兼容和性能之间比较平衡。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;作为云原生生态的一个开发者，开发中经常碰到的一个需求是要频繁测试应用在 Kubernetes 环境下的运行状态，在 CI 中可能还要快速测试多个不同 Kubernetes 集群的配置，例如单点，高可用，双栈，多集群等等。因此能够低成本的在本地单机环境快速创建管理 Kuber</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/tags/kubernetes/"/>
    
    <category term="ci" scheme="http://oilbeater.com/tags/ci/"/>
    
  </entry>
  
  <entry>
    <title>Golang 中预分配 slice 内存对性能的影响（续）</title>
    <link href="http://oilbeater.com/2024/01/09/alloc-slice-for-golang-2-md/"/>
    <id>http://oilbeater.com/2024/01/09/alloc-slice-for-golang-2-md/</id>
    <published>2024-01-09T08:16:22.000Z</published>
    <updated>2024-03-06T08:11:02.825Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#%E5%9F%BA%E7%A1%80%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95">基础性能测试</a></li><li><a href="#%E6%95%B4%E4%B8%AA-slice-append">整个 Slice Append</a></li><li><a href="#%E5%A4%8D%E7%94%A8-slice">复用 Slice</a></li><li><a href="#syncpool">sync.Pool</a></li><li><a href="#bytebufferpool">bytebufferpool</a></li><li><a href="#%E6%80%BB%E7%BB%93">总结</a></li></ul><p>之前写了一篇 <a href="https://oilbeater.com/2023/07/19/pre-alloc-slice-for-golang/">Golang 中预分配 slice 内存对性能的影响</a>，探讨了一下在 Slice 中预分配内存对性能的影响，之前考虑的场景比较简单，最近又做了一些其他测试，补充一下进一步的信息。包括整个 Slice append，sync.Pool 对性能的影响。</p><h1 id="基础性能测试"><a href="#基础性能测试" class="headerlink" title="基础性能测试"></a>基础性能测试</h1><p>最初的 BenchMark 代码，只考虑了 Slice 是否初始化分配空间的情况，具体的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> prealloc_test</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;sync&quot;</span></span><br><span class="line">    <span class="string">&quot;testing&quot;</span></span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">var</span> length = <span class="number">1024</span></span><br><span class="line"><span class="keyword">var</span> testtext = <span class="built_in">make</span>([]<span class="type">byte</span>, length, length)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocateByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">        <span class="keyword">var</span> init []<span class="type">byte</span></span><br><span class="line">        <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">            init = <span class="built_in">append</span>(init, testtext[j])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocateByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">        <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">            init = <span class="built_in">append</span>(init, testtext[j])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        569978              2151 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          804807              1304 ns/op            1024 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>可见没有预分配的情况下多了 8 次内存分配，两个相对比可以粗略的认为 40% 的时间消耗在了这额外的 8 次内存分配。</p><p>这两个测试用例使用的是循环里逐个 append 元素，但是 Slice 还支持整个 Slice 进行 append 在这种情况下的性能差距是没有体现出来的。而且在这两个测试用例里我们其实无法知道内存分配所占的时间消耗占整个时间的占比。</p><h1 id="整个-Slice-Append"><a href="#整个-Slice-Append" class="headerlink" title="整个 Slice Append"></a>整个 Slice Append</h1><p>因此加入两个整个 Slice Append 的测试用例，观察预分配内存对性能还有没有这么大的影响。新增的用例代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">        <span class="keyword">var</span> init []<span class="type">byte</span></span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        569978              2151 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          804807              1304 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                3829890               311.5 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3968048               306.7 ns/op          1024 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>可见两个用例都只用了一次内存分配，消耗时间几乎相同，而且大幅低于逐个元素进行 append 的情况。一方面整个 Slice append，在 Slice 扩容时就知道了最终的大小没必要进行动态内存分配，降低了内存分配的开销。另一方面整个 Slice append 在实现上会进行整段复制，降低了循环的开销，性能会提升很多。</p><p>但在这里每次还是会有一次内存分配，我们依然无法确定这一次内存分配所占的整体时间比例。</p><h1 id="复用-Slice"><a href="#复用-Slice" class="headerlink" title="复用 Slice"></a>复用 Slice</h1><p>为了计算一次内存分配的消耗，我们设计一个新的测试用例，将 Slice 的创建放到循环外，循环内每次最后将 Slice 的 length 设为 0，给下次进行复用。这样在大量的测试下只会进行一次内存分配，平均下来就可以忽略不计了。具体的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate2</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">        init = init[:<span class="number">0</span>]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        514904              2171 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          761772              1333 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                4041459               320.9 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3854649               320.1 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate2-12                63147178                18.63 ns/op            0 B/op          0 allocs/op</span><br></pre></td></tr></table></figure><p>可见这次测试统计上没有内存分配了，整体消耗时间也降为了之前的 5%。因此大致可以计算出在之前的测试用例里每一次内存分配会消耗 95% 的时间，这个占比还是很惊人的。因此对于性能敏感的场景还是需要尽可能的复用对象，避免反复的对象创建的内存开销。</p><h1 id="sync-Pool"><a href="#sync-Pool" class="headerlink" title="sync.Pool"></a>sync.Pool</h1><p>简单的场景下可以像上个测试用例里一样手动的清空 Slice 在循环内进行复用，但是真实场景里对象的创建通常会发生在代码的各个地方，就需要统一的进行管理和复用了，Golang 里的 <code>sync.Pool</code> 就是做这个事情的，而且使用起来也很简单。但是内部实现还是比较复杂的，为了性能进行了大量无锁化的设计，具体实现可以参考<a href="https://unskilled.blog/posts/lets-dive-a-tour-of-sync.pool-internals/">Let’s dive: a tour of sync.Pool internals</a>。</p><p>使用 <code>sync.Pool</code> 重新设计的测试用例如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> sPool = &amp;sync.Pool&#123; </span><br><span class="line">        New: <span class="function"><span class="keyword">func</span><span class="params">()</span></span> any &#123;</span><br><span class="line">                b := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">                <span class="keyword">return</span> &amp;b</span><br><span class="line">        &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPoolByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">        b.ResetTimer()</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">                <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">                b := sPool.Get().(*[]<span class="type">byte</span>)</span><br><span class="line">                buf := *b</span><br><span class="line">                <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">                        buf = <span class="built_in">append</span>(buf, testtext[j])</span><br><span class="line">                &#125;</span><br><span class="line">                buf = buf[:<span class="number">0</span>]</span><br><span class="line">                sPool.Put(b)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPool</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">        b.ResetTimer()</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">                <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">                bufPtr := sPool.Get().(*[]<span class="type">byte</span>)</span><br><span class="line">                buf := * bufPtr</span><br><span class="line">                buf = <span class="built_in">append</span>(buf, testtext...)</span><br><span class="line">                buf = buf[:<span class="number">0</span>]</span><br><span class="line">                sPool.Put(bufPtr)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中 <code>New</code> 用来给 <code>sync.Pool</code> 一个在没有可用对象时创建对象的构造函数，使用的时候使用 <code>Get</code> 方法从 Pool 里获取一个对象，用完了再用 <code>Put</code> 方法把对象还给 <code>sync.Pool</code>。这里主要注意一下对象的生命周期，以及放回到 <code>sync.Pool</code> 时需要清空对象，避免脏数据。测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        469431              2313 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          802392              1339 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPoolByElement-12                1212828               961.5 ns/op             0 B/op          0 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                3249004               370.2 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3268851               368.2 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate2-12                62596077                18.63 ns/op            0 B/op          0 allocs/op</span><br><span class="line">BenchmarkPool-12                        32707296                35.59 ns/op            0 B/op          0 allocs/op</span><br></pre></td></tr></table></figure><p>可见使用 <code>sync.Pool</code> 也可以避免内存分配，由于 <code>sync.Pool</code> 还有一些额外的处理性能消耗会比手动复用 Slice 稍高一些，不过考虑到使用的便利性以及相比不使用还是有明显的性能提升，还是一个不错的方案。</p><p>但是直接使用 <code>sync.Pool</code> 也有下面两个问题：</p><ol><li>对于 Slice 的情况 <code>New</code> 分配的初始内存是固定的，运行时使用空间超出的话，可能还会有大量动态的内存分配调整。</li><li>另一个极端是 Slice 被动态扩容很大后放回到 <code>sync.Pool</code> 中，可能会造成内存的泄漏和浪费。</li></ol><h1 id="bytebufferpool"><a href="#bytebufferpool" class="headerlink" title="bytebufferpool"></a>bytebufferpool</h1><p>为了达到实际运行时更优的性能，<a href="https://github.com/valyala/bytebufferpool">bytebufferpool</a> 这个项目在 <code>sync.Pool</code> 的基础上运用了一些简单的统计规律，尽可能的减少了上面提到的两个问题在运行时的影响。（该项目的作者是俄罗斯人，手下还有 fasthttp, quicktemplate 和 VictoriaMetrics 几个项目，个顶个都是性能优化的优秀案例，战斗民族经常会搞这种性能推极限的项目。</p><p>代码里主要的结构如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// bytebufferpool/pool.go</span></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">  minBitSize = <span class="number">6</span> <span class="comment">// 2**6=64 is a CPU cache line size</span></span><br><span class="line">  steps      = <span class="number">20</span></span><br><span class="line"> </span><br><span class="line">  minSize = <span class="number">1</span> &lt;&lt; minBitSize</span><br><span class="line">  maxSize = <span class="number">1</span> &lt;&lt; (minBitSize + steps - <span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">  calibrateCallsThreshold = <span class="number">42000</span></span><br><span class="line">  maxPercentile           = <span class="number">0.95</span></span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">type</span> Pool <span class="keyword">struct</span> &#123;</span><br><span class="line">  calls       [steps]<span class="type">uint64</span></span><br><span class="line">  calibrating <span class="type">uint64</span></span><br><span class="line"> </span><br><span class="line">  defaultSize <span class="type">uint64</span></span><br><span class="line">  maxSize     <span class="type">uint64</span></span><br><span class="line"> </span><br><span class="line">  pool sync.Pool</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中 <code>defaultSize</code> 的作用是 <code>New</code> 的时候给 Slice 分配的大小，<code>maxSize</code> 的作用是超过这个大小的 Slice <code>Put</code> 时会拒绝。核心的算法其实就是在运行时根据统计到的 Slice 使用大小信息动态的去调整 <code>defaultSize</code> 和 <code>maxSize</code> ，避免额外的内存分配同时还要避免内存泄漏。</p><p>这个动态统计的过程也比较简单，就是将 <code>Put</code> 到 Pool 里的 Slice 大小划分了 20 个区间范围进行统计，当 <code>Put</code> 次数达到 <code>calibrating</code> 后就进行一次排序，将这个时间段内使用最为频繁的区间大小作为 <code>defaultSize</code> 这样在统计上就可以避免不少额外的内存分配。然后按大小排序，将 95% 分位大小设置为  <code>maxSize</code>，这样就避免了在统计上长尾大的对象进入 Pool。就靠着这样动态调整这两个值，在统计上可以在运行时获得更优的性能。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>Slice 初始化尽可能指定 capacity</li><li>避免在循环中初始化 Slice</li><li>性能敏感路径考虑使用 <code>sync.Pool</code></li><li>内存分配的性能开销可能远大于业务逻辑</li><li>bytebuffer 的复用可以考虑看下 bytebufferpool</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%9F%BA%E7%A1%80%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95&quot;&gt;基础性能测试&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%95%B4%E4%B8%AA-slice-append&quot;&gt;整个</summary>
      
    
    
    
    
    <category term="性能" scheme="http://oilbeater.com/tags/%E6%80%A7%E8%83%BD/"/>
    
    <category term="golang" scheme="http://oilbeater.com/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>不需要封装的 Overlay 容器网络</title>
    <link href="http://oilbeater.com/2023/07/30/xmasq-overlay-without-encap/"/>
    <id>http://oilbeater.com/2023/07/30/xmasq-overlay-without-encap/</id>
    <published>2023-07-30T05:24:33.000Z</published>
    <updated>2024-03-07T06:22:59.792Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8-overlay-%E7%BD%91%E7%BB%9C">为什么要用 Overlay 网络</a></li><li><a href="#%E5%AE%B9%E5%99%A8-overlay-%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BC%80%E9%94%80%E6%9D%A5%E6%BA%90">容器 Overlay 网络的开销来源</a></li><li><a href="#%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3-overlay-%E5%B0%81%E8%A3%85%E7%9A%84%E5%BC%80%E9%94%80">如何解决 Overlay 封装的开销</a></li><li><a href="#%E6%88%91%E7%9A%84%E6%83%B3%E6%B3%95">我的想法</a></li></ul><p>上周在看 <a href="https://antrea.io/">Antrea</a> 的<a href="https://github.com/antrea-io/antrea/wiki/Community-Meetings#july-17-2023">会议纪要</a>时，发现了一篇上海交大和 VMware 合作的论文 —— <a href="https://arxiv.org/pdf/2305.05455.pdf">XMasq: Low-Overhead Container Overlay Network Based on eBPF</a>。论文里介绍不少通过 eBPF 做容器网络性能优化的方法，不过最令人印象深刻的是他们最终无需给容器数据包封装额外的隧道头就可以实现 Overlay 网络，这里就简单介绍下作者们是如何做到这一点的。</p><h2 id="为什么要用-Overlay-网络"><a href="#为什么要用-Overlay-网络" class="headerlink" title="为什么要用 Overlay 网络"></a>为什么要用 Overlay 网络</h2><p>Overlay 网络相比 Underlay 网络可以完全解耦应用层和底层物理网络,确保两者的灵活性。应用层不需要关心底层物理网络的路由规则等细节,物理网络也不需要针对容器 IP 进行专门路由配置。因此 Overlay 网络作为一种不调底层网络实现的容器组网技术，快速的在容器领域铺开了，主流的开源网络插件处于安装兼容的考虑，基本上都会将 Overlay 作为默认的安装选项。</p><h2 id="容器-Overlay-网络的开销来源"><a href="#容器-Overlay-网络的开销来源" class="headerlink" title="容器 Overlay 网络的开销来源"></a>容器 Overlay 网络的开销来源</h2><p>但是在灵活性的便利下，带来的是性能方面的开销。根据论文里的测量，主流的 Overlay 网络插件，相比于 HOST 网络，吞吐量会下降 25%，CPU 消耗会增加 34%~44%，延迟会上升 45%~58%。</p><p>当然，这里面的性能开销并不完全是 Overlay 的封装带来的，由于容器网络本身的流量路径和 HOST 网络就有比较大的区别，Overlay 封装其实只是其中的一环。如下图所示：<br><img src="/../images/container-network-overhead.png" alt="Alt text"></p><p>其中 1~4，7~10 都是容器网络相比宿主机网络额外带来的链路，Overlay 的封装主要在 4 和 7，分别占到了 egress 和 ingress 开销的 24% 和 29%。</p><p>其他几个链路上的优化在 Cilium 和其他开源项目上其实已经见到过一些了，主要思路是通过 eBPF bypaas 掉部分链路，将数据包能够从容器网络栈直通到物理网卡，下面主要介绍下这篇论文是如何解决 Overlay 封装的问题。</p><h2 id="如何解决-Overlay-封装的开销"><a href="#如何解决-Overlay-封装的开销" class="headerlink" title="如何解决 Overlay 封装的开销"></a>如何解决 Overlay 封装的开销</h2><p>常见的 Overlay 隧道如 VXlan 和 Geneve 都是通过给容器网络的数据包封装一个 UDP 的 Header 实现的 Overlay。外层的 UDP 头部记录宿主机实际的 IP 和 Mac，内层数据包的 Header 里记录容器网络的 IP 和 Mac，外层作为真实网络的通信标识，内层作为容器网络的通信标识。那么有没有可能把两层的信息通过一层全带走，从而实现不需要额外的 UDP Header 呢？这就是这篇论文作者的一个很有意思的工作。</p><p>外层的 IP 和 Mac 作为真实通信的标识是不可能省略的，但是 IP 的 Header 中有一部分字段比如 DSCP 和 ID 是很少被使用的。如果能将这两个字段利用起来，编码内层的信息，那么我们就可以不用内层的容器 IP 和 Mac 这一层了。这里可以想象一下用 iptables 来做 nat 其实是将源 IP 和端口映射成目标 IP 和端口，也可以理解为一种编码的映射关系。</p><p>作者在内核中通过 eBPF 的 Map 缓存了容器网络的 Mac 和 IP 信息，并生成了一个 key 来对应每一组 IP 和 Mac。这样，当容器再进行跨主机网络通信时，可以直接将目标地址修改为对应宿主机的地址，同时将这个 key 写入 IP 中的指定的保留字段。等数据包到达对端后，对端就可以通过这个 key 查询本地的缓存将数据包的地址再还原为容器的地址。这样并不需要额外的封装，通过 IP 和 Mac 头的直接替换，就完成了跨主机的 Overlay 网络。</p><p>由于论文中还应用了很多其他 bypass 的优化，没有单独衡量 Overlay 改造带来的性能提升，从整体效果来看性能优化后的效果较普通 Overlay 网络有很大的提升基本接近 HOST 网络。</p><h2 id="我的想法"><a href="#我的想法" class="headerlink" title="我的想法"></a>我的想法</h2><p>按照我之前的经验使用 UDP 进行 Overlay 封装还有一个很大的性能问题，那就是如果内部数据包是 TCP 的时候，TCP 其实有很多网卡的 Offload 优化，能大幅提升吞吐量，但如果是封装后这种 TCP in UDP 的形式，很多网卡 Offload 的能力就失效了。我们之前在一些虚拟化的平台，使用 UDP 的封装吞吐量只有使用 STT 这种 TCP 进行封装的十分之一。不过这个差别也和网卡的能力，操作系统内核相关，在作者测试的网卡上并没有表现出来明显的差异，有可能是物理网卡支持这种 TCP in UDP 的 Offload。</p><p>这篇论文的目的是希望把这套性能优化做成一个 CNI 无关的插件，所有的容器网络都可以使用，但是论文里大量的 eBPF bypaas 其实也会跳过 CNI Datapath 的处理，会导致原有 CNI 功能的缺失。所以做一个通用插件的意义可能并不大。直接做一个新的 CNI ，不要求其他功能，只追求 Overlay 的通用性和极致的性能其实更合适一些，没准能拿下不少的细分领域市场。反而是 Overlay 封装这块如果能单独拿出来，做成和 VXlan，Geneve 并列的一种封装格式，倒是能真正做到 CNI 无关的一个通用模块。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8-overlay-%E7%BD%91%E7%BB%9C&quot;&gt;为什么要用 Overlay 网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%AE%</summary>
      
    
    
    
    
    <category term="performance" scheme="http://oilbeater.com/tags/performance/"/>
    
    <category term="network" scheme="http://oilbeater.com/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>如何在本地用 CPU 跑大模型</title>
    <link href="http://oilbeater.com/2023/07/27/local-cpu-llm/"/>
    <id>http://oilbeater.com/2023/07/27/local-cpu-llm/</id>
    <published>2023-07-27T10:51:07.000Z</published>
    <updated>2023-07-27T11:34:18.321Z</updated>
    
    <content type="html"><![CDATA[<p>之前觉得大模型在本地运行是个不可能的事情，我甚至没有一块 GPU。但是尝试了一下 llama.cpp 发现几分钟就把 Meta 最新开源的 Llama 2 在 CPU 上运行起来了，速度和质量都还可以接受，很多想法突然就变得可行了。记录一下搭建的过程，希望对感兴趣的人有帮助。</p><h1 id="需要的配置"><a href="#需要的配置" class="headerlink" title="需要的配置"></a>需要的配置</h1><p>尽管不需要 GPU 和加速卡，但是大模型对磁盘和内存还是有需求的，我运行的是 Llama 2 7B 的模型，经过了 GGML 处理，选择了 int8 的精度。这个模型跑起推理来大概需要 7G 的磁盘空间，运行起来也需要 7G 左右的内存，长期运行的话内存有 10G 会比较保险。CPU 的话 1 核也能跑，但给到 6 核会比较流畅。</p><h1 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h1><p>下载模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin</span><br></pre></td></tr></table></figure><p>这是一个处理过的模型，减小了体积和精度，专门为端侧运行做了优化，也不需要申请 Meta 的授权直接就能下载。</p><p>下载并安装 <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ggerganov/llama.cpp.git</span><br><span class="line"><span class="built_in">cd</span> llama.cpp</span><br><span class="line">make</span><br></pre></td></tr></table></figure><p>这是一个高性能的 Llama 推理工具集，可以方便的做 chat，api 等等服务。</p><p>移动模型文件到对应目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> ../llama-2-7b-chat.ggmlv3.q8_0.bin models/</span><br></pre></td></tr></table></figure><p>好了现在你就可以在本地运行一个 chat 了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./main -m ./models/llama-2-7b-chat.ggmlv3.q8_0.bin -c 512 -b </span><br><span class="line">1024 -n 256 --keep 48 \</span><br><span class="line">    --repeat_penalty 1.0 --color -i -t 4 \</span><br><span class="line">    -r <span class="string">&quot;User:&quot;</span> -f prompts/chat-with-bob.txt</span><br></pre></td></tr></table></figure><h1 id="更多配置"><a href="#更多配置" class="headerlink" title="更多配置"></a>更多配置</h1><p>在 examples 目录下还可以看到 llama.cpp 的其他用法，比如提供 API 服务，提供 embedding 和 fine-tune，甚至还有一个兼容 OpenAI API 的转换器。</p><p>如果你的机器有 GPU 或者用的是 M 系列芯片的 Mac 那么可以通过 make 参数提高推理的性能。</p><p>如果你的机器再厉害一些可以考虑去 huggingface 上去下载更大参数量的模型。</p><h1 id="为什么要本地运行大模型"><a href="#为什么要本地运行大模型" class="headerlink" title="为什么要本地运行大模型"></a>为什么要本地运行大模型</h1><p>其实我手头 OpenAI 的 GPT3&#x2F;GPT3，Google Bard，Claude 2 的 API 使用权都有，那为啥还要费心思在本地用 CPU 跑质量并不如他们的大模型呢？</p><p>第一个原因是穷，和我想做的事情相关。我想用大模型做代码分析和信息摘要，一个项目可能有上万个 commit 跑一遍就破产了。</p><p>第二个原因是穷，买不起 GPU 和 M 系列的 Mac，只能找穷人的方法。</p><p>第三个原因是我觉得未来端侧的大模型会更实用，也更能贴近个人的场景进行垂直方向的定制化，想借着这个机会看看这个领域到啥地步了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;之前觉得大模型在本地运行是个不可能的事情，我甚至没有一块 GPU。但是尝试了一下 llama.cpp 发现几分钟就把 Meta 最新开源的 Llama 2 在 CPU 上运行起来了，速度和质量都还可以接受，很多想法突然就变得可行了。记录一下搭建的过程，希望对感兴趣的人有帮助</summary>
      
    
    
    
    
    <category term="llama" scheme="http://oilbeater.com/tags/llama/"/>
    
    <category term="AI" scheme="http://oilbeater.com/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>新的网络性能优化技术 —— BIG TCP</title>
    <link href="http://oilbeater.com/2023/07/24/big-tcp/"/>
    <id>http://oilbeater.com/2023/07/24/big-tcp/</id>
    <published>2023-07-24T15:21:15.000Z</published>
    <updated>2023-07-24T16:14:12.136Z</updated>
    
    <content type="html"><![CDATA[<p>在翻 Cilium Release Blog 时发现了一个叫 Big TCP 的内核技术，这个技术今年 2 月左右刚被合并到内核。看介绍是专门为 100Gbit 以上的高速网络设计的，在将吞吐量提升 50% 的时候还能大幅降低延迟，于是就去了解了一下这个技术。</p><h1 id="高速网络的性能的瓶颈和误区"><a href="#高速网络的性能的瓶颈和误区" class="headerlink" title="高速网络的性能的瓶颈和误区"></a>高速网络的性能的瓶颈和误区</h1><p>提到网络性能，很多人会很自然的联想到网卡的性能，但是做过网络性能优化的人会知道瓶颈更多其实是在 CPU 上。一般使用 iperf3、qperf 这类压测软件进行测试，尤其在小包情况下基本是无法打满网卡带宽的，这时候 CPU 会先于网卡到达瓶颈。</p><p>因此做网络性能的优化通常做的都是 CPUU 相关的优化，不管是 DPDK，Offload，XDP 加速的原理要么是绕过内核栈，要么是卸载部分工作给网卡，本质上都是为了节省 CPU 资源，来处理更多的数据包。</p><p>以 100G 的网卡为例，以太网的 MTU 是 1500 字节，那么在不做任何优化的情况下 CPU 想要跑满网卡，每秒要处理将近 800 万个数据包，如果每个数据包都要走完整的网络栈，现代 CPU 单核还远远处理不了这个量级的数据包。</p><p>为了能让单核跑出尽可能好的网络性能，就需要内核和网卡驱动共同协作，将一部分工作 Offload 给网卡，这也就是 GRO(Generic Receive Offload) 和 TSO(TCP Segment Offload) 相关的技术。</p><h1 id="GRO-和-TSO"><a href="#GRO-和-TSO" class="headerlink" title="GRO 和 TSO"></a>GRO 和 TSO</h1><p>CPU 处理不了那么多数据的一个原因是需要给每个数据包封装协议头，计算校验和等等会浪费大量的资源，而在 100G 这种带宽下 1500 的 MTU 又实在太小了，不得不拆分出这么多小的数据包。如果将数据包的拆分和组装交给网卡来做，那么内核需要处理的数据包的规模就降下来了，这就是 GRO 和 TSO 提升性能的基本原理。</p><p><img src="/../images/tso-gro.png" alt="Alt text"></p><p>如上图所示，在发送数据包的时候内核可以按照 64K 来处理数据包，网卡通过 TSO 拆分成 1.5K 的小包进行传输，接收端网卡通过 GRO 将小包聚合成 64K 的大包在发送给内核处理。这样一来，内核需要处理包的数量就降为了原来的四十分之一。因此理论上开了这两个 Offload 网络的吞吐量在大包的情况下会有数十倍的提升，在我们之前的测试中这个性能大概会差十倍左右。</p><p>而且在主流内核和网卡驱动里只支持 TCP 的 Offload，所以经常会看到的一个不太符合直觉的现象就是 iperf3 的 TCP 性能要远远好于 UDP 的性能，差距大概也在十倍左右。</p><h1 id="Big-TCP-又做了什么？"><a href="#Big-TCP-又做了什么？" class="headerlink" title="Big TCP 又做了什么？"></a>Big TCP 又做了什么？</h1><p>既然已经有了这个 Offload 那么 Big TCP 又做了什么呢？Big TCP 要做的事情就是让内核里处理的这个数据包变的更大，这样整体要处理的数据包就会进一步下降来提升性能。之前内核处理的数据包大小为 64K 的主要限制在于 IP 包的头部有个长度字段，这个字段长度为 16bit，因此理论上一个 IP 包最大长度就是 64K。</p><p>怎样才能突破这个长度限制呢，内核的作者在这里用了一些很 hack 的方法，在 IPv6 的数据包中有一个 hop-by-hop 的 32 位字段可以存储一些附加信息，那么内核里就可以把 IP 包的长度设置为 0，然后从 hop-by-hop 字段中获取真实的数据包长度，这样一个数据包最大就可以到 4GB 的容量。但是处于稳妥的考虑，目前最大只能设置为 512K，即使这样要处理的数据包也变为了原来的八分之一，相比没有卸载的情况就是将近三百分之一。</p><p>根据开发者的<a href="https://lwn.net/Articles/883713/">测试</a>，吞吐量有将近 50% 的提升，延迟也有将近 30% 的下降，效果可以说相当显著了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;Standard&#x27;</span> performance with current (74KB) limits.</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..10&#125;; <span class="keyword">do</span> ./netperf -t TCP_RR -H iroa23  -- -r80000,80000 -O MIN_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT|<span class="built_in">tail</span> -1; <span class="keyword">done</span></span><br><span class="line">77           138          183          8542.19    </span><br><span class="line">79           143          178          8215.28    </span><br><span class="line">70           117          164          9543.39    </span><br><span class="line">80           144          176          8183.71    </span><br><span class="line">78           126          155          9108.47    </span><br><span class="line">80           146          184          8115.19    </span><br><span class="line">71           113          165          9510.96    </span><br><span class="line">74           113          164          9518.74    </span><br><span class="line">79           137          178          8575.04    </span><br><span class="line">73           111          171          9561.73    </span><br><span class="line"></span><br><span class="line">Now <span class="built_in">enable</span> BIG TCP on both hosts.</span><br><span class="line"></span><br><span class="line">ip <span class="built_in">link</span> <span class="built_in">set</span> dev eth0 gro_ipv6_max_size 185000 gso_ipv6_max_size 185000</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..10&#125;; <span class="keyword">do</span> ./netperf -t TCP_RR -H iroa23  -- -r80000,80000 -O MIN_LATENCY,P90_LATENCY,P99_LATENCY,THROUGHPUT|<span class="built_in">tail</span> -1; <span class="keyword">done</span></span><br><span class="line">57           83           117          13871.38   </span><br><span class="line">64           118          155          11432.94   </span><br><span class="line">65           116          148          11507.62   </span><br><span class="line">60           105          136          12645.15   </span><br><span class="line">60           103          135          12760.34   </span><br><span class="line">60           102          134          12832.64   </span><br><span class="line">62           109          132          10877.68   </span><br><span class="line">58           82           115          14052.93   </span><br><span class="line">57           83           124          14212.58   </span><br><span class="line">57           82           119          14196.01 </span><br></pre></td></tr></table></figure><p>而 <a href="https://lwn.net/Articles/920017/">Big TCP 的 IPv4 支持</a>要晚一些，主要原因是 IPv4 内并没有类似 IPv6 中 hop-by-hop 的可选信息字段，因此也就不能在 IP 头中保存真实长度信息。不过作者另辟蹊径，直接从内核的 skb-&gt;len 中计算数据包的真实长度信息，反正这个数据包只要在送出去之后长度是正确的就可以，在机器内部其实可以完全依赖 skb 保存状态信息，这样实现了超过 64K 的 TCP 数据包。理论上 IPv4 的做法会更通用，不过 IPv6 已经先实现了，所以就存在了两种不同实现 Big TCP 的方案。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这个技术对内核和硬件驱动都有一定的要求，内核需要 6.3 才正式支持，而网卡驱动的支持可能还需要联系硬件厂商。但 Big TCP 还是一个值得令人期待的技术，能够在不需要应用程序调整的情况下显著提升网络的性能，在特定场景下的收益还是很大的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在翻 Cilium Release Blog 时发现了一个叫 Big TCP 的内核技术，这个技术今年 2 月左右刚被合并到内核。看介绍是专门为 100Gbit 以上的高速网络设计的，在将吞吐量提升 50% 的时候还能大幅降低延迟，于是就去了解了一下这个技术。&lt;/p&gt;
&lt;h</summary>
      
    
    
    
    
    <category term="performance" scheme="http://oilbeater.com/tags/performance/"/>
    
    <category term="network" scheme="http://oilbeater.com/tags/network/"/>
    
    <category term="kernel" scheme="http://oilbeater.com/tags/kernel/"/>
    
  </entry>
  
  <entry>
    <title>Golang 中预分配 slice 内存对性能的影响</title>
    <link href="http://oilbeater.com/2023/07/19/pre-alloc-slice-for-golang/"/>
    <id>http://oilbeater.com/2023/07/19/pre-alloc-slice-for-golang/</id>
    <published>2023-07-19T10:48:09.000Z</published>
    <updated>2023-07-19T15:36:30.930Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#slice-%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80">Slice 内存分配理论基础</a></li><li><a href="#%E5%AE%9A%E9%87%8F%E6%B5%8B%E9%87%8F">定量测量</a></li><li><a href="#lint-%E5%B7%A5%E5%85%B7-prealloc">Lint 工具 prealloc</a></li><li><a href="#%E6%80%BB%E7%BB%93">总结</a></li></ul><p>在我代码 review 的过程中经常会关注代码里 slice 的初始化是否分配了预期的内存空间，也就是凡是 <code>var init []int64</code> 的我都会要求尽可能改成 <code>init := make([]int64, 0, length)</code> 格式。但是这个改进对性能究竟有多少影响并没有什么定量的概念，只是教条的去要求。这篇博客会介绍一下预分配内存提升性能的理论基础，定量测量，和自动化检测发现的工具。</p><h1 id="Slice-内存分配理论基础"><a href="#Slice-内存分配理论基础" class="headerlink" title="Slice 内存分配理论基础"></a>Slice 内存分配理论基础</h1><p>Golang Slice 扩容的代码在<a href="https://github.com/golang/go/blob/go1.20.6/src/runtime/slice.go#L157">slice.go 下的 growslice</a>。大体思路是在 Slice 容量小于 256 时<br>每次扩容会创建一个容量翻倍的新 slice；当容量大于 256 后，每次扩容会创建一个容量为原先的 1.25 倍的新 slice。之后会将旧 slice 的数据复制到新的 slice，最终返回新的 slice。</p><p>扩容的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">newcap := oldCap</span><br><span class="line">doublecap := newcap + newcap</span><br><span class="line"><span class="keyword">if</span> newLen &gt; doublecap &#123;</span><br><span class="line">newcap = newLen</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">const</span> threshold = <span class="number">256</span></span><br><span class="line"><span class="keyword">if</span> oldCap &lt; threshold &#123;</span><br><span class="line">newcap = doublecap</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// Check 0 &lt; newcap to detect overflow</span></span><br><span class="line"><span class="comment">// and prevent an infinite loop.</span></span><br><span class="line"><span class="keyword">for</span> <span class="number">0</span> &lt; newcap &amp;&amp; newcap &lt; newLen &#123;</span><br><span class="line"><span class="comment">// Transition from growing 2x for small slices</span></span><br><span class="line"><span class="comment">// to growing 1.25x for large slices. This formula</span></span><br><span class="line"><span class="comment">// gives a smooth-ish transition between the two.</span></span><br><span class="line">newcap += (newcap + <span class="number">3</span>*threshold) / <span class="number">4</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Set newcap to the requested cap when</span></span><br><span class="line"><span class="comment">// the newcap calculation overflowed.</span></span><br><span class="line"><span class="keyword">if</span> newcap &lt;= <span class="number">0</span> &#123;</span><br><span class="line">newcap = newLen</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因此理论上如果预分配好 slice 的容量，不需要动态扩张我们可以在好几个地方有性能的提升：</p><ol><li>内存只需要一次分配，不需要反复分配。</li><li>不需要反复进行数据复制。</li><li>不需要反复对旧的 slice 进行垃圾回收。</li><li>内存准确分配，不存在动态分配导致的容量浪费。</li></ol><p>理论上来看，预分配 slice 容量相比动态分配会带来性能提升，但具体提升有多少就需要定量测量了。</p><h1 id="定量测量"><a href="#定量测量" class="headerlink" title="定量测量"></a>定量测量</h1><p>我们参考 <a href="https://github.com/alexkohler/prealloc/blob/master/prealloc_test.go">prealloc</a> 的代码进行简单修改来测量不同容量的 slice 预分配和动态分配对性能的影响。</p><p>测试代码如下，通过修改 <code>length</code> 可以观察不同情况下的性能数据：</p><figure class="highlight go"><figcaption><span>title</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> prealloc_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;testing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> length = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">b.ResetTimer()</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line"><span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line"><span class="keyword">var</span> init []<span class="type">int64</span></span><br><span class="line"><span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">init = <span class="built_in">append</span>(init, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">b.ResetTimer()</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line"><span class="comment">// Preallocate our initial slice</span></span><br><span class="line">init := <span class="built_in">make</span>([]<span class="type">int64</span>, <span class="number">0</span>, length)</span><br><span class="line"><span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">init = <span class="built_in">append</span>(init, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一个函数测试动态分配的性能，第二个函数测试预分配的性能。通过下面的命令可以执行测试：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go <span class="built_in">test</span> -bench=. -benchmem prealloc_test.go</span><br></pre></td></tr></table></figure><p>在 <code>length = 1</code> 情况下的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocate-12       40228154                27.36 ns/op            8 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12         55662463                19.97 ns/op            8 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>在 <code>length</code> 为 1 的情况下，理论上动态分配和静态分配都要进行一次初始化的内存分配，性能不应该有差异，但是实测下来，预分配的耗时为动态分配的 70%，即使在两者内存分配次数一直的情况下，预分配依然有 1.4x 的性能优势。目测性能提升和变量的连续分配相关。</p><p>在 <code>length = 10</code> 情况下的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocate-12        5402014               228.3 ns/op           248 B/op          5 allocs/op</span><br><span class="line">BenchmarkPreallocate-12         21908133                50.46 ns/op           80 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>在 &#96;length&#96;&#96; 为 10 的情况下，预分配依然只进行了一次性能分配，动态分配进行了 5 次性能分配，预分配的性能是动态分配性能的 4 倍。可见即使在 slice 规模较小的时候，预分配依然会有比较明显的性能提升。</p><p>下面是在 <code>length</code> 分别为 129,1025 和 10000 情况下的测试结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># length = 129</span></span><br><span class="line">BenchmarkNoPreallocate-12         743293              1393 ns/op            4088 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocate-12          3124831               386.1 ns/op          1152 B/op          1 allocs/op</span><br><span class="line"></span><br><span class="line"><span class="comment"># length = 1025</span></span><br><span class="line">BenchmarkNoPreallocate-12         169700              6571 ns/op           25208 B/op         12 allocs/op</span><br><span class="line">BenchmarkPreallocate-12           468880              2495 ns/op            9472 B/op          1 allocs/op</span><br><span class="line"></span><br><span class="line"><span class="comment"># length = 10000</span></span><br><span class="line">BenchmarkNoPreallocate-12          14430             86427 ns/op          357625 B/op         19 allocs/op</span><br><span class="line">BenchmarkPreallocate-12            56220             20693 ns/op           81920 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>在更大容量下，静态分配依然只做一次内存分配，但是性能提升并没有相应成倍增长，整体性能会是动态分配的 2 到 4 倍。应该是在这个过程中有一些其他的消耗，或者 golang 对大容量的复制有特殊的优化，因此性能差距并没有拉大。</p><p>当把 slice 的内容换成更复杂的 struct 时，原以为复制会带来更大的性能开销，但实测复杂 struct 预分配和动态分配的性能差距反而更小，看上去还是有很多内部的优化，表现和直觉并不一致。</p><h1 id="Lint-工具-prealloc"><a href="#Lint-工具-prealloc" class="headerlink" title="Lint 工具 prealloc"></a>Lint 工具 prealloc</h1><p>尽管预分配内存可以带来一定的性能提升，但是在比较大的项目中完全依赖人工去 review 这个问题很容易出现纰漏。这时候就需要用到一些 lint 工具来自动做代码扫描了。<a href="https://github.com/alexkohler/prealloc">prealloc</a> 就是这样一个工具可以扫描潜在的能够预分配但却没有预分配的 slice，并且可以整合到 <a href="https://golangci-lint.run/usage/linters/#prealloc">golangci-lint</a> 中。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>整体来看 slice 的内存预分配是个比较简单但却能有比较好优化效果的方法，即使在 slice 容量很小的情况下，预分配依然能有比较明显的性能提升。通过 prealloc 这种静态代码扫描工具，可以比较方便的实现这类潜在优化的检测并集成到 CI 中简化日后的操作。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#slice-%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80&quot;&gt;Slice 内存分配理论基础&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5</summary>
      
    
    
    
    
    <category term="性能" scheme="http://oilbeater.com/tags/%E6%80%A7%E8%83%BD/"/>
    
    <category term="golang" scheme="http://oilbeater.com/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>无需改动代码的性能优化方法 —— PGO</title>
    <link href="http://oilbeater.com/2023/06/24/optimization-without-changing-code-pgo/"/>
    <id>http://oilbeater.com/2023/06/24/optimization-without-changing-code-pgo/</id>
    <published>2023-06-24T08:58:29.000Z</published>
    <updated>2023-06-26T08:22:25.904Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#pgo-in-general">PGO in General</a></li><li><a href="#pgo-in-golang">PGO in Golang</a><ul><li><a href="#%E6%94%B6%E9%9B%86-profile-%E4%BF%A1%E6%81%AF">收集 Profile 信息</a></li><li><a href="#%E7%BC%96%E8%AF%91%E4%BC%98%E5%8C%96">编译优化</a></li></ul></li><li><a href="#pgo-for-kernel">PGO for Kernel</a></li><li><a href="#%E6%80%BB%E7%BB%93">总结</a></li></ul><p>Golang 在 1.20 引入了 <a href="https://go.dev/doc/pgo">PGO(Profile-guided optimization)</a> 的优化，根据官方博客的介绍可以在不更改代码的情况下达到 2%-7% 的性能提升。在 1.21 的 <a href="https://tip.golang.org/doc/go1.21">Release Note</a> 中，Golang 将该功能升级到 GA 并在自己的构建中开启了 PGO，将 Golang 自身编译器的性能提升了 2%-4%。PGO 本身是个编译器的优化方法，并不和特定语言相关，Golang 目前也只是很初步的应用了 PGO，这篇文章将会以 Golang 为例介绍 PGO 的工作原理，可能的优化方向和一个应用 PGO 优化 Linux Kernel 的例子。</p><h1 id="PGO-in-General"><a href="#PGO-in-General" class="headerlink" title="PGO in General"></a>PGO in General</h1><p>通常我们认为静态编译型语言的运行性能要好于动态解释型的语言，但是随着 <a href="https://en.wikipedia.org/wiki/Just-in-time_compilation">JIT(Just In Time)</a> 技术的引入，动态语言性能有了大幅提升，因为 JIT 可以根据运行时的情况进行针对的强化。</p><p>例如：</p><ul><li>热点代码编译成机器码</li><li>函数内联</li><li>分支预测</li><li>循环展开</li><li>类型推断</li><li>内存分配优化</li><li>寄存器优化</li></ul><p>JIT 通过一系列运行时的优化可以达到和编译型语言接近的性能，在部分情况下由于可以动态根据情况做一些编译期间无法确认的优化，甚至会有比编译型语言更好的性能。例如 Golang 中的函数内联是写死的规则稍，主要看函数大小，而不是函数使用频率。分支预测和循环展开编译器由于不知道分支运行时的频率分布，也无法做专门的优化。Golang 中 Slice 和 Map 的初始化大小需要通过参数手动指定或者自动根据写死的规则进行扩容，无法根据运行时信息分配一个合适的初始化大小。</p><p>于是一个很自然的想法就是能否将类似 JIT 的技术应用到编译型语言，通过运行时的信息去优化代码性能，传统的做法是通过人类对代码运行时理解的经验，主动的在代码中加入编译提示信息，帮助编译器优化，例如 C++ 中的 inline 函数，C 语言中的宏，GCC 提供的 likely&#x2F;unlikely。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> likely(x) __builtin_expect(!!(x), 1) <span class="comment">//gcc内置函数, 帮助编译器分支优化</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> unlikely(x) __builtin_expect(!!(x), 0)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>* argv[])</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> x = <span class="number">0</span>;</span><br><span class="line">    x = <span class="built_in">atoi</span>(argv[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">unlikely</span>(x == <span class="number">3</span>))&#123;  <span class="comment">//告诉编译器这个分支非常不可能为true</span></span><br><span class="line">        x = x + <span class="number">9</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        x = x - <span class="number">8</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;x=%d\n&quot;</span>, x);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这种方式存在几个问题：</p><ol><li>需要对所有分支，函数进行考虑，工作量大，并且调整复杂。</li><li>需要依赖程序员凭借经验推测，不一定符合实际场景。</li><li>只能对自己编写的代码进行调整比较方便，一些依赖的库函数调整困难，比如一个应用开发者可能需要向下调整到 glibc 的代码。</li></ol><p>那么能不能有一种方法可以不调整代码，自动根据运行时的信息对代码的编译进行全局的优化呢，这个方法就是 PGO。通过收集运行时的 Profile 信息反过来优化编译过程。</p><p>一个典型的使用 PGO 的工作流分为下面几步：</p><ol><li>构建初始版本二进制，不带任何 PGO 的优化。</li><li>在生产环境收集 Profile 信息。</li><li>重新构建二进制，并使用 2 中收集的 Profile 信息进行构建优化。</li><li>回到 2，持续迭代。</li></ol><p>Google 通过 <a href="https://research.google/pubs/pub45290/">AutoFDO</a> 实现了持续的 PGO，同时也介绍了 PGO 存在的问题：</p><ol><li>需要配合 pprof 信息，生产环境完整的 Profile 会带来性能下降，通常使用 Sample 的方式牺牲一定精度，开销在 1%。</li><li>需要随着代码动态调整 Profile，不是个一次性的优化，Profile 信息可能会有泄密隐患，如何构建完整流程是个挑战。</li><li>整体性能提升有限，整体性能优化在 10% 左右。</li><li>二进制体积增加。</li></ol><h1 id="PGO-in-Golang"><a href="#PGO-in-Golang" class="headerlink" title="PGO in Golang"></a>PGO in Golang</h1><p>Golang 目前有两个主流的编译器，gc(go compiler) 和 <a href="https://github.com/golang/gofrontend">gccgo</a>。</p><table><thead><tr><th></th><th>gc</th><th>gccgo</th></tr></thead><tbody><tr><td>优点</td><td><ol>1. 官方支持</ol><ol>2. 兼容性好</ol><ol>3. 编译速度较快</ol></td><td><ol>1. 可以使用 GCC 更多优化能力，性能较好</ol><ol>2. 可以支持更多 CPU 架构和系统</ol></td></tr><tr><td>缺点</td><td><ol>1. 优化较为保守</ol></td><td><ol>1. 跟随 GCC 发版，不支持 Golang 新特性，潜在兼容性问题</ol><ol>2. 安装使用复杂</ol><ol>3. 较慢编译速度</ol></td></tr></tbody></table><p>gccgo 使用前后端分离架构，后端已经支持 PGO，这里主要讨论的是官方 gc 的 PGO 优化。</p><h2 id="收集-Profile-信息"><a href="#收集-Profile-信息" class="headerlink" title="收集 Profile 信息"></a>收集 Profile 信息</h2><p>Golang 的 PGO 目前只支持通过 CPU Profile 进行优化，我们可以通过 Golang 标准库里的 runtime&#x2F;pprof 或者 net&#x2F;http&#x2F;pprof 进行 CPU Profile 的采集，如果是其他的 Profile 采集器的数据如果能转换成 <a href="https://github.com/google/pprof/tree/main/proto">Google pprof</a> 的格式也可以兼容。</p><p>需要注意的是由于 PGO 使用的是类似 JIT 的优化方式，因此最好在真实的生产环境收集 Profile 信息，才能最贴合程序实际的运行条件，方便进行后期的优化。也可将多台机器上收集的 Profile 信息进行合并 <code>go tool pprof -proto a.pprof b.pprof &gt; merged.pprof</code>.</p><h2 id="编译优化"><a href="#编译优化" class="headerlink" title="编译优化"></a>编译优化</h2><p><code>go build -gpo=/tmp/foo.pprof</code> 即可在编译过程中通过 Profile 信息进行优化。Golang 目前针只实现了<a href="https://go-review.googlesource.com/c/proposal/+/430398/10/design/55022-pgo-implementation.md#208">函数内联优化</a>，尝试将调用比例大于 2% 的函数进行内联。更多的优化还在画大饼，目前画的大饼可以参考 <a href="https://github.com/golang/go/issues/55022#issuecomment-1245605666%E3%80%82">https://github.com/golang/go/issues/55022#issuecomment-1245605666。</a></p><p><img src="/../images/pgo-in-go.png" alt="Alt text"></p><p>在实践中经常碰到的场景是在跑完 profile 后不会直接重新编译，而是在下次代码变更后再编译发布。这样带来的问题就是 Profile 的信息和代码是存在差异的，Golang 目前通过一些启发式的规则，可以在代码和 Profile 不一致的情况下尽可能的工作。</p><h1 id="PGO-for-Kernel"><a href="#PGO-for-Kernel" class="headerlink" title="PGO for Kernel"></a>PGO for Kernel</h1><p>PGO 一般用于应用的性能优化，但是一些服务器上的应用有可能是系统调用密集型的，在这种情况下其实可以根据应用对内核进行 PGO，打造一个针对应用优化过后的内核。这里可以参考一下早些年我上学时实验室一个大神师弟的工作 <a href="http://coolypf.com/kpgo.htm">Profile-Guided Operating System Kernel Optimization</a>，我当时也是通过这个工作才了解到了 PGO。</p><p><img src="/../images/kernel-pgo.png" alt="Alt text"></p><p>图片里显示的是不同软件在优化后的吞吐量提升，可以看到在完全不用动应用代码，只是优化内核的编译，吞吐量就会有 2%~10% 的提升。在 Nginx 的例子里出现了性能的下降，我记的当时大神说过是 GCC 在 PGO 的一个 Bug，导致了错误的优化。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>PGO 是一个不需要改动代码就可以获得性能优化的方法，结合 Golang 的 pprof 可以做很好的配合，但是优化程度也相应有限，而且需要配合发布上线流程，可以作为一个性能优化的尝试。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#pgo-in-general&quot;&gt;PGO in General&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#pgo-in-golang&quot;&gt;PGO in Golang&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%94%B6%E9%9B%</summary>
      
    
    
    
    
    <category term="性能" scheme="http://oilbeater.com/tags/%E6%80%A7%E8%83%BD/"/>
    
    <category term="golang" scheme="http://oilbeater.com/tags/golang/"/>
    
    <category term="PGO" scheme="http://oilbeater.com/tags/PGO/"/>
    
    <category term="Linux" scheme="http://oilbeater.com/tags/Linux/"/>
    
    <category term="Kernel" scheme="http://oilbeater.com/tags/Kernel/"/>
    
  </entry>
  
  <entry>
    <title>我和耗子叔的回忆</title>
    <link href="http://oilbeater.com/2023/05/19/memory-with-haochen/"/>
    <id>http://oilbeater.com/2023/05/19/memory-with-haochen/</id>
    <published>2023-05-19T00:05:02.000Z</published>
    <updated>2023-06-24T08:56:22.090Z</updated>
    
    <content type="html"><![CDATA[<h1 id="我和耗子叔的回忆"><a href="#我和耗子叔的回忆" class="headerlink" title="我和耗子叔的回忆"></a>我和耗子叔的回忆</h1><h2 id="初始酷壳"><a href="#初始酷壳" class="headerlink" title="初始酷壳"></a>初始酷壳</h2><p>和很多人一样，第一次接触耗子叔是通过 coolshell.cn 博客。</p><p>当时印象最深，对我之后影响最大的应该是 <a href="https://coolshell.cn/articles/2105.html">分享：我的READER订阅</a> 。当年尽管我已经是大三的计算机专业的学生，但是课本外的专业知识还是相当差劲，也不知道哪里才能获得一手的信息和资料。让我一个象牙塔里的学生，接触到工业界最先进的技术就是从这个列表开始的。这个 RSS 的列表陪伴我度过了很多年，很长时间我的计算机技术相关的信息都是依赖这个列表。</p><h2 id="亚马逊的擦肩而过"><a href="#亚马逊的擦肩而过" class="headerlink" title="亚马逊的擦肩而过"></a>亚马逊的擦肩而过</h2><p>几年后在我实习的时候，碰巧去了亚马逊中国耗子叔在的那个部门，不过当时耗子叔已经去阿里了，没能在亚马逊碰到。实习结束后我专门写了篇博客记录当时在亚马逊实习的经历，抱着蹭流量的心态我在微博发博客的时候 @ 了一下耗子叔。本来是抱着蹭到就算赚的心态，没想到耗子叔很快就转发了，当时博客流量就上来了，我还拿出去和同学们炫耀了一段。开始写博客的初衷也是受耗子叔博客的影响，这个博客几乎是我早些年工作没啥成就时，唯一能拿得出手的东西了。</p><p><img src="/../images/image1.png" alt="image1.png"></p><h2 id="从阿里到-Docker"><a href="#从阿里到-Docker" class="headerlink" title="从阿里到 Docker"></a>从阿里到 Docker</h2><p>第一份工作去了阿里，总算是和耗子叔到了一个公司，但是部门差的太远，没有什么业务的交叉，起初也就没什么交流。当时 Docker 开始崭露头角了，阿里内部各个部门都有一拨人都看到了这个技术的突破性，想在阿里内部推广这个技术，几个不同部门的人组了个群聊这些事，我和耗子叔凑巧也在这个群里。当时我还是个应届生，肯定掀不起什么风浪，但其他高级别的人也都碰到了不少阻碍，所以不断有人跳出去单干，我们也戏称这个群变成了前橙会群。</p><p>在当时阿里内部大领导已经拍板不会推进 Docker 了，耗子叔在这个过程中也遇到了不少风波，里面涉及到很多复杂的人和事这里就不多说了。内部做不了，我当时其实也有了去找个创业公司做 Docker 的想法。</p><p>作为社恐的我那天还是鼓足勇气和耗子叔私聊了这件事，耗子叔还是出乎我意料的快速的就恢复我了。在谈到容器在阿里的时候耗子叔情绪还是挺激动的，觉得阿里做事不地道，在阿里搞 Docker 已经不可能了，说了很多的气话。然后我再提及了想去外面的创业公司做 Docker 的事情，本以为情绪都烘到这个地步了，应该会和我再吐槽几句阿里是垃圾，让我赶快去外面吧。结果耗子叔话头一转，开始劝我在阿里先稳住，有一定积累再去看外面的机会。</p><p><img src="/../images/image2.png" alt="image2.png"></p><p>我在那一刻突然意识到，耗子叔是那种我之前很少没见过的，并不是以公司立场甚至不是以个人的立场去考虑问题，而是真的完全出于为你好的立场去考虑问题。而我当时其实只是想找个大牛去肯定我的一个决策。</p><p>之后我还是去了那个创业公司，一直到现在，今天回过来看，如果从经济的角度那我当年还是应该听话的。鬼知道阿里转身就是 All in 云原生。</p><h2 id="多年后的合作"><a href="#多年后的合作" class="headerlink" title="多年后的合作"></a>多年后的合作</h2><p>之后很长一段时间和耗子叔都没什么联系，最近几年我开始做开源项目，一直不瘟不火，不知道该怎么去推广。这时候耗子叔也开始创业了，并且把公司内部的技术分享放到网上，我就又萌生了去蹭流量的想法。说来惭愧，这么多年都没帮过什么忙，每次一有事情还都想着去蹭。另一方面耗子叔创业也在云原生领域，所谓同行是冤家，了解这个圈子的应该知道这个圈子友商之间交情都挺差的，见面不打架就算客气了，我这还想明目张胆的去蹭流量。</p><p>本着蹭到就算赚我还是发了微信，想在他们的分享会上分享一下我们最近的技术进展。耗子叔依然出乎我意料快速的就答应了我，让然后就张罗起来了。</p><p><img src="/../images/image3.png" alt="image3.png"></p><p>尽管很多年都没联系，但耗子叔像老朋友一样就要和我语音聊聊，一聊就聊了一个半小时。从云原生这个领域来说，我们俩几乎是同时进入的，要说正儿八经当职业来做我还更早一些，我本来以为自己还算老兵了，尤其在网络这个领域我也还算可以，但耗子叔的很多观点还是让我觉得自己狭隘了。</p><p>我们聊到当时为了让 kube-ovn 这个项目能被更多人使用，我做了很多在云原生领域并不推荐的功能，让更多非云原生最佳实践也能跑起来，这个成为了我们早期吸引用户的最大卖点。耗子叔很直接的指出我的做法过于功利了，快速的获取用户变成了我最优先的目标，为了完成这个目标我其实在鼓励用户使用并不符合最佳实践的做法。虽然我的做法能吸引用户，但是并没有让用户发挥出云原生最大的价值，这种形式上的云原生最终并不会产生业务上的收益。如果是他的话，他会劝说用户尽可能使用最佳实践，而他所有产品功能也都是依托最佳实践基础来做的，尽管初期改造后会很痛苦，但是改造后能够确定性的有大幅度的改善，而不是为了云原生而云原生。</p><p>说实话，在之前我一直觉得云原生领域已经挺无聊的了，东西就是那些，每天都是各种和现实磨合，甚至都忘了当初是为啥要加入这个领域。大概是理想多次被现实扭曲，所以才变得这么功利吧。</p><p>耗子叔之后介绍了当时 MegaEase 的愿景，当时在外面能看到的还只有一个网关，但背后其实有个更大的梦想。耗子叔希望透过流量入口和云原生的技术把复杂的多云管理变的简单，通过云原生可以做一个跨公有云的系统，符合这个最佳实践的应用，存储、网络、计算都可以在多个云间快速的迁移。这样可以把多个公有云变成自己的资源池，把价格给打下来，给用户提供更优质低价的云服务。</p><p>当时听完后我是有点懵的，技术上我是听懂了的，但是还是被里面的一些有想象力的做法惊住了。耗子叔看气氛差不多了就问我，你们公司现在做的产品我看过了，挺没意思的，也不咋地，你要不要过来和我干这个？我内心 os 了一句你知道做这个不咋地的东西我有多努力嘛？嘴上说那等我把这个公司做黄了再去你们那吧。耗子叔听完哈哈一笑，说这倒也不至于，不至于。</p><p>那时候我们都还以为会有合作的机会，没想到之后再也不会有机会了。</p><h2 id="回想"><a href="#回想" class="headerlink" title="回想"></a>回想</h2><p>回想这些年来和耗子叔的交往，每次都是我有求于他，每次我多觉得这是不是太占用他宝贵的资源了。但是每次都会得到他毫不迟疑的，全情投入的帮助。他对技术理想的执着，对有着技术热情人的支持，只需要只言片语就深深的感染了我，让我在现实的阴霾中又看到了理想的光辉。如果我们每个人做事的时候想想耗子叔哲哲情况会怎么做，遇到有人需要帮助时想想如果是耗子叔会怎样帮我，那我想耗子叔不只有数字分身，也会在我们每个人的精神世界中有个分身。我想说他是个高尚的人，纯粹的人，在搜这句话出处是发现出自《纪念白求恩》，里面的一些话似乎正是再说耗子叔，就让这段引文作为这个纪念的结尾吧。</p><blockquote><p>从前线回来的人说到白求恩，没有一个不佩服，没有一个不为他的精神所感动。晋察冀边区的军民，凡亲身受过白求恩医生的治疗和亲眼看过白求恩医生的工作的，无不为之感动。每一个共产党员，一定要学习白求恩同志的这种真正共产主义者的精神。<br>白求恩同志是个医生，他以医疗为职业，对技术精益求精；在整个八路军医务系统中，他的医术是很高明的。这对于一班见异思迁的人，对于一班鄙薄技术工作以为不足道、以为无出路的人，也是一个极好的教训。<br>我和白求恩同志只见过一面。后来他给我来过许多信。可是因为忙，仅回过他一封信，还不知他收到没有。对于他的死，我是很悲痛的。现在大家纪念他，可见他的精神感人之深。我们大家要学习他毫无自私自利之心的精神。从这点出发，就可以变为大有利于人民的人。一个人能力有大小，但只要有这点精神，就是一个高尚的人，一个纯粹的人，一个有道德的人，一个脱离了低级趣味的人，一个有益于人民的人。 </p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;我和耗子叔的回忆&quot;&gt;&lt;a href=&quot;#我和耗子叔的回忆&quot; class=&quot;headerlink&quot; title=&quot;我和耗子叔的回忆&quot;&gt;&lt;/a&gt;我和耗子叔的回忆&lt;/h1&gt;&lt;h2 id=&quot;初始酷壳&quot;&gt;&lt;a href=&quot;#初始酷壳&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>2023 周报 #2：Github Action 优化和 ChatGPT</title>
    <link href="http://oilbeater.com/2023/02/26/2023-Weekly-2/"/>
    <id>http://oilbeater.com/2023/02/26/2023-Weekly-2/</id>
    <published>2023-02-26T13:49:45.000Z</published>
    <updated>2023-06-24T08:56:22.090Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Github-Action-优化"><a href="#Github-Action-优化" class="headerlink" title="Github Action 优化"></a>Github Action 优化</h1><p>Kube-OVN 前段时间做了 E2E 测试的重构，增加了大量的测试，导致 Github Action 经常被占满，PR 经常好久才能合并，这周花时间和小伙伴优化了 Action 的执行时间，如果你也在 Github 上运行 Kubernetes 相关的 workflow 下面的优化方法可能会对你有帮助。 </p><ul><li>根据代码变动选择运行的测试和构建。比如在 <code>Kube-OVN</code> 中 <code>NetworkPolicy</code> 兼容性测试花费时间较长，那就可以根据代码目录判断，如果变动代码和 NetworkPolicy 无关，这部分测试就不运行了。</li><li>Github public repo 最大并发数是 20，如果 workflow 拆分的太细，超过 20 个并发，那么准备阶段的开销可能会大于并发节省的时间，需要在并发数量和每次任务的准备时间之间做权衡。</li><li>Cache 的 restore 和 save 都需要花时间，1G 内容 restore 大概 45s，save 要 3min，要考虑流水线的情况选择使哪些使用 cache，哪些就不用了。</li><li><code>kind</code> 起一个 <code>Kubernetes</code> 集群大概要 1m，<code>k3d</code> 差不多 10s，我们的环境有些兼容性问题，如果没有兼容性问题 CI 测试可以优先使用 <code>k3d</code>。</li><li><code>Ginkgo</code> 有并发测试的选项，很多 <code>Kubernetes</code> 的测试需要等待状态就绪，CPU 消耗并不大，可以适度增加并发度。</li><li>同样是因为 <code>Kubernetes</code> 中有很多等待状态的 <code>sleep</code>，可以把 <code>sleep</code> 间隔调小。比如 <code>sleep</code> 从 5s 调到 1s，那平均下来每次运行会节省 2s。</li><li><code>kubectl delete</code> 会默认等待删除完全结束，在删除 Pod 时可能会花较长时间，尤其是批量删除会一个个等，可以用 <code>--wait=false</code> 避免等待。</li><li>同样是和等待状态相关，设了 <code>readinessProbe</code> 的工作负载会等待 <code>initialDelaySeconds</code> 之后再探测，状态才会变成 <code>ready</code>，可以把 <code>initialDelaySeconds</code> 删掉，这样能加速启动。</li></ul><h1 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h1><p>这周菲律宾的 globe 手机卡到手了，总算能成功注册一个 openAI 的账号，不需要关各种代理的限制，能比较充分的去使用了。这个星期主要用 chatGPT 做了如下几个事情：</p><ul><li>写了封拖了很久的英文邮件，输入邮件的目的和一些背景信息让它写，第一次输出的有些过于客气和啰嗦，让它自己简化了一次后又太干脆了，结合两个版本自己微调一下就发出去了，前后不到 5 分钟就搞定了。</li><li>优化了给 Cilium 提的一个 Feature Request，这次是我自己先写了个初版，中途有些概念问题和一些背景知识问了一下，最后把文本交给他优化，一下子感觉 native 了不少，也是再微调一下就直接发了。</li><li>看的一些英文内容，不太理解的地方直接把整段辅助过去让他给我解释，不仅有翻译，还有背景知识，比一般的词典都好用。比如这句 <code>No provider or user of an interactive computer service shall be treated as the publisher or speaker of any information provided by another information content provider.</code></li><li>学 JavaScript 的过程中临时遇到的一些疑问，想到了就去问，概念和入门性的东西讲的相当详细，感觉就算找个老师都不会那么生动耐心的去讲。</li><li>直接帮我写了几个 JavaScript 的函数，发现 chatGPT 超级喜欢正则，还会详细讲解正则表达式，最后还给了几个测试用例。不过有一个函数正则写错了，看测试用例看出问题，告诉他哪里有问题，他就改正了。</li><li>帮我爸写了篇退休感言，帮我爸同事要写的书列了个大纲，编了个猪八戒和林黛玉的故事。我爸思考了一天后和我说文字工作者不存在了，现在的人想要学东西太容易了，编这个程序的人不简单，这个东西会不会卡脖子，小爱什么的不是一个层次的东西。</li><li>问 <code>logseq</code> 一个不存在的分享发布功能，他非说有，还告诉我是怎么实现的。页面交互，后台逻辑，可能用到的技术都编出来了，其中一个用 gist 来实现的方案看上去还不错。</li><li>让他帮我列了一下今年的 Roadmap，高度吻合后又让他列了下竞品的 Roadmap。</li></ul><p>给我的感受是使用 chatGPT 后，很多任务的难度从创作级别降到了 review 级别，不管是文本生成，代码生成甚至是 Roadmap 生成，原本需要从 0 开始创作，现在变成了基于一个还不错的输出做质量检查和微调。由于任务完全变成了另一个类型，从产出的角度来讲效率绝对是大幅提升了。但是我隐隐有些担心如果形成了路径依赖，跳过了中间过程直接拿到结果，创造的能力会不会受到影响？</p><p>对于用 chatGPT 辅助学习，实际效果也很不错，相比搜索引擎需要在很多信息中做过滤，chatGPT 能一下子提供直接相关的信息，可以减少很多注意力的转移。不过胡说八道的情况存在的比例也不低，还是要有一定的判断。当感觉出有问题的时候还是要追问几句，再去搜索引擎交叉验证一下。</p><p>我希望未来所有软硬件的用户交互方式都能提供一个 chat 的接口，在经历了鼠标、键盘、触屏后又能以一种更高效的形式回归到语音交互。想一想这不就是老罗当年的 TNT 么。  </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Github-Action-优化&quot;&gt;&lt;a href=&quot;#Github-Action-优化&quot; class=&quot;headerlink&quot; title=&quot;Github Action 优化&quot;&gt;&lt;/a&gt;Github Action 优化&lt;/h1&gt;&lt;p&gt;Kube-OVN 前段时间做</summary>
      
    
    
    
    
    <category term="周报" scheme="http://oilbeater.com/tags/%E5%91%A8%E6%8A%A5/"/>
    
  </entry>
  
  <entry>
    <title>2023 周报 #1：Twitter，第二大脑和数字移民</title>
    <link href="http://oilbeater.com/2023/02/19/2023-weekly-1/"/>
    <id>http://oilbeater.com/2023/02/19/2023-weekly-1/</id>
    <published>2023-02-19T15:27:45.000Z</published>
    <updated>2023-06-24T08:56:22.090Z</updated>
    
    <content type="html"><![CDATA[<p>最近在 Twitter 上关注了很多写博客的小伙伴，发现很多都有写周报的习惯，DNA 又动了。给自己写周报是个有意思的事情，能在忙碌中让自己找到生活中的闪光点，于是就跟一下风，看看自己能不能重拾写作的乐趣。</p><h1 id="重回-Twitter"><a href="#重回-Twitter" class="headerlink" title="重回 Twitter"></a>重回 Twitter</h1><p>我的 Twitter 账号 13 年前就注册了，但是最近为了能更好的学一下英语表达才开始使用。意外发现里面的中文内容质量很高，而且发现了很多有意思的小伙伴。</p><p>和国内的很多社交平台相比，Twitter 限制更少的环境更能激发思考和表达的欲望。尽管在国内平台有些话题的讨论并不涉及审核的红线，但是审核的存在还是会潜移默化的影响表达的质量，并且压制了表达。当然在 Twitter 上也得设置重重的过滤规则才能让自己的 Timeline 不是那么乌烟瘴气。</p><p>后续我的一些观点和想法主要都会在 Twitter 上发布，感兴趣的小伙伴也可以关注一下<a href="https://twitter.com/liumengxinfly">我的 Twitter</a>。</p><p>在 Twitter 上发现了个奇怪的现象，每当国内有个地方出新闻，想去搜索的时候发现全是些奇奇怪怪的推文排在最前，难道这是另一种奇怪的信息管制？</p><p>总体感觉下来 Twitter 经过过滤后还是个不错的信息源，但是收集会存在一定的困难。feedly 等订阅工具需要付费，<a href="https://twitter.com/SaveToNotion">SaveToNotion</a> 和 <a href="https://twitter.com/readwise">readwise</a> 会产生大量无用的垃圾回复，影响 Timeline。目前看起来 logseq 的 Twitter block 可能是个相对轻量的收集方式了。</p><p>此外推荐一下 <a href="https://ponyexpress.notion.site/Twemex-Manual-71b320f4d95f48d38ca68fe6eaa3c49e">Twemex</a>， 是个不错的浏览器插件，可以展示关注用户历史上被 like 最多的 Tweet，快速查看精华内容。</p><h1 id="第二大脑"><a href="#第二大脑" class="headerlink" title="第二大脑"></a>第二大脑</h1><p>如果不了解第二大脑概念，可以参考一篇最近发现的博客 <a href="https://justgoidea.com/blog/post-024">在 Heptabase 中构建第二大脑</a>。第二大脑简单来说就是一个笔记，把日常学习、工作和生活中的思考记录下来，每个人多多少少其实都做过。但是能不能作为一个系统整体运转起来，在笔记背后其实还是要有一套工作流程和方法论去支撑的。</p><p>其实这个事情零零散散搭建了一段时间了，中途用过很多工具组合，也放弃过很多次，不过最近找到了一些感觉，开始持续的去运行这个系统。目前我这套系统里，主要是 feedly + 微信阅读（可以订阅公众号）做信息收集。初筛的信息（一般就是过一下标题和第一段）打 tag 后发到 Cubox 作为知识库。Cubox 里的内容精读后再把笔记和摘要输出到 logseq。</p><p>后续计划把部分 logseq 整理后的内容，用 <a href="https://github.com/pengx17/logseq-publish">logseq-publish</a> 发布出来，看看能不能和大家有更多的互动。</p><h1 id="数字移民"><a href="#数字移民" class="headerlink" title="数字移民"></a>数字移民</h1><p>事情的起因和最近大火的 chatGPT 有关，当想体验是发现遇到了重重阻碍。VPN、海外手机卡和海外信用卡其实绕路或者用第三方平台也能搞定，但用第三方平台尤其是手机号有比较大的安全隐患，注册的账号也容易出问题，并不是彻底的解决方案。于是在找合法途径办理国外的一套东西时关注到了数字移民。</p><p>数字移民这个事情甚至成为了一些政府的生意，感兴趣的可以看下 <a href="https://www.frontlinefellowship.io/blog/youyou?categoryId=320164">愛沙尼亞的中國「數字居民」：通往新身份之路</a> 如果想办理美国的数字身份可以参考<a href="https://twitter.com/madawei2699/status/1624564508168683527">这个推文</a>。</p><p>不过整体来看成本都不低，而且风险未知，我先试着搞套能用的海外手机号码和银行卡，后续有进展再来分享。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近在 Twitter 上关注了很多写博客的小伙伴，发现很多都有写周报的习惯，DNA 又动了。给自己写周报是个有意思的事情，能在忙碌中让自己找到生活中的闪光点，于是就跟一下风，看看自己能不能重拾写作的乐趣。&lt;/p&gt;
&lt;h1 id=&quot;重回-Twitter&quot;&gt;&lt;a href=&quot;</summary>
      
    
    
    
    
    <category term="周报" scheme="http://oilbeater.com/tags/%E5%91%A8%E6%8A%A5/"/>
    
  </entry>
  
  <entry>
    <title>烧钱工作法 —— 远程开发是怎样提升我的效率？</title>
    <link href="http://oilbeater.com/2023/01/15/how-remote-coding-improve-productivity/"/>
    <id>http://oilbeater.com/2023/01/15/how-remote-coding-improve-productivity/</id>
    <published>2023-01-15T04:20:45.000Z</published>
    <updated>2023-02-19T15:05:26.719Z</updated>
    
    <content type="html"><![CDATA[<p>这段时间为了解决本地开发环境资源和网络的问题尝试了下远程开发(IDE 前端在本地，代码后端再远端服务器的模式)，实践了一段时间发现远程开发带来的意外好处是可以通过实时烧钱大幅提升任务的紧迫度和开发效率，这个带来的效率提升甚至比之前预想的能从计算资源和网络里抠出来的还多。分享一下这段时间的经验，希望对考虑远程开发的人有帮助。</p><h2 id="为什么要远程开发"><a href="#为什么要远程开发" class="headerlink" title="为什么要远程开发"></a>为什么要远程开发</h2><p>最开始考虑远程开发的动机主要有下面几个：</p><ul><li>我这边的本地开发和测试可能需要启动多个 Kubernetes 集群，对 CPU 和内存消耗都比较大，17 年的 MacBook Pro 启动和运行速度都比较慢。</li><li>生活工作 App 在一个机器磁盘很容易莫秒奇妙的不够，尝试新项目下镜像经常磁盘不足，需要来回扣空间。</li><li>网络问题，需要各种切代理，叠加 Docker 环境会变得更复杂。</li><li>Mac 下的 Linux 开发还是存在工具和用法的不兼容，并不是那么平滑，很多时候还要起 Linux VM。</li><li>换新 Mac 貌似只能解决部分 CPU 性能问题（存疑，因为 Docker 需要虚拟化，ARM 在没有特定加速指令下可能性能会更差），其他问题依然存在，而且新 Mac 实在是在抢钱。</li><li>看了下 GCP 的 Spot 实例，可以按秒计费，算下来买电脑的钱购买好几年的实例了，而且理论上可以解决上面所有的问题。</li></ul><h2 id="选择过程"><a href="#选择过程" class="headerlink" title="选择过程"></a>选择过程</h2><p>最早开始调研的是 Github 的 CodeSpaces，因为和 Github 在一块的开发体验还是很流畅的，不过我用了多年 Goland 切 VS Code 不太习惯。而且貌似 CodeSpaces 是起的容器开发环境，而我这边有很多再跑容器的测试需求，有时还需要测 eBPF 需要完整的 Linux 环境就只能作罢。</p><p>回到 Goland 其实 JetBrains 自己也有类似 CodeSpaces 的 Space 提供全托管的服务，但问题也是类似的感觉提供的是个容器开发环境。AWS 和 GCP 上也提供了 Jetbrains 的半托管服务，会根据你的需求自动启停 VM 运行 IDE 的 Backend。但是看上去用的不是 Spot 实例，而且默认是两小时无交互才会关机。考虑到我这边如果流畅开发可能需要的资源比较多，这个浪费还是比较严重的。</p><p>最后我这边选择的方案是用最基础的 GCP 香港的 Spot 实例，几次调整后配置扩到了 8C16G，配合 EIP 固定公网地址，再通过脚本控制开关机来节省费用。Goland 这边下载个 Gateway 的插件，就可以引导你去通过 SSH（要给这个 SSH 设置代理，不然还是有网络不稳定情况）连接到自己的开发机器并做相应配置。这样就完成了远程开发环境的搭建，并且有个完整的 Linux 环境整体的配置都很灵活，还可以根据自己的需求继续扩充配置。</p><h2 id="使用体验"><a href="#使用体验" class="headerlink" title="使用体验"></a>使用体验</h2><p>该说不说，用上远程开发后，最初的几个问题都解决了，下载资源网络基本无感知，扩容到 8C16G 后代码的编译，起 kind 集群都飞快。</p><p>不过最直接的感受并不是以上那些，当启动机器的脚本启动后我这边就感觉点起了一个烧钱的火炉，由于 GCP 的计算资源是按秒计费的，那种感觉真的就是实时烧钱。所以基本上火炉一开就什么都顾不上了，注意里全部集中到了当前的任务上，根本不想分心。这段时间别人找我就根本不想理；要是有什么操作或者思路的错误导致耽误时间了，就想抽自己；之前懒得弄的一些脚本自动化，和一些工具的高效使用技巧，快捷键什么的也都开始研究了。由于这种被烧着工作的感觉过于刺激，有时候一些不想干又必须干的工作，并不是必须远程开发，比如文档什么的，我也会先把钱烧起来，然后就很刺激的完成了。</p><p>现在想起来其实想要提高自己的效率，并不一定要很强的自制力去痛苦的逼自己去做不想做的事情，而是要有不断的反馈让自己能尽快的看到效果。这种烧钱的负反馈效果可能比正反馈效果更好，GCP 这种实时烧钱的实时负反馈会刺激的完全不想别的事情了。</p><p>有感兴趣的可以也来试下，分享下体验，看看烧钱工作法是不是能提升你的效率。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这段时间为了解决本地开发环境资源和网络的问题尝试了下远程开发(IDE 前端在本地，代码后端再远端服务器的模式)，实践了一段时间发现远程开发带来的意外好处是可以通过实时烧钱大幅提升任务的紧迫度和开发效率，这个带来的效率提升甚至比之前预想的能从计算资源和网络里抠出来的还多。分享</summary>
      
    
    
    
    
    <category term="技术" scheme="http://oilbeater.com/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="效率" scheme="http://oilbeater.com/tags/%E6%95%88%E7%8E%87/"/>
    
    <category term="云计算" scheme="http://oilbeater.com/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    <category term="远程开发" scheme="http://oilbeater.com/tags/%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>2022 年终总结</title>
    <link href="http://oilbeater.com/2022/12/30/2022-summary/"/>
    <id>http://oilbeater.com/2022/12/30/2022-summary/</id>
    <published>2022-12-30T06:21:07.000Z</published>
    <updated>2023-02-19T15:05:26.719Z</updated>
    
    <content type="html"><![CDATA[<p>作为六年没有更新博客的僵尸博主，在看了 <a href="https://twitter.com/gaocegege">Ce Gao</a> 连续九年的<a href="http://gaocegege.com/Blog/%E9%9A%8F%E7%AC%94/newyear2022">年终总结</a>后 DNA 动了。收拾收拾博客，就有了这篇六年更的年度总结。在这里先祝不离不弃的各位过年好！</p><h2 id="晋升"><a href="#晋升" class="headerlink" title="晋升"></a>晋升</h2><p>今年做了工作八年来第一次晋升答辩，甚至之前都没做过转正答辩，然后一下就干到在这个公司的天花板了，拿下了名义上的技术合伙人。加上来的比较早还可以自称是初创工程师(Founding Engineer)，基本上把纯技术线能拿到的 Title 都收了。</p><p>回想一下这个过程，其实运气占了很大的成分，并不是个常规的流程。由于来的比较早，当时也算国内第一批参与容器的人，吃了不少技术的红利，现在再看看当初一块讨论技术的小伙伴，只要还在坚持做这块的，基本都飞黄腾达了。所谓一个人的命运，当然要靠自我奋斗，但是也要考虑到历史的进程。</p><p>尽管是走的技术线，但是这些年来纯技术的时间可能都不到一半。 反而是借着初创公司机制不完善和各个职位都缺人的机会，从市场、商务、售前、实施、售后的所有工作都接触了。甚至一直不善言谈的我，曾经是这个公司第二个售前，第一波去驻场的研发，以至于我有很长时间都是售前和驻场的面试官。</p><p>我自己来看，单论技术能力和产品能力，拿这个 Title 其实还是有段距离，但是这种非常规的一条龙经历，可以让我跳出常规技术维度的考察。</p><h2 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h2><p>落到具体的工作层面，今年明显碰到了墙。一方面今年正式有了团队要管理，技术相关的工作需要分配到团队其他人，同时其他岗位的人也在不断完善，原来我一个人就可以全包下来的一条线的职能需要不断分解给其他人。这对于之前单打独斗的我其实会很别扭，总感觉事情落不到实处，原来手拿把攥的事突然就变的虚无缥缈了，现在反而比自己之前亲力亲为更焦虑。今年来尝试着尽可能的从一线的动手处理上抽出来，把尽可能多的挑战和机会给到团队里的人，去帮助他们成长。但是过程中沟通，跨团队的协作，进度和质量的把控等方面等碰到了很多问题，收和放的度把握的也不是太好。希望新的一年里能和团队进一步成长，减少过程中的摩擦，能让事情自然而然的发生，也减少一些自己的焦虑。</p><p>另一方面本来今年希望在 Kube-OVN 的国际化上进行一些发力，但是语言上的墙还是结结实实的装上了。在一些 Issue 和 Slack 的异步文字沟通还好，但是当机会进一步发展需要对话的时候，几次线上交流的效果都很差。此外今年也收到了一些海外会议邀请我们去做分享，但同样又有语言能力的问题最终没有成形。最近也在从头看一些英语发音，表达和基础语法的内容，希望新的一年里可以完成一次英语的分享。</p><p>尽管之前我的直接技术工作比例也不是那么高，但是今年几乎就只做辅助相关的技术工作了，不再做直接产出相关的技术工作，又被大量非技术的事情占据精力，一度对技术产生了些迷茫。后来在 Twitter 上偶然发现了 <a href="https://twitter.com/bboreham">Bryan Boreham</a> 这个化石级别的工程师，在我出生前他就开始写代码了。现在还在疯狂的给 Prometheus，Grafana 相关项目写代码，而且随便打开一段代码都透露着优雅和功力。看完老爷子的代码和经历，又激发了我最初对技术单纯的热情，甚至自己头脑风暴出了几个新的项目 idea。最近自己也开始做一些调整了，希望新的一年里能 Get 到一些新的技术，参与一些新的项目。</p><h2 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h2><p>今年基本上把从公司离开后的全部时间都花在了孩子上，那种不计成本付出的感觉和看到孩子对你毫无保留的互动的感觉是无与伦比的。但是对时间和精力的挤占也是显而易见的，再叠加工作的调整和疫情的不确定性，整个一年都有种在失控边缘的感觉。今年下来基本上锻炼，读书和学习的时间都被压缩没了，生活的节奏和之前发生了很大的变化。由于各种失控的情况对情绪也产生了大量的消耗，其实陪伴孩子的效果也不是很好，很多时候都很被动。希望新的一年里能调整好生活的节奏，更好的关心身边的人。另外一个悲伤的事情，就是这个熊孩子现在对足球没兴趣了，我已经不知道该怎么拯救中国足球了，要不我还是改行去当足球教练吧。</p><h2 id="疫情"><a href="#疫情" class="headerlink" title="疫情"></a>疫情</h2><p>今年疫情的形式可谓急转直下，我的感觉是到 21 年底整体的疫情控制都还不错，北京除了长假也不需要核酸。甚至政府还很乐观的同时对房地产，互联网和教培开刀。结果 22 年从西安、上海、郑州到年底的北京，一次次指望大力出奇迹后疫情最终还是失控，社会面上一片动荡，政策也是急转弯。客观来说我们家到今年 11 月中旬才第一次碰到封小区，之前只要记着做核酸日常影响也不大，然后这最后一个半月就发生了仿佛好几年的事情，各种世界观被冲击。我一度想去美团当配送去解决运力不足的问题，结果自己就趴窝了，我趴窝期间运力就爬升回来了。</p><p>我觉得之前的动态清零在实践上存在很多问题，但现在的放开从实践来看显然也不是什么理想的做法。我对之前将近八成的无症状数据很愤怒，这种数据和现实如此离谱的分离不知道谁能解释一下。从理性角度来看现在各种政策的空间其实都不大，指望着回到 19 年前的状态是不现实的，我们必然要和这个病毒长期搏斗了。希望科研的人员不要躺平，在疫苗、检测和特效药方面都能有新的科技突破，能把精力集中在对抗病毒上而不是在对抗人上。</p><h2 id="2023"><a href="#2023" class="headerlink" title="2023"></a>2023</h2><p>无论如何 2022 都已经过去了，希望 2023 年我们都可以保持心中的热爱，去做更好的自己。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;作为六年没有更新博客的僵尸博主，在看了 &lt;a href=&quot;https://twitter.com/gaocegege&quot;&gt;Ce Gao&lt;/a&gt; 连续九年的&lt;a href=&quot;http://gaocegege.com/Blog/%E9%9A%8F%E7%AC%94/newyear</summary>
      
    
    
    
    
    <category term="生活" scheme="http://oilbeater.com/tags/%E7%94%9F%E6%B4%BB/"/>
    
    <category term="工作" scheme="http://oilbeater.com/tags/%E5%B7%A5%E4%BD%9C/"/>
    
    <category term="总结" scheme="http://oilbeater.com/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>聊聊一致性哈希</title>
    <link href="http://oilbeater.com/2016/12/18/consist-hash/"/>
    <id>http://oilbeater.com/2016/12/18/consist-hash/</id>
    <published>2016-12-18T22:03:34.000Z</published>
    <updated>2023-02-19T15:05:26.719Z</updated>
    
    <content type="html"><![CDATA[<p>既然有一致性哈希，就肯定还有不一致哈希，为啥平时没人说不一致哈希呢？因为常见的哈希都是不一致的，所以就不修饰了，到了一致性哈希才特殊加个描述词修饰一下。</p><p>哈希一般都是将一个大数字取模然后分散到不同的桶里，假设我们只有两个桶，有 2、3、4、5 四个数字，那么模 2 分桶的结果就是：</p><p><img src="http://7xl5hp.com1.z0.glb.clouddn.com/unconsist-hash-1.png"></p><p>这时我们嫌桶太少要给哈希表扩容加了一个新桶，这时候所有的数字就需要模 3 来确定分在哪个桶里，结果就变成了：</p><p><img src="http://7xl5hp.com1.z0.glb.clouddn.com/unconsist-hash-2.png"></p><p>可以看到新加了一个桶后所有数字的分布都变了，这就意味着哈希表的每次扩展和收缩都会导致所有条目分布的重新计算，这个特性在某些场景下是不可接受的。比如分布式的存储系统，每个桶就相当于一个机器，文件分布在哪台机器由哈希算法来决定，这个系统想要加一台机器时就需要停下来等所有文件重新分布一次才能对外提供服务，而当一台机器掉线的时候尽管只掉了一部分数据，但所有数据访问路由都会出问题。这样整个服务就无法平滑的扩缩容，成为了有状态的服务。</p><p>要想实现无状态化，就要用到一致性哈希了，一致性哈希中假想我们有很多个桶，先定一个小目标比如 7 个，但一开始真实还是只有两个桶，编号是 3 和 6。哈希算法还是同样的取模，只不过现在分桶分到的很可能是不存在的桶，那么就往下找找到第一个真实存在的桶放进去。这样 2 和 3 都被分到了编号为 3 的桶， 4 和 5 被分到了编号为 6 的桶。</p><p><img src="http://7xl5hp.com1.z0.glb.clouddn.com/consist-hash-1.png"></p><p>这时候再添加一个新的桶，编号是 4，取模方法不变还是模 7：</p><p><img src="http://7xl5hp.com1.z0.glb.clouddn.com/consist-hash-2.png"></p><p>因为 3 号桶里都是取模小于等于 3 的，4 号桶只需要从 6 号桶里拿走属于它的数字就可以了，这种情况下只需要调整一个桶的数字就可分成了重新分布。可以想象下即使有 1 亿个桶，增加减少一个桶也只会影响一个桶的数据分布。</p><p>这样增加一个机器只需要和他后面的机器同步一下数据就可以开始工作了，下线一个机器需要先把他的数据同步到后面一台机器再下线。如果突然掉了一台机器也只会影响这台机器上的数据。实现中可以让每台机器同步一份自己前面机器的数据，这样即使掉线也不会影响这一部分的数据服务。</p><p>这里还有个小问题要是编号为 6 的机桶下线了，它没有后一个桶了，数据该咋办？为了解决这个问题，实现上通常把哈希空间做成环状，这样 3 就成了 6 的下一桶，数据给 3 就好了：</p><p><img src="http://7xl5hp.com1.z0.glb.clouddn.com/consist-hash-3.png"></p><p>用一致性哈希还能实现部分的分布式系统无锁化，每个任务有自己的编号，由于哈希算法的确定性，分到哪个桶也是确定的就不存在争抢，也就不需要分布式锁了。</p><p>既然一致性哈希有这么多好的特性，那为啥主流的哈希都是非一致的呢？主要一个原因在于查找效率上，普通的哈希查询一次哈希计算就可以找到对应的桶了，算法时间复杂度是 O(1)，而一致性哈希需要将排好序的桶组成一个链表，然后一路找下去，k 个桶查询时间复杂度是 O(k)，所以通常情况下的哈希还是用不一致的实现。</p><p>当然 O(k) 的时间复杂度对于哈希来说还是不能忍的，想一下都是O(k) 这个量级了用哈希的意义在哪里？既然是在排好序的桶里查询，很自然的想法就是二分了，能把时间复杂度降到 O(logk)，然而桶的组合需要不断的增减，所以是个链表的实现，二分肯定就不行了，还好可以用跳转表进行一个快速的跳转也能实现 O(logk) 的时间复杂度。</p><p><img src="http://7xl5hp.com1.z0.glb.clouddn.com/finger-table.png"></p><p>在这个跳转表中，每个桶记录距离自己 1，2，4 距离的数字所存的桶，这样不管查询落在哪个节点上，对整个哈希环上任意的查询一次都可以至少跳过一半的查询空间，这样递归下去很快就可以定位到数据是存在哪个桶上。</p><p>当然这写都只是一致性哈希实现方式中的一种，还有很多实现上的变体。比如选择数字放在哪个桶，上面的介绍里是选择顺着数字下去出现的第一个桶，其实也可以选择距离这个数字最近的桶，这样实现和后面的跳转表规则也会有变化。同样跳转表也有多种不同的算法实现，感兴趣的可以去看一下 CAN，Chord，Tapestry，Pastry 这四种 DHT 的实现，有意思的是它们都是 2001 年发出来的 paper，所以 2001 年大概是 P2P 下载的元年吧。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;既然有一致性哈希，就肯定还有不一致哈希，为啥平时没人说不一致哈希呢？因为常见的哈希都是不一致的，所以就不修饰了，到了一致性哈希才特殊加个描述词修饰一下。&lt;/p&gt;
&lt;p&gt;哈希一般都是将一个大数字取模然后分散到不同的桶里，假设我们只有两个桶，有 2、3、4、5 四个数字，那么模</summary>
      
    
    
    
    
    <category term="技术" scheme="http://oilbeater.com/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="哈希" scheme="http://oilbeater.com/tags/%E5%93%88%E5%B8%8C/"/>
    
    <category term="算法" scheme="http://oilbeater.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>《西部世界》与弗洛伊德的精神分析</title>
    <link href="http://oilbeater.com/2016/12/11/westworld-and-frued/"/>
    <id>http://oilbeater.com/2016/12/11/westworld-and-frued/</id>
    <published>2016-12-11T22:03:34.000Z</published>
    <updated>2023-02-19T15:05:26.719Z</updated>
    
    <content type="html"><![CDATA[<p>这部神剧有太多可说的了，恰巧追剧的这段时间在看心理学导论的东西，就当回票友从心理学的角度分析一下。</p><p>第一季中两个创始者为了让 host 获得自己的意识绞尽了脑汁，而在真实世界心理学家对人类意识的探索也存在很久了，最为大众知道名字的心理学家大概就是弗洛伊德了，他开创的精神分析理论也在这部剧中有着种种的体现。</p><p>精神分析理论将人格分为三个层次：本我（id），自我(ego)和超我(supereog)。本我就是原生状态不受任何约束，只想吃喝玩乐贪污腐败，发生不正当关系，自我开心至上的状态，就是本本来来的自己。而超我是有着社会主义核心价值观，全心全意为人民服务，操守道德水准高到天际线的理想状态的自己。然而这两种状态是冲突的，所以真实情况下的自我，是不断在两者的矛盾冲突中调和的状态，今天贪污一下，明天为人民服务一下这个样子。而当本我和超我的矛盾冲突无法在自我这里得到调解的话，心理问题就出现了。</p><p>对于 host 来说，本我是那个按着故事线固定行进的状态，超我就是那个觉醒后能够支配自己的状态，而本我就是处在这两者之间，一会儿觉得我好像想起来了什么，一会儿又回到 loop 中去了。而当他们实在想不透的时候就会出各种各样的故障，如剧中所说几年来越来越多的 host 觉醒了，但大部分都精神分裂被退休了。</p><p>当本我和超我出现冲突时最常用的方法就是压抑一方，对人类来说一般是压抑本我的欲望，而对 host 来说则变成了认为删除超我状态的记忆。被压抑的部分并没有消失，而是成为了潜意识的一部分，这一部分只会在一些特定的场景或者外力的触发下才会被激活，最典型的就是梦的产生。host 在梦境中会不断重复那些被删除掉的记忆，在进入某些特定场景和特殊刺激下会激活被删除的记忆，这一部分和潜意识的理论不谋而合。当实在压抑不住了，冲突变得不可调和，那么就要生病了。</p><p>既然有病就要治，而精神分析理论的治疗方法是不打针不吃药，两张椅子面对面，聊一聊你心中的苦闷，谈一谈你心中的困惑，找到本我和超我之间的冲突，达到你心中的 inner peace。剧中 analyse host 的过程简直是教科书搬的精神分析流派的心理诊断过程，不断的追问被压抑的记忆不断问你心里是怎么想的。而治疗的目的是要让患者自己『顿悟』到自己症状产生的原因，顿悟后进入更高的境界从而获得解脱，所以这种疗法有时候也被称为『顿悟疗法』。再看 Dolores 的觉醒过程，经常和 Arnold，Ford one on one，最后终于顿悟进入觉醒状态。从这个角度讲两个老头都是心理治疗师。</p><p><img src="http://7xl5hp.com1.z0.glb.clouddn.com/vlcsnap-2016-12-11-21h50m59s192.png"></p><p>当然心理学流派还有很多，其实弗洛伊德的精神分析流在当今的科学评价是比较低的，但是没办法啊这个理论天生带自我冲突矛盾写剧本就是好使。如果是采用行为主义这种强调学习和环境刺激的流派那就变成俩老头用经典条件反射训练 host 估计就没人看了。</p><p>最后期待一下下一集觉醒态的 Dolores，和还没完全觉醒但已经是 host 最强态的 Maeve 能正面对手戏一下，然后期待一下亚裔小哥作为硕果仅存的人类能刷点存在感。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这部神剧有太多可说的了，恰巧追剧的这段时间在看心理学导论的东西，就当回票友从心理学的角度分析一下。&lt;/p&gt;
&lt;p&gt;第一季中两个创始者为了让 host 获得自己的意识绞尽了脑汁，而在真实世界心理学家对人类意识的探索也存在很久了，最为大众知道名字的心理学家大概就是弗洛伊德了，他</summary>
      
    
    
    
    
    <category term="随想" scheme="http://oilbeater.com/tags/%E9%9A%8F%E6%83%B3/"/>
    
    <category term="心理学" scheme="http://oilbeater.com/tags/%E5%BF%83%E7%90%86%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>Google 是如何做负载均衡的？</title>
    <link href="http://oilbeater.com/2016/11/21/google-loadbalancert/"/>
    <id>http://oilbeater.com/2016/11/21/google-loadbalancert/</id>
    <published>2016-11-21T00:03:34.000Z</published>
    <updated>2023-02-19T15:05:26.719Z</updated>
    
    <content type="html"><![CDATA[<p>Google 使用的技术一般都自带光环，吸引程序员的注意，基础设施方面的东西就更是如此，年初 Google 发布了篇论文介绍内部的负载均衡器的实现，让我们有机会一睹可能是全球最好的负载均衡器。</p><p>通常情况下的负载均衡要在灵活性和性能之间做权衡，用户态软件层面有 Haproxy 和 Nginx 这样的老牌负载均衡软件，他们一般配置和使用起来都比较容易，但是由于需要数据包从网卡到内核再到软件一层层向上处理，再一层层向下转发，堆栈比较深单机性能通常都比较一般。为了提高单机性能，减少堆栈层级就有了 Linux 里华人之光的 LVS，工作在内核层的负载均衡器，性能有着数量级的提高，然而配置起来相对也比较复杂而且对网络条件要求也有特殊要求。那 Google 有什么秘密的配方来达到高性能呢？</p><p>一般来说负载均衡器本身就是后端服务横向扩展的一个接入点，对于一般站点一个负载均衡器就够了，然而应对 Google 这种级别的流量，负载均衡器本身也要能横向扩展，还要处理负载均衡器的高可用，Google 又是如何做到的呢？</p><p>以我们访问 <a href="http://www.google.com/">www.google.com</a> 为例，第一步 DNS 服务器会根据请求的位置返回一个离请求地理位置最近的 VIP 地址，先在 DNS 这一层做一个横向扩展。接下来请求达到 VIP 对应的路由器，路由器通过 ECMP 协议，可以将请求平均分配到下面对等的多个负载均衡器上，这样在路由器这一层做了个负载均衡，让后面的负载均衡器也实现了横向扩展。再往下是一个类似于 LVS 中 DR 模式的分发，负载均衡器将请求包转发给服务器同时将源地址改为客户请求时的地址，服务器响应时将响应包的源地址改为 VIP 的地址直接打给路由器而不通过负载均衡器来降低负载均衡器压力。流程图如下</p><p><img src="http://7xl5hp.com1.z0.glb.clouddn.com/lb-overview.jpeg"></p><p>对了，Google 的这个负载均衡器叫 Maglev，磁悬浮列车的意思。自然是要做到极致的性能，只看流程似乎和 LVS 中 DR 模式很类似，但内部就完全不一样了。</p><p>简单的说虽然 LVS 已经做到 Linux 内核里了，但是在 Google 看来 Linux 是性能的瓶颈，到 LVS 之前还要经过完整的 TCP&#x2F;IP 协议栈以及内核的一系列 filter 模块，而这些对于转发来说是没有必要的。于是 Google 的做法就是简单粗暴的绕过内核，把 Maglev 直接架在网卡上对接网卡的输入和输出队列，来一个数据包也不需要完整的 TCP&#x2F;IP 协议栈的解析，进来的包只要分析前几个字节，拿出源地址，源端口，目标地址，目标端口和协议号这个五元组对于转发来说就已经足够了。剩下的诸如 payload，序列号之类的东西统统不关心直接塞到网卡输出口给后面就行了。</p><p><img src="http://7xl5hp.com1.z0.glb.clouddn.com/lb-packet.jpeg"></p><p>前几天听美团的介绍他们的负载均衡器是用的类似的思路用 DPDK 直接在网卡上编写应用，性能相对 LVS 就有数倍的提升。看 Maglev 的实现为了榨干性能是直接写在网卡上的，都没有 DPDK 之类的封装。绕过内核就可以自己分配内存管理内存就可以进一步的压榨性能。论文中可以看到 Maglev 是直接和网卡共享内存的，这样就不需要将数据包再从网卡进行一次复制到负载均衡器中，也不需要把数据包再从负载均衡器复制到网卡，网卡入口队列，负载均衡器，网卡出口队列共享一个数据池空间，三个指针不断的移动处理数据包，可谓是在内存这里做到了极致。</p><p><img src="http://7xl5hp.com1.z0.glb.clouddn.com/lb-pool.jpeg"></p><p>当然诸如 CPU 绑定，每个 CPU 专职处理一个线程来避免 cache miss，memory contention 这种常规优化也是都有的。最终的效果就是处理一个数据包平均需要 350ns，而性能发生抖动的情况一般是由于网卡发数据包是批量的或者要等待一个 timer 的中断，而这个中断的时间是 50us 所以当流量小的时候这个延迟可能会达到 50us。（我觉得 Google 这么写其实是在炫耀自己性能好，瓶颈在网卡刷新速度上，而不在负载均衡器上）</p><p>通常的负载均衡器都是一个单点，而 Maglev 是一个集群，集群就会碰到很多的问题。严格来说负载均衡器不能算是一个无状态的服务，因为 TCP 连接本身是有状态的，一组会话内的请求包必须转发到相同的后端服务器，不然服务器端的 TCP 会话就乱套了。对于单点的负载均衡器来说很好解决，记录个转发表里面有每个数据包的五元组和它第一转发到哪台机器，来一个新的数据包查这个表就知道给谁了。而像 Maglev 这样的集群数据包是通过路由器 ECMP 随机分发的，第一个数据包是这个 Maglev node 处理，下一个就不知道去哪个 Maglev node 了。而且集群就会涉及滚动式的更新和随机的故障,这样本机的转发表也就很可能会丢失。</p><p>之前听美团的介绍，他们的做法是在多台机器之间做内存的同步每次更新都要进行一次同步来保证所有机器转发表的一致。而 Google 一帮人不愧是搞研究出身的，直接就上了一致性哈希这个大杀器。这样的话可以直接通过五元组散列到后端的一台固定服务器，这样硬生生的把有状态服务做成了无状态,如此一来 Maglev 层面个就可以随意的更新，上线下线了。顺便的一个好处就是后端增加下线服务器都只会影响到当前这台机器所处理的连接不会造成所有连接的 rehash。当然只用一个普通的一致性哈希算法也没啥意思，Google 为了自己的需求专门写了个 Maglev Hashing</p><p>论文里还介绍了很多负载均衡器运维方面的经验以及设计的过程和经验，还说了下一些新的发展方向，感兴趣的可以看一下<a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44824.pdf">论文原文</a>，当然你要翻下墙（逃</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Google 使用的技术一般都自带光环，吸引程序员的注意，基础设施方面的东西就更是如此，年初 Google 发布了篇论文介绍内部的负载均衡器的实现，让我们有机会一睹可能是全球最好的负载均衡器。&lt;/p&gt;
&lt;p&gt;通常情况下的负载均衡要在灵活性和性能之间做权衡，用户态软件层面有 </summary>
      
    
    
    
    
    <category term="技术" scheme="http://oilbeater.com/tags/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="论文阅读" scheme="http://oilbeater.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    <category term="Google" scheme="http://oilbeater.com/tags/Google/"/>
    
    <category term="LB" scheme="http://oilbeater.com/tags/LB/"/>
    
    <category term="哈希" scheme="http://oilbeater.com/tags/%E5%93%88%E5%B8%8C/"/>
    
    <category term="算法" scheme="http://oilbeater.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>破译上帝的密码</title>
    <link href="http://oilbeater.com/2016/10/30/decode-god-code/"/>
    <id>http://oilbeater.com/2016/10/30/decode-god-code/</id>
    <published>2016-10-30T21:03:34.000Z</published>
    <updated>2023-02-19T15:05:26.719Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xl5hp.com1.z0.glb.clouddn.com/dna-decode.jpg"></p><p>上一篇<a href="http://oilbeater.com/%E5%8D%9A%E5%AE%A2/2016/10/23/god-is-a-coder.html">《上帝是个程序员》</a>介绍了上帝为了防止人肉智能破解对基因的种种加密，然而这并不能阻止人肉智能破解的步伐，我们居然也取得了一些进展。</p><p>我们先定一个小目标，比如实现星际旅行，最大的问题可能还不在于航天飞机，而是在于人的寿命。离我们最近的半人马座有 4 光年的距离，就算飞船达到了十分之一光速一个来回也是 80 年，而这个速度也已经是现阶段最快飞行器速度的将近两千倍了。</p><p>所以星际旅行不能只靠航天飞船了，还要靠生物学家。看过《三体》的同学都知道，星际旅行冬眠下就可以了嘛，而这居然是有科学依据的。一些虫子正常的寿命只有 20 多天，但是当气候条件发生恶劣变化时它们会自动进入一种假死状态，停止一切的进食活动和新陈代谢，这种状态可以维持四五个月，等气候适宜的时候再恢复过来。这样从时间跨度上讲虫子的生命被延长廊五至六倍，对于人来说这就是四五百年的时间，五百年前那还是明王朝呢。虫子的基因量相对于人来说要少很多，培养起来也比较简单，我们用暴力破解的手段一段一段的敲除掉基因，看看什么时候虫子丧失冬眠的能力，也有机会找到控制冬眠的基因。当然从虫子到人还有很漫长的道路，不过科学家已经在做很多类似的实验，包括灵长类和小鼠身上都已经可以诱发冬眠来延长寿命的现象。</p><p>当然冬眠的时候人也失去了行动能力，这样延长寿命好像也挺无聊的，除了星际旅行冬眠还有另一个用途。就像《三体》里罗辑大魔王被破壁者用定向基因病毒攻击，当时的医疗水平不足以救治，为了让大魔王活下来就进行冬眠，等医疗技术成熟再弄醒。那么我们的医疗水平在基因技术的加持下又会有怎样的发展呢。</p><p>乔布斯老爷子在那场著名的永远愤怒，永远挨饿的斯坦福毕业演讲中提到他的胰腺癌可以治好是因为那是少数的可治愈的胰腺癌，当然最后老爷子还是没有治好。不过胰腺癌作为癌中的王者一般从发现后都活不过两年，而乔老爷子还是挺了八年。从自传中可以看到，这背后有一个专业化的团队来服务。一般疾病的治疗方案可以说都是广谱的，并不会考虑个体和癌细胞的特点，这样就会造成同样的治疗方案有的人效果特别好，有的人就没效果。而癌症更特殊一些，它的基因突变的更加频繁，很有可能上个阶段管用的方案，下个阶段就无效了。想要对症下药，就必须时时刻刻分析病人当前的基因特性以及癌细胞的基因特性，再根据这些特性制作相应的药物和治疗方案，这样针对性强而且副作用小。乔老爷子背后的团队就是干这个的，所以老爷子才能又活跃这么久。而且看过自传的也知道，老乔经常幻想着用他的精神扭曲力来消灭癌症，一会儿素食，一会儿不洗澡什么的各种不配合治疗，如果不是这么作的话，罗玉龙应该也不会这么早就进入手机行业造锤子。</p><p>当然现阶段这种方案的花销太大了，不是首富级别的就别想了。但是未来随着这些病例样本的积累，以及基因测序成本下降，这些技术很可能进入民用。之后感冒发烧就不再是给广谱的抗生素类药物，而是针对病毒进行测序分析再给出对应的治疗药品。一些大规模的疫情，像 SARS 也会第一时间发现，并能准确的判断，而不是像当时碰到一个发烧的就跟要死人一样隔离起来。这些技术现在在美国有类似 Google 和 Human Longevity 这样的商业公司在研究，一旦有商业公司介入，相信这个领域离成熟已经不太远了。</p><p>只研究如何治病并不能突破人类寿命的上限，一些微生物的寿命只有不到一天，而一些树木能存活上千年，生物的衰老都是编码在基因里的。如果我们能够找到这一部分编码并进行控制，那么长生不老是不是也可以实现呢？所以现在要好好锻炼身体，保持健康，万一让我们赶上了好时代能去做个环银河系旅行呢？</p><p>P.S. 感兴趣的可以搜一下《星际远航中的生物学》这个演讲，看一下生物学已经多么匪夷所思了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;http://7xl5hp.com1.z0.glb.clouddn.com/dna-decode.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;上一篇&lt;a href=&quot;http://oilbeater.com/%E5%8D%9A%E5%AE%A2/2016/10/23/go</summary>
      
    
    
    
    
    <category term="随想" scheme="http://oilbeater.com/tags/%E9%9A%8F%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>做科研一样做产品</title>
    <link href="http://oilbeater.com/2016/10/30/code-as-expirement/"/>
    <id>http://oilbeater.com/2016/10/30/code-as-expirement/</id>
    <published>2016-10-30T21:03:34.000Z</published>
    <updated>2023-02-19T15:05:26.719Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xl5hp.com1.z0.glb.clouddn.com/blind.jpg"></p><p>题图是一个双盲试验。</p><p>计算机最早作为一门科学自然要用严谨的科学方法来做实验得出结论。而当计算机进入工业界，敏捷高速的迭代取代了严谨的科学步骤，但是回过头来我们又能从科学研究的步骤中吸取到哪些经验来改善我们在工业界的产品质量呢？</p><p>一个典型的研究过程首先要寻找问题，然后提出针对问题的一种假设，为了验证假设的正确就要设计实验了，之后需要实施实验并收集数据，根据数据之后就要做结论看假设是不是正确的，如果假设恰好是正确的，那么恭喜就可以发论文接收同行评审了，论文最后都要装模作样的写一下 future work，尽管大多数情况 future work 都是做不了的 work，但万一有能 work 的就可以形成个闭环继续研究下去了。</p><p>做产品也是类似的过程，先要发现一个生活中的问题，没有问题也要编一个出来好去忽悠。然后就要想这个是不是云计算能解决啊，大数据能解决啊人工智能能解决啊，这样可以再忽悠一票。接下来就需要具体的设计代码实现了。做出来了自然要进行测试或者招一批早期用户来内测，收集一下用户的反馈和使用体验，来验证最早的解决方案并不是忽悠人。再之后根据用户的数据和反馈就可以发现下一步的问题和 future work，就可以再去忽悠来形成一个闭环（逃</p><p>所以说尽管形式不一样但是做科研和做产品还是有相通的地方。科研中的设计实验和之后的分析数据中由很多有意思的方法对开发测试的过程都很有启发。</p><p>科研实验中很重要的一点就是一定要有对照组，给一组人喝一种特殊的药水，给另一组人喝等量的白开水。由于被实验者是人，当人知道自己被实验观察室的各种反应会和平时不一样更倾向于表现出实验所希望的状态。而实施实验的观察者心理也会起变化更倾向于接受他们所希望的实验结果。所以一般的实验都会设计双盲，被试者不知道自己到底是实验组还是对照组也不知道实验目的，观察统计结果的人也不知道这个人是哪一组，这样可以尽可能的保证实验过程和统计数据的真实性和客观性。</p><p>这种策略在很多互联网的产品中都已经应用到了，像 FaceBook 会有线上的 A&#x2F;B test，一套功能的两种方案线上 pk 看谁的效果好，或者判断一个新的方案到底要不要上线给所有用户。这时候用户是不知道自己是实验组还是对照组的，甚至都不知道自己参加了试验，统计数据大多也是机器通过标准的流程生成，尽量减少人的主观干预，也算是实现了双盲。</p><p>双盲的根本原因在于人都是有情绪变化的，无法在实验状态下保证客观，难免会在表现和结论中掺杂个人情感的因素。这些问题存在于所有人和人之间的交互上，那么别的场景下是不是也可以用双盲的策略呢，比如 code review 的时候写代码的人不知道谁会 review ，reviewer 不知道谁写的代码，代码的统一性会不会更好？开发测试的流程中，开发不知道代码会给谁测，测试不知道是谁写的代码会不会使代码质量更好？是不是还有别的场景也可以尝试双盲呢？</p><p>在实验状态下保证客观，难免会在表现和结论中掺杂个人情感的因素。这些问题存在于所有人和人之间的交互上，那么别的场景下是不是也可以用双盲的策略呢，比如 code review 的时候写代码的人不知道谁会 review ，reviewer 不知道谁写的代码，代码的统一性会不会更好？开发测试的流程中，开发不知道代码会给谁测，测试不知道是谁写的代码会不会使代码质量更好？是不是还有别的场景也可以尝试双盲呢？</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;http://7xl5hp.com1.z0.glb.clouddn.com/blind.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;题图是一个双盲试验。&lt;/p&gt;
&lt;p&gt;计算机最早作为一门科学自然要用严谨的科学方法来做实验得出结论。而当计算机进入工业界，敏捷高速的迭代取代了</summary>
      
    
    
    
    
    <category term="随想" scheme="http://oilbeater.com/tags/%E9%9A%8F%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>上帝是个程序员</title>
    <link href="http://oilbeater.com/2016/10/23/god-is-a-coder/"/>
    <id>http://oilbeater.com/2016/10/23/god-is-a-coder/</id>
    <published>2016-10-23T21:03:34.000Z</published>
    <updated>2023-02-19T15:05:26.719Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xl5hp.com1.z0.glb.clouddn.com/dna.png"></p><p>上帝是个程序员，那他用什么编程语言呢?尘世间的程序员在潮流的忽悠下，各种语言都要做人工智能，上帝老人家用 DNA 写出了人这么个产品，顺便实现了目前为止最高级的人肉智能。</p><p>作为人肉智能的我们，总想着反编译自己这个生成品，试图破解生命的缘由。经过层层实验我们发现人的各种性状都是由基因控制的，人的生老病死都脱离不了基因。基因就像一份程序代码，而每个生命就像是这份代码运行时的一个进程，有着运行时的种种变化和各种可能，但都无法脱离基因的束缚。那些编码在基因里的身体缺陷就仿佛代码里的 bug，而那些异于常人的天赋又像是那些令人叹为观止的代码片段。</p><p>人肉智能一旦发现自己的能力都编码在这些 DNA 片段中，自然想要了解学习并为己所用。而上帝老人家作为久经考验的程序员，这些代码混淆、反破解、故意的代码质量劣化这些技术都玩的溜溜的，即使是俄国人也搞不定。首先人家编译出来的人肉码就不是二进制的，是由 A、T、C、G 四对碱基组成的四进制码，先在维度上就对人肉智能进行了一次高维碾压。一般的反编译好歹还知道我运行在什么样的 CPU 上，知道这个 CPU 对应的指令集，这样就可以从编译结果往下走，至少能暴力翻译成机器指令，虽说不一定能看懂，但程序入口函数入口一些功能的代码大致还是能分出来的。然而你知道人肉这个 CPU 的指令集么?这就彻底堵死了往下走的道路，只能通过上层的性状和基因进行关联来翻译了。</p><p>好在我们发现了基因是先通过生成氨基酸，组合成蛋白质来决定生命的表现，貌似给我们提供了一个指令集合的方向，先换成中间语言再看看。然而经过实验发现基因和蛋白并不是简单的指令集映射关系，确实存在基因和氨基酸对应的片段，但是这只是一少部分，大量的基因片段并不直接和氨基酸对应，但他们的功能还不是专门的混淆代码，他们似乎控制着蛋白质该怎么折叠变成什么形状，控制着别的基因段要不要翻译。就仿佛那些直接参与生成的只是程序代码里的数据段，这只是一小部分大部分控制数据的算法我们还是没有找到对应的关系，只能再次进行猜测和实验。尽管每个细胞内包含的基因物质基本一致，但随着人类的生长，细胞的分化每个细胞所表达的性状也天差地别，我们甚至不能确定这段基因到底是真正没用，还是这个细胞并没有执行到这个代码路径。极其怀疑上帝同志出于表达能力的要求选择的是一门上下文相关文法的语言写的人肉智能，顺便把破解难度提了好几个量级。</p><p>当然好程序员不能只想着反破解，也不能只完成一个人肉智能的功能就完事，还得保证系统的稳定性，不能出个问题就要去线上调试，一铲子把电缆挖断了也能缓过来，被 DDOS 了也不能打的种族灭绝。所以就有了一条 DNA 链条里存在着多份冗余还能相互纠错，每个链条都是个双链结构做到了双活，同一份基因片段可能存在多个不同的链条上做双活的异地多活，多个染色体上的基因片段又有共同完成一件事情。多个染色体就这样组成了个混合 Raid 0，1，3，6 的超级稳定的存储系统。而且这个系统在每个细胞内都有一个备份，这样缺个胳膊少个腿还不至于就挂了做了双活的异地多活的异地多活。同一份基因片段又存在在无数的人身上，这样就完成了双活的异地多活的异地多活的异地多活。什么两地三中心，全球分布式，十一个九的可靠性在这个系统面前都弱爆了。</p><p>只要是程序就会有 bug，只要是程序员就会被产品经理改需求，上帝作为高级程序员也很头疼这件事。于是他决定不写完美的代码，而是让代码有随机出错的概率，这样有可能正好错对了，或者错的正好满足产品经理的需求了。要是错的不满足的就直接死掉好了，这样错的靠谱的就活下来了。当然对外不能这么说，显得很没技术含量的样子，就包装一下叫进化算法，这样自己不用干活了，还显得高大上了，说我的程序可以在你调研用户需求前就满足用户需求了，就再也不怕产品经理了。</p><p>上帝实乃顶尖级程序员，等我们人肉智能破解了这些代码后，能做些什么呢?实际上我们的进展还是挺快的，现在科学家们已经做出很多匪夷所思的事情了，以至于我越来越相信『二十一世纪是生物的时代』这个说法很可能是真的，等我收集一下资料下次再谈谈。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;http://7xl5hp.com1.z0.glb.clouddn.com/dna.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;上帝是个程序员，那他用什么编程语言呢?尘世间的程序员在潮流的忽悠下，各种语言都要做人工智能，上帝老人家用 DNA 写出了人这么个产品，顺便实现了</summary>
      
    
    
    
    
    <category term="随想" scheme="http://oilbeater.com/tags/%E9%9A%8F%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>最惊心动魄的上线是怎样的?</title>
    <link href="http://oilbeater.com/2016/10/07/most-difficult-release/"/>
    <id>http://oilbeater.com/2016/10/07/most-difficult-release/</id>
    <published>2016-10-07T21:03:34.000Z</published>
    <updated>2023-02-19T15:05:26.719Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xl5hp.com1.z0.glb.clouddn.com/spacex.jpg"></p><p>题图为 SpaceX CRS-1 火箭发射，就是发出去又被回收的那个。</p><p>初入阿里时，听前辈讲某年双十一开始前几分钟需要紧急上线（要知道为了保证双十一的稳定基本上国庆时就不允许做发布了），这时候以振飞为首的技术保障部老大们围着一个女 PE，盯着上线。上线完成后一切正常，那个 PE 就昏过去了。</p><p>从技术角度讲，这次上线可能技术难度并不大，操作步骤可能和平时也类似，但压力之大绝对是难以想象的。甚至那个时候技术都是次要的了，能抗住压力抖着手把熟悉的步骤完成才是最重要的。据说完成上线的那个 PE 后来被表彰了。</p><p>这两天看《硅谷钢铁侠-埃隆马斯克的冒险人生》，又发现了他所创建的两个公司中发生的两起惊心动魄的上线。</p><p>一个是特斯拉刚出 Model S 时，经常会出现些小问题把手不能弹出，雨刷速率异常等，这种问题通常都是需要送修的，但是特斯拉的做法是趁车主睡觉的时候通过网络连接到问题车辆进行软件更新。车主第一天还在抱怨有问题，睡一觉起来发现没了，就是这么神奇。但是这种做法其实是风险很大的，可以对比一下之前阿里云误删文件的风波，安全的 agent 升级把用户文件删除了，那还只是个机器，这可是辆可以载人的车啊，里面万一有人怎么办？不知道特斯拉的工程师在更新的时候会不会手抖？不过看书上的介绍他们似乎很乐于这种事情，一开始还是修复 bug，后来牵引力控制功能、更快的充电速度、智能语音也通过这种方式开始推送了，用户似乎也很喜欢这种车不断进化的魔法。这种做法恐怕是传统汽车企业想都不敢想的事把？</p><p>另一个是 SpaceX 的一次发射，给空间站运送补给。由于意外的强光干扰，导致激光探测器无法正确识别距离，工程师折腾了两个半小时飞船依然无法和空间站对接。尽管 SpaceX<br>以低价闻名，但一次发射依然需要千万美元，而且当时这家初创公司的资金也不多，每次失败都是极其致命的损失，而且这是 SpaceX<br>第一次执行空间站对接任务，会关系到之后的订单。情急之下，工程师决定向飞船上传新的软件，减少视觉传感器使用的帧数，以此来消除太阳光对机器的影响。前面两个案例我觉得还在理解之中，这次在线升级飞船就完全超乎想象了。我都想不出来这种东西该怎么测试，难道飞船也是可以找个流量低峰期改一下试试不行就再改一下试试这么玩么？最终结果当然是皆大欢喜，飞船和空间站成功对接，SpaceX 也得到了 NASA 4.4 亿美元的拨款来设计载人的飞船。</p><p>特斯拉和 SpaceX 这种线上调试，偷偷更新的风格很像不断试错，小步迭代的互联网风格。一般的传统企业说到互联网思维大多是做个网站把产品放到网上，然后在网上做 marketing 和 sale，而这两家企业是把互联网的方法论融合到产品的设计，发布，上线和售后。想想汽车和飞船有个厂商可以上传更新的后门，有就罢了还真在用，是一种怎样的体验？或许这种互联网的方法论真的可以带我们驶向星辰大海。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;http://7xl5hp.com1.z0.glb.clouddn.com/spacex.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;题图为 SpaceX CRS-1 火箭发射，就是发出去又被回收的那个。&lt;/p&gt;
&lt;p&gt;初入阿里时，听前辈讲某年双十一开始前几分钟需要紧急上</summary>
      
    
    
    
    
    <category term="随想" scheme="http://oilbeater.com/tags/%E9%9A%8F%E6%83%B3/"/>
    
    <category term="工作" scheme="http://oilbeater.com/tags/%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
</feed>
