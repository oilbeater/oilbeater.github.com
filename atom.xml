<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Oilbeater 的自习室</title>
  <icon>http://oilbeater.com/icon.png</icon>
  
  <link href="http://oilbeater.com/atom.xml" rel="self"/>
  
  <link href="http://oilbeater.com/"/>
  <updated>2025-08-11T02:24:41.737Z</updated>
  <id>http://oilbeater.com/</id>
  
  <author>
    <name>Oilbeater</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>现代化你的 Go 代码</title>
    <link href="http://oilbeater.com/2025/08/08/go-modernize/"/>
    <id>http://oilbeater.com/2025/08/08/go-modernize/</id>
    <published>2025-08-08T17:10:00.000Z</published>
    <updated>2025-08-11T02:24:41.737Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>[!note] TL; DR<br><code>go run golang.org/x/tools/gopls/internal/analysis/modernize/cmd/modernize@latest -test --fix ./...</code></p><ol><li>自动使用现代化语法简化 Go 代码。</li><li>目前无法和 golangci-lint 集成，只能这么用。</li><li>新的 <code>slices</code> 和 <code>maps</code> 标准库包含大量通过泛型和迭代器的实现，可以简化之前代码。</li></ol></blockquote><p><a href="https://github.com/golang/tools/tree/master/gopls/internal/analysis/modernize/testdata/src">https://github.com/golang/tools/tree/master/gopls/internal/analysis/modernize/testdata/src</a></p><h2 id="efaceany-replace-interface-by-the-‘any’-type-added-in-go1-18"><a href="#efaceany-replace-interface-by-the-‘any’-type-added-in-go1-18" class="headerlink" title="efaceany: replace interface{} by the ‘any’ type added in go1.18."></a>efaceany: replace interface{} by the ‘any’ type added in go1.18.</h2><table><thead><tr><th>Old</th><th>New</th></tr></thead><tbody><tr><td><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> x <span class="keyword">interface</span>&#123;&#125;</span><br><span class="line"><span class="string">``</span><span class="string">``</span></span><br><span class="line"></span><br><span class="line">&lt;/td&gt;&lt;td&gt;</span><br><span class="line"></span><br><span class="line"><span class="string">``</span><span class="string">`go</span></span><br><span class="line"><span class="string">var x any</span></span><br></pre></td></tr></table></figure></td></tr></tbody></table><h2 id="stringsseq-replace-Split-in-“for-range-strings-Split-…-”-by-go1-24’s-more-efficient-SplitSeq-or-Fields-with-FieldSeq"><a href="#stringsseq-replace-Split-in-“for-range-strings-Split-…-”-by-go1-24’s-more-efficient-SplitSeq-or-Fields-with-FieldSeq" class="headerlink" title="stringsseq: replace Split in “for range strings.Split(…)” by go1.24’s more efficient SplitSeq, or Fields with FieldSeq."></a>stringsseq: replace Split in “for range strings.Split(…)” by go1.24’s more efficient SplitSeq, or Fields with FieldSeq.</h2><table><thead><tr><th>Old</th><th>New</th></tr></thead><tbody><tr><td><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _, line := <span class="keyword">range</span> strings.Split(<span class="string">&quot;a,b,c&quot;</span>, <span class="string">&quot;,&quot;</span>) &#123;</span><br><span class="line"><span class="built_in">println</span>(line)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, line := <span class="keyword">range</span> strings.Fields(<span class="string">&quot;a b c&quot;</span>) &#123;</span><br><span class="line"><span class="built_in">println</span>(line)</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">``</span><span class="string">``</span></span><br><span class="line"></span><br><span class="line">&lt;/td&gt;&lt;td&gt;</span><br><span class="line"></span><br><span class="line"><span class="string">``</span><span class="string">`go</span></span><br><span class="line"><span class="string">for line := range strings.SplitSeq(&quot;a,b,c&quot;, &quot;,&quot;) &#123;</span></span><br><span class="line"><span class="string">println(line)</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">for line := range strings.FieldsSeq(&quot;a b c&quot;) &#123; </span></span><br><span class="line"><span class="string">println(line)</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure></td></tr></tbody></table><h2 id="mapsloop-replace-a-loop-around-an-m-k-v-map-update-by-a-call-to-one-of-the-Collect-Copy-Clone-or-Insert-functions-from-the-maps-package-added-in-go1-21"><a href="#mapsloop-replace-a-loop-around-an-m-k-v-map-update-by-a-call-to-one-of-the-Collect-Copy-Clone-or-Insert-functions-from-the-maps-package-added-in-go1-21" class="headerlink" title="mapsloop: replace a loop around an m[k]&#x3D;v map update by a call to one of the Collect, Copy, Clone, or Insert functions from the maps package, added in go1.21."></a>mapsloop: replace a loop around an m[k]&#x3D;v map update by a call to one of the Collect, Copy, Clone, or Insert functions from the maps package, added in go1.21.</h2><table><thead><tr><th>Old</th><th>New</th></tr></thead><tbody><tr><td><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> key, value := <span class="keyword">range</span> src &#123;</span><br><span class="line">dst[key] = value</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">``</span><span class="string">``</span></span><br><span class="line"></span><br><span class="line">&lt;/td&gt;&lt;td&gt;</span><br><span class="line"></span><br><span class="line"><span class="string">``</span><span class="string">`go</span></span><br><span class="line"><span class="string">dst := maps.Clone(src)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">maps.Copy(dst, src)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">maps.Insert(dst, src)</span></span><br></pre></td></tr></table></figure></td></tr></tbody></table><h2 id="minmax-replace-an-if-else-conditional-assignment-by-a-call-to-the-built-in-min-or-max-functions-added-in-go1-21"><a href="#minmax-replace-an-if-else-conditional-assignment-by-a-call-to-the-built-in-min-or-max-functions-added-in-go1-21" class="headerlink" title="minmax: replace an if&#x2F;else conditional assignment by a call to the built-in min or max functions added in go1.21."></a>minmax: replace an if&#x2F;else conditional assignment by a call to the built-in min or max functions added in go1.21.</h2><table><thead><tr><th>Old</th><th>New</th></tr></thead><tbody><tr><td><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x := a </span><br><span class="line"><span class="keyword">if</span> a &lt; b &#123; </span><br><span class="line">x = b</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">``</span><span class="string">``</span></span><br><span class="line"></span><br><span class="line">&lt;/td&gt;&lt;td&gt;</span><br><span class="line"></span><br><span class="line"><span class="string">``</span><span class="string">`go</span></span><br><span class="line"><span class="string"> x := max(a, b)</span></span><br></pre></td></tr></table></figure></td></tr></tbody></table><h2 id="rangeint-replace-a-3-clause-“for-i-0-i-n-i-”-loop-by-“for-i-range-n”-added-in-go1-22"><a href="#rangeint-replace-a-3-clause-“for-i-0-i-n-i-”-loop-by-“for-i-range-n”-added-in-go1-22" class="headerlink" title="rangeint: replace a 3-clause “for i :&#x3D; 0; i &lt; n; i++” loop by “for i :&#x3D; range n”, added in go1.22."></a>rangeint: replace a 3-clause “for i :&#x3D; 0; i &lt; n; i++” loop by “for i :&#x3D; range n”, added in go1.22.</h2><table><thead><tr><th>Old</th><th>New</th></tr></thead><tbody><tr><td><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">10</span>; i++ &#123; </span><br><span class="line"><span class="built_in">println</span>(i)</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">``</span><span class="string">``</span></span><br><span class="line"></span><br><span class="line">&lt;/td&gt;&lt;td&gt;</span><br><span class="line"></span><br><span class="line"><span class="string">``</span><span class="string">`go</span></span><br><span class="line"><span class="string">for i := range 10 &#123; </span></span><br><span class="line"><span class="string">println(i)</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure></td></tr></tbody></table><h2 id="slicescontains-replace-‘for-i-elem-range-s-if-elem-needle-…-break-’-by-a-call-to-slices-Contains-added-in-go1-21"><a href="#slicescontains-replace-‘for-i-elem-range-s-if-elem-needle-…-break-’-by-a-call-to-slices-Contains-added-in-go1-21" class="headerlink" title="slicescontains: replace ‘for i, elem :&#x3D; range s { if elem &#x3D;&#x3D; needle { …; break }’ by a call to slices.Contains, added in go1.21."></a>slicescontains: replace ‘for i, elem :&#x3D; range s { if elem &#x3D;&#x3D; needle { …; break }’ by a call to slices.Contains, added in go1.21.</h2><table><thead><tr><th>Old</th><th>New</th></tr></thead><tbody><tr><td><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> slice &#123;</span><br><span class="line"><span class="keyword">if</span> slice[i] == needle &#123;</span><br><span class="line"><span class="built_in">println</span>(<span class="string">&quot;found&quot;</span>)</span><br><span class="line"> <span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">``</span><span class="string">``</span></span><br><span class="line"></span><br><span class="line">&lt;/td&gt;&lt;td&gt;</span><br><span class="line"></span><br><span class="line"><span class="string">``</span><span class="string">`go</span></span><br><span class="line"><span class="string">if slices.Contains(slice, needle) &#123;</span></span><br><span class="line"><span class="string">println(&quot;found&quot;)</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure></td></tr></tbody></table><h2 id="slicesdelete-replace-append-s-i-s-i-1-…-by-slices-Delete-s-i-i-1-added-in-go1-21"><a href="#slicesdelete-replace-append-s-i-s-i-1-…-by-slices-Delete-s-i-i-1-added-in-go1-21" class="headerlink" title="slicesdelete: replace append(s[:i], s[i+1]…) by slices.Delete(s, i, i+1), added in go1.21."></a>slicesdelete: replace append(s[:i], s[i+1]…) by slices.Delete(s, i, i+1), added in go1.21.</h2><table><thead><tr><th>Old</th><th>New</th></tr></thead><tbody><tr><td><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test = <span class="built_in">append</span>(test[:i], test[i+<span class="number">1</span>:]...)</span><br><span class="line"><span class="string">``</span><span class="string">``</span></span><br><span class="line"></span><br><span class="line">&lt;/td&gt;&lt;td&gt;</span><br><span class="line"></span><br><span class="line"><span class="string">``</span><span class="string">`go</span></span><br><span class="line"><span class="string">test = slices.Delete(test, i, i+1)</span></span><br></pre></td></tr></table></figure></td></tr></tbody></table><h2 id="sortslice-replace-sort-Slice-s-func-i-j-int-bool-return-s-i-s-j-by-a-call-to-slices-Sort-s-added-in-go1-21"><a href="#sortslice-replace-sort-Slice-s-func-i-j-int-bool-return-s-i-s-j-by-a-call-to-slices-Sort-s-added-in-go1-21" class="headerlink" title="sortslice: replace sort.Slice(s, func(i, j int) bool { return s[i] &lt; s[j] }) by a call to slices.Sort(s), added in go1.21."></a>sortslice: replace sort.Slice(s, func(i, j int) bool { return s[i] &lt; s[j] }) by a call to slices.Sort(s), added in go1.21.</h2><table><thead><tr><th>Old</th><th>New</th></tr></thead><tbody><tr><td><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sort.Slice(s, <span class="function"><span class="keyword">func</span><span class="params">(i, j <span class="type">int</span>)</span></span> <span class="type">bool</span> &#123; <span class="keyword">return</span> s[i] &lt; s[j] &#125;)</span><br><span class="line"><span class="string">``</span><span class="string">``</span></span><br><span class="line"></span><br><span class="line">&lt;/td&gt;&lt;td&gt;</span><br><span class="line"></span><br><span class="line"><span class="string">``</span><span class="string">`go</span></span><br><span class="line"><span class="string">slices.Sort(s)</span></span><br></pre></td></tr></table></figure></td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;[!note] TL; DR&lt;br&gt;&lt;code&gt;go run golang.org/x/tools/gopls/internal/analysis/modernize/cmd/modernize@latest -test --fix ./...&lt;/</summary>
      
    
    
    
    
    <category term="golang" scheme="http://oilbeater.com/tags/golang/"/>
    
    <category term="lint" scheme="http://oilbeater.com/tags/lint/"/>
    
  </entry>
  
  <entry>
    <title>GoBGP 快速上手</title>
    <link href="http://oilbeater.com/2025/07/08/gobgp-guide/"/>
    <id>http://oilbeater.com/2025/07/08/gobgp-guide/</id>
    <published>2025-07-08T22:48:00.000Z</published>
    <updated>2025-08-11T02:24:41.737Z</updated>
    
    <content type="html"><![CDATA[<p>在做 MetalLB 和 Calico 的一些测试时需要验证 BGP 相关的功能，然而每次测试要去联系运维团队配合网络变更是不太现实的，并且大多数情况我们只要验证 BGP 信息正常交换就可以了，不需要真的去改变物理网络。于是就找到了 <a href="https://github.com/osrg/gobgp">GoBGP</a> 这个软件路由器进行模拟。</p><p>GoBGP 本身有着很复杂的配置，但是如果你只是像我一样只是想有个虚拟的路由器，让客户端把 BGP 信息发送过来，检查路由表信息是否正确更新，那看这篇文章就可以了。</p><h2 id="下载-GoBGP-二进制文件"><a href="#下载-GoBGP-二进制文件" class="headerlink" title="下载 GoBGP 二进制文件"></a>下载 GoBGP 二进制文件</h2><p>去 <a href="https://github.com/osrg/gobgp/releases">Release</a> 制品列表里找到对应系统的制品解压即可，重要的只有两个二进制文件： <code>gobgpd</code> 虚拟路由器进程，<code>gobgp</code>命令行工具，用来查看路由对不对。</p><h2 id="启动虚拟路由器"><a href="#启动虚拟路由器" class="headerlink" title="启动虚拟路由器"></a>启动虚拟路由器</h2><p>创建一个 <code>gobgp.toml</code> 文件，最简单的配置就照着我下面这个就好了，大部分基础的云原生领域 BGP 相关的软件测试用这个就够了。</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[global.config]</span></span><br><span class="line">  <span class="attr">as</span> = <span class="number">65000</span>                          <span class="comment"># 测试环境随便写一个就好</span></span><br><span class="line">  <span class="attr">router-id</span> = <span class="string">&quot;10.0.0.1&quot;</span>              <span class="comment"># 测试环境随便写一个就好，写 GoBGP 所在节点 IP 日志清晰一些</span></span><br><span class="line"></span><br><span class="line"><span class="section">[[dynamic-neighbors]]</span>                 <span class="comment"># 被动连接模式，省去了配固定客户端</span></span><br><span class="line">  <span class="section">[dynamic-neighbors.config]</span></span><br><span class="line">    <span class="attr">prefix</span>     = <span class="string">&quot;192.168.0.0/24&quot;</span>     <span class="comment"># 允许哪个 IP 范围内的客户端来连接 </span></span><br><span class="line">    <span class="attr">peer-group</span> = <span class="string">&quot;ext-ebgp&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="section">[[peer-groups]]</span>                       <span class="comment"># 复制粘贴就好了</span></span><br><span class="line">  <span class="section">[peer-groups.config]</span></span><br><span class="line">    <span class="attr">peer-group-name</span> = <span class="string">&quot;ext-ebgp&quot;</span></span><br><span class="line">    <span class="attr">peer-as</span>         = <span class="number">65000</span></span><br><span class="line">  <span class="section">[[peer-groups.afi-safis]]</span></span><br><span class="line">    <span class="section">[peer-groups.afi-safis.config]</span></span><br><span class="line">      <span class="attr">afi-safi-name</span> = <span class="string">&quot;ipv4-unicast&quot;</span></span><br></pre></td></tr></table></figure><p>启动 <code>gobgpd</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./gobgpd -f gobgp.toml</span><br></pre></td></tr></table></figure><p>在另一个终端观察当前的路由表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./gobgp global rib</span><br></pre></td></tr></table></figure><p>这样基本上 MetalLB，Calico 的基础 BGP 能力就可以测试了。如果想更配更复杂的模式，比如 <a href="https://github.com/osrg/gobgp/blob/master/docs/sources/route-reflector.md">Router Reflector</a>，<a href="https://github.com/osrg/gobgp/blob/master/docs/sources/evpn.md">EVPN</a> 那就再去翻 <a href="https://github.com/osrg/gobgp?tab=readme-ov-file#documentation">GoBGP 的文档</a>吧。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在做 MetalLB 和 Calico 的一些测试时需要验证 BGP 相关的功能，然而每次测试要去联系运维团队配合网络变更是不太现实的，并且大多数情况我们只要验证 BGP 信息正常交换就可以了，不需要真的去改变物理网络。于是就找到了 &lt;a href=&quot;https://git</summary>
      
    
    
    
    
    <category term="networking" scheme="http://oilbeater.com/tags/networking/"/>
    
    <category term="gobgp" scheme="http://oilbeater.com/tags/gobgp/"/>
    
    <category term="tutorial" scheme="http://oilbeater.com/tags/tutorial/"/>
    
  </entry>
  
  <entry>
    <title>怎样的开源文档规范最合适？</title>
    <link href="http://oilbeater.com/2025/07/06/oss-docs-best-practise/"/>
    <id>http://oilbeater.com/2025/07/06/oss-docs-best-practise/</id>
    <published>2025-07-06T13:00:10.000Z</published>
    <updated>2025-08-11T02:24:41.737Z</updated>
    
    <content type="html"><![CDATA[<p>最近打算把 Kube-OVN 的文档整理一遍，同时思考怎样的文档规范能尽可能做到：</p><ol><li>降低 Maintainer 的维护成本（用户看完文档后不会再去找 Maintainer ）。</li><li>能够充分复用（不看文档的人找过来可以直接甩文档）。</li><li>同时对社区的用户也尽可能有帮助（少走弯路）。</li></ol><p>我决定先按照下面的结构重构一下文档，来看看效果。</p><ol><li><strong>概要</strong>：一两句话描述这个功能达到什么效果，解决什么问题，有什么优势。之前很多文档都是以技术细节开头，反而把真正能实现的效果给模糊掉了。明确功能是什么后，用户就可以确认自己是否还需要继续往下看了。</li><li><strong>局限性</strong>：这个功能不是什么，什么没做到，解决不了什么问题，使用有什么限制条件。这个其实和第一部分一样重要，不然用户可能有不切实际的预期等到很靠后的时候才知道某个功能其实是不符合预期的或者限制条件不满足，也浪费了双方很多时间。有人问为什么不能用的时候也可以甩文档。</li><li><strong>实现原理</strong>：我们用到的 OVN&#x2F;OVS 的哪些能力，简要的介绍以及对应功能的参考文档。到这里我们其实期望用户有一些更深的理解，不至于出了问题毫无思路再去扒代码或者找到 Maintainer。我们也建议真在生产使用 Kube-OVN 的用户要有一些从底层开始排查的能力，Kube-OVN 虽然做了很多抽象让使用变得简单，但是在用户的环境里使用的异常还是需要用户自己有能力排查。</li><li><strong>使用步骤</strong>：之前的常规内容，有了前面的铺垫，这一部分可以尽可能简化，只需要步骤即可。</li></ol><p>大家有什么其他的建议也可以一块来讨论一下。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近打算把 Kube-OVN 的文档整理一遍，同时思考怎样的文档规范能尽可能做到：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;降低 Maintainer 的维护成本（用户看完文档后不会再去找 Maintainer ）。&lt;/li&gt;
&lt;li&gt;能够充分复用（不看文档的人找过来可以直接甩文档）。&lt;</summary>
      
    
    
    
    
    <category term="opensource" scheme="http://oilbeater.com/tags/opensource/"/>
    
    <category term="documents" scheme="http://oilbeater.com/tags/documents/"/>
    
  </entry>
  
  <entry>
    <title>Kube-OVN 是如何自动修复 CVE 的</title>
    <link href="http://oilbeater.com/2025/06/28/kube-ovn-autoupdate/"/>
    <id>http://oilbeater.com/2025/06/28/kube-ovn-autoupdate/</id>
    <published>2025-06-28T00:02:21.000Z</published>
    <updated>2025-08-11T02:24:41.737Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这篇博客是内部分享的一个提示文稿，并没有做仔细整理，大体思路就是捕获所有 Kube-OVN 的依赖：操作系统，Golang，程序依赖，环境组件（k8s,kubevirt 等），然后激进的实时更新所有依赖做到动态清零。</p></blockquote><p>Kube-OVN CVEs 问题修复的流程经历了如下几个阶段：</p><ul><li>发版&#x2F;按需统一进行手动修复</li><li>每次 commit 检测可修复安全漏洞，手动进行修复</li><li>master 依赖自动更新，发版分支部分依赖自动更新，少量手动修复</li><li>未来目标：所有依赖自动更新，避免手动修复</li></ul><h2 id="按需修复"><a href="#按需修复" class="headerlink" title="按需修复"></a>按需修复</h2><p>优势：</p><ul><li>相比每个安全漏洞单独修复，整体修复频次低<br>劣势：</li><li>发版集中修复，研发还要兼顾发版期间赶进度，测试，bug 修复，时间压力大</li><li>依赖更新没有经过日常验证，存在未知的风险</li><li>大部分安全修复工作没有实际意义还需要花费人工精力</li></ul><h2 id="每次-Commit-检测修复"><a href="#每次-Commit-检测修复" class="headerlink" title="每次 Commit 检测修复"></a>每次 Commit 检测修复</h2><p>增加工作：</p><ul><li>流水线增加 trivy 扫描，存在安全问题无法合并代码</li><li><a href="https://github.com/kubeovn/kube-ovn/blob/master/.github/workflows/build-x86-image.yaml#L3437">https://github.com/kubeovn/kube-ovn/blob/master/.github/workflows/build-x86-image.yaml#L3437</a></li><li><a href="https://github.com/kubeovn/kube-ovn/actions/runs/15760235247?pr=5376">https://github.com/kubeovn/kube-ovn/actions/runs/15760235247?pr=5376</a></li></ul><p>优势：</p><ul><li>将 CVE 修复打散到平时，发版时时间压力较低</li><li>可以快速发版</li><li>大部分依赖更新已经得到了日常自动化测试的验证，风险也较低<br>劣势：</li><li>最后一次提交和发版扫描之间存在时间差，理论上会遗漏一部分 CVE</li><li>会干扰平时正常功能提交，bug 修复，提交经常因为不相关的 CVE 问题被阻塞</li><li>大量的手动修复</li></ul><h2 id="所有依赖自动更新"><a href="#所有依赖自动更新" class="headerlink" title="所有依赖自动更新"></a>所有依赖自动更新</h2><p>只要依赖更新版本就尝试更新，不考虑是否是安全更新。尝试解决比 CVE 修复更大的一个问题，自动就解决了 CVE 问题的修复。</p><p>我们的依赖项：</p><ul><li>OS 镜像及其依赖：<code>ubuntu:24.04</code>, <code>apt-get install ....</code></li><li>Go 语言版本，以及代码依赖库</li><li>上游依赖：<code>OVS</code>, <code>OVN</code></li><li>其他协作组件依赖： <code>kubernetes</code>, <code>kubevirt</code>,<code>multus</code>, <code>metallb</code>, <code>cilium</code>, <code>talos</code></li><li>采用比较激进的更新策略，依赖大版本更新我们也会尝试更新</li></ul><p>要做的事情：</p><ul><li>OS 镜像部分增加流水线每天重新构建，自动修复 OS 和 apt 库里解决的问题<ul><li><a href="https://github.com/kubeovn/kube-ovn/blob/master/.github/workflows/build-kube-ovn-base.yaml">https://github.com/kubeovn/kube-ovn/blob/master/.github/workflows/build-kube-ovn-base.yaml</a></li><li>每日自动无需人工干预</li></ul></li><li>Go 相关使用 renovate 进行自动更新<ul><li>Go 版本，<code>go.mod</code> 里的所有依赖版本</li><li>实时更新 + auto merge</li><li>出现合并问题手动处理</li><li><a href="https://github.com/kubeovn/kube-ovn/pull/5354">https://github.com/kubeovn/kube-ovn/pull/5354</a></li><li><a href="https://github.com/kubeovn/kube-ovn/blob/master/renovate.json">https://github.com/kubeovn/kube-ovn/blob/master/renovate.json</a></li><li>不需要特殊配置，按照 renovate 自动检测流程来即可</li></ul></li><li>上游组件依赖<ul><li>OVS OVN 每日构建，策略更激进直接从上游分支构建，上游不发版我们也会更新</li><li>相信上游都是 bugfix，我们这样可以增强稳定性</li></ul></li><li>其他组件<ul><li>使用 renovate 的自定义正则匹配，Dockerfile, Makefile, Action 里所有依赖软件版本自动更新</li><li><a href="https://github.com/kubeovn/kube-ovn/issues/5291">https://github.com/kubeovn/kube-ovn/issues/5291</a></li><li><a href="https://github.com/kubeovn/kube-ovn/blob/master/Makefile">https://github.com/kubeovn/kube-ovn/blob/master/Makefile</a></li></ul></li></ul><p>优点：</p><ul><li>大部分 CVE 会在不知道情况下被修复，少量可在一天内自动修复，特殊情况再手动修复</li><li>大量上游的 bugfix，性能提升和新功能被自动合入，软件整体稳定性提升</li><li>大量的版本适配验证工作，如 k8s 版本更新，KubeVirt 版本更新的适配验证也都自动进行，风险提前知晓</li><li>人工干预量较少</li></ul><p>劣势：</p><ul><li>依赖更新多比较吵，需要设置聚合策略</li><li>更新量太大无法人工测试，需要有自动化测试保证</li><li>需要积极适配上游版本变化</li><li>存在上游新版本不稳定风险，目前两年内遇到过两次</li></ul><h2 id="renovate-相比-dependabot-优势"><a href="#renovate-相比-dependabot-优势" class="headerlink" title="renovate 相比 dependabot 优势"></a>renovate 相比 dependabot 优势</h2><ul><li>可以自定义依赖捕获，Dockerfile、Makefile 里的非标准依赖可以捕获</li><li>可以在非 master 分支运行</li><li>有 auto merge 能力</li><li></li></ul><h2 id="还未解决的问题"><a href="#还未解决的问题" class="headerlink" title="还未解决的问题"></a>还未解决的问题</h2><ul><li>自动化测试误报导致无法自动合并</li><li>已发版分支的 Go 相关更新还未自动化<ul><li>renovate 的 security-only 策略效果还未知</li><li><a href="https://github.com/kubeovn/kube-ovn/blob/master/renovate.json#L45">https://github.com/kubeovn/kube-ovn/blob/master/renovate.json#L45</a></li><li>发版分支可能需要重新定义策略</li></ul></li><li>部分依赖还没有自动更新</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;这篇博客是内部分享的一个提示文稿，并没有做仔细整理，大体思路就是捕获所有 Kube-OVN 的依赖：操作系统，Golang，程序依赖，环境组件（k8s,kubevirt 等），然后激进的实时更新所有依赖做到动态清零。&lt;/p&gt;
&lt;/blockquot</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>关于培训</title>
    <link href="http://oilbeater.com/2025/05/24/about-training/"/>
    <id>http://oilbeater.com/2025/05/24/about-training/</id>
    <published>2025-05-24T10:40:04.000Z</published>
    <updated>2025-08-11T02:24:41.737Z</updated>
    
    <content type="html"><![CDATA[<p>这两天接了个 Kube-OVN 培训的任务，事实上这已经是这几年第三次给这家公司培训相同的内容了，形式也从线上转到线下还要求手把手的敲命令，可想而知之前几次的最终效果怎样。我可以理解需求方的技术焦虑，但我认为培训这种形式是极其低效的，更多的只是培训当时能获得一些虚假的满足感，不会产生什么长期收益。</p><p>一方面培训是一种被动学习方式，完全由老师带节奏前进，那听讲的人只是被动的吸收，很难有深度的思考。另一方面需要等到培训才了解的知识大概率是实际上用不到的知识，如果每天都用的话根本来不及等这种一年一次的培训。如果实际用不到，那么哪怕当场掌握了，很快也会什么都剩不下。</p><p>我觉得这和我们一直以来的教育理念有些关系，传统上我们认为认真听老师讲课，努力记笔记，不会的问题堵着老师问，都是优秀的学习习惯。但这些都是偏被动的学习方法，我们很多年的教育过程中其实缺乏主动学习的机会。这种被动的方式早期的效果可能会很好，因为早期学校里的知识难度不大，而且我们展示学习成果的方式就是考试，而考试是可以通过过拟合的方式来提升效果的，并不能真正反映你对知识的掌握程度。</p><p>在我的经历里，到了初中阶段，最出色的那批人就不再是认真听课，努力记笔记那批人了。而是换成了那批给人感觉平时一直玩，考试成绩还很好的一批人，事实上越往后这批人的比例越高。我认为这里并不是智商的区别而是被动学习和主动学习模式的区别。主动学习的人会更多依赖自己思考来解决问题，而不是依赖老师的讲解，这个方法看似低效其实最终效果会更好。想象一下同样一道题，一个人是老师之前讲过所以考试的时候做出来了，另一个人是上课没听到但是考试的时候临场也做出来了。从考试的结果来说两个人是一样的，但两个人的实际能力是天差地别的。后者实际上拥有更好的学习方法和能力，并且在更难的问题上有更好的泛化能力，自然可以显得游刃有余。就好比前者一直在用冒泡算法去排序，后者则是不断用快排，在小规模上前者可能更简单更不容易出错，效果会比后者好，但是后者一旦调整好了，在更大规模的数据上会有指数级别的优势。</p><p>这也是为什么有些转行的程序员的表现会比科班的表现好。一方面他们采用的是主动学习这种高速方法，另一方面科班的被动学习学到的大量都是实际中毫无用处的知识，两者的基础差距本身就没那么大，用一个更高速的方法很容易短期就实现超越。</p><p>所以还是要主动学习，学点有用的。（不要再折腾供应商搞什么高级培训了</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这两天接了个 Kube-OVN 培训的任务，事实上这已经是这几年第三次给这家公司培训相同的内容了，形式也从线上转到线下还要求手把手的敲命令，可想而知之前几次的最终效果怎样。我可以理解需求方的技术焦虑，但我认为培训这种形式是极其低效的，更多的只是培训当时能获得一些虚假的满足感</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>如何超越 OpenShift</title>
    <link href="http://oilbeater.com/2025/04/23/surpass-openshift/"/>
    <id>http://oilbeater.com/2025/04/23/surpass-openshift/</id>
    <published>2025-04-23T10:40:43.000Z</published>
    <updated>2025-08-11T02:24:41.737Z</updated>
    
    <content type="html"><![CDATA[<p>OpenShift 作为容器平台的标杆产品，同时也是开源商业化的标杆，一直是被人试图追赶或者超越的对象。但是如果只是照着 OpenShift 的产品模仿，那么当追上 OpenShift 时只可能有一种情况，那就是 OpenShift 停止发展，过了一年后你终于追上了，然后会因同样原因被淘汰。</p><p>那么有没有什么方法能够追上甚至超越 OpenShift 呢？我认为要从 OpenShift 本身商业模式的选择和技术路线上的选择入手，从他们在这种选择下不可避免的缺点入手，做出差异化，才有超越的可能。</p><h2 id="OpenShift-并不-Open"><a href="#OpenShift-并不-Open" class="headerlink" title="OpenShift 并不 Open"></a>OpenShift 并不 Open</h2><p>OpenShift 里的 Open 可能和 OpenAI 里的 Open 是同一个 Open。如果你尝试不通过 RedHat 的销售自己去部署一套 OpenShift 就会知道我在说什么了。</p><p>OpenShift 的所有组件确实是开源的，但如果你是一个纯粹的社区用户会步步受挫。一个功能你可能找不到对应组件，找到组件可能找不到对应源码，找到源码又没有文档指导如何编译使用。这些现象在那些完全是 OpenShift 自己使用的组件里已经见怪不怪了。大概 OpenShift 这部分的社区只是对客户和合作伙伴开放的。</p><p>而对于那些非 OpenShift 专属的组件，OpenShift 采取的策略会是一旦选择就大举投入，争取到对应组件的社区的主导权。所以会看到的一个现象是有些社区很出名的项目 OpenShift 的人完全不投入，而一个项目在平稳发展了几年后会突然涌入大量 OpenShift 的人。</p><p>这些都是 OpenShift 在商业化与开源之间权衡后的选择，并没有对错之分，但这会给更开放的项目留出机会。如果新的产品能够降低参与门槛，收取更广泛的反馈，让更多的贡献者来参与创新，那么我认为它的上限将会超越 OpenShift。</p><h2 id="OpenShift-的技术并不先进"><a href="#OpenShift-的技术并不先进" class="headerlink" title="OpenShift 的技术并不先进"></a>OpenShift 的技术并不先进</h2><p>受限于第一点因素，OpenShift 并不能广泛的吸收整个生态的最新成果。在生态内某个组件和 OpenShift 专属的组件功能重叠情况下，OpenShift 内部的研发人员是很难有动力切换到另一个社区或者另一个公司主导的开源组件。</p><p>以我比较了解的网络为例，OpenShift 早期通过 Haproxy 实现了 Route 来打通集群外访问集群内的流量。在当时 Ingress 还不成熟，Route 是一个相比社区先进的多的方案，OpenShift 的方案在当时是绝对领先的。但是随着 Ingress 的成熟，社区生态内各种网关都在飞速发展，而 OpenShift 受限于自己早期的实现和用户用法很长时间都没有去支持 Ingress 这个在社区已经标准化的功能。现阶段 Ingress 规范已经进化到 GatewayAPI 有一段时间了，大量 AI Gateway 的新场景都在通过 GatewayAPI 进行扩展，而 OpenShift 现如今还没有支持 GatewayAPI，最近正在计划在之前的 Haproxy 上同时支持 Route，Ingress 和 GatewayAPI。</p><p>类似的案例在 OpenShift 内还有很多，早期可能还是一个优秀的方案，但由于 OpenShift 专属导致不开放，随着社区的发展，原先优势的方案反而变成了阻碍前进的障碍。就像现在在 Kubernetes 上做 Ingress Gateway 不会有人去参考 OpenShift 的实现，在很多细分领域 OpenShift 已经并不是最先进的解决方案了，尤其是在那些由 OpenShift 专属组件提供服务的领域。现在的 OpenShift 在我看来就是一个覆盖面积很广，但平庸且无趣的平台。</p><p>容器平台的生产过程其实和手机的生产很像，都是从成百上千个供应链供应商那里选择需要的配件，然后组装成一个成品。如果能够保持开放，选择供应链上最先进的那些配件，或者根据场景快速组合出一个针对特定场景的产品，那么在技术竞争力上应该会远超 OpenShift。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>要想真正追赶甚至超越 OpenShift，关键不是在已有功能上亦步亦趋，而是要做到更开放、更领先。这样才能摆脱追随者路径，真正形成对 OpenShift 的差异化优势，成为下一轮技术浪潮的主导者。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;OpenShift 作为容器平台的标杆产品，同时也是开源商业化的标杆，一直是被人试图追赶或者超越的对象。但是如果只是照着 OpenShift 的产品模仿，那么当追上 OpenShift 时只可能有一种情况，那就是 OpenShift 停止发展，过了一年后你终于追上了，然后会</summary>
      
    
    
    
    
    <category term="OpenShift" scheme="http://oilbeater.com/tags/OpenShift/"/>
    
    <category term="Product" scheme="http://oilbeater.com/tags/Product/"/>
    
  </entry>
  
  <entry>
    <title>DeepSeek MLA -- 为成本优化而生的注意力机制</title>
    <link href="http://oilbeater.com/2025/04/14/deepseek-mla/"/>
    <id>http://oilbeater.com/2025/04/14/deepseek-mla/</id>
    <published>2025-04-14T17:10:03.000Z</published>
    <updated>2025-08-11T02:24:41.736Z</updated>
    
    <content type="html"><![CDATA[<p>DeepSeek 第一次出名是因为 DeepSeek V2 做到了一百万 Token 只要 0.14 美元。同期的 GPT-4 是 30 美元，当时被认为极具性价比的 GPT-3.5 也要 1.5 美元。这个突破性价格的出现在国内引发了一轮价格战，大批大厂模型大幅降价甚至免费。然而和其他大厂烧钱补贴的逻辑不同，DeepSeek 是通过一系列的技术创新实现了成本数量级的下降。这篇文章就来介绍一下这背后最关键的一个技术创新 —— MLA(Multi-Head Latent Attention)。</p><p>MLA 最本质的数学技巧并不复杂，论文里也是一段话就说完了，看完让人感叹竟然还有如此精妙的解法。但是由于和整个 Transformer 架构耦合会导致理解起来有些困难，我这里会尽量简化描述，哪怕你之前完全不了解 Transformer 应该也可以领会到这个方法的精妙。</p><p>当然还是需要有一些线性代数的基础，如果你还记的一个形状为 5*4 的矩阵乘形状为 4*3 的矩阵，结果是一个形状为 5 * 3 的矩阵就可以继续了。</p><p><img src="/../images/20250414233740.png"></p><h2 id="KVCache"><a href="#KVCache" class="headerlink" title="KVCache"></a>KVCache</h2><p>大模型推理的成本的瓶颈在哪里？答案可能会出乎意料 —— 是显存。显卡有大量计算单元，而推理任务又是线性的一次只能出一个 Token，为了充分利用显卡的计算资源达到最大的吞吐，一般会同时运行尽可能多的生成任务。而每个任务在推理过程中都会占用大量显存，如果想运行尽可能多的任务就需要运行时显存占用足够小。而 MLA 在运行时的显存占用是原始注意力机制的 <strong>6.7%</strong>，你没看错，不是降低了 6.7%，是降低了 <strong>93.3%</strong>。打个比喻这一刀下去不是腰斩，而是脚踝斩。在不考虑模型本身的显存占用情况下，近似可以认为 MLA 在相同显存下可以容纳 <strong>15 倍</strong>的生成任务。</p><p>虽然 MLA 的论文里没有细说这个灵感的来源，但我认为他们是从原先的 KVCache 倒推，诞生了一种全新的注意力机制。到这里就不得不说下 KVCache 是什么。</p><p>大模型每生成一个 Token 都需要对之前所有的 Token 进行计算，来得出最新的一个 Token 是什么。但是一般任务不会生成一个 Token 就结束，往往要一直生成到结束符号，这就会导致前面的 Token 每一次都要重复计算。于是 KVCache 的作用就是把之前每个 Token 计算生成的中间结果保存下来，这样就可以避免重复计算了。你可以理解为每个 Token 都被映射成了一个 1000 * 1000 的矩阵，那么我们有没有什么办法来减少这个矩阵的内存占用呢？</p><p><img src="/../images/20250414234534.png"></p><h2 id="MLA"><a href="#MLA" class="headerlink" title="MLA"></a>MLA</h2><p>这里有意思的事情终于可以开始了，我们可以用两个小矩阵相乘来近似一个大矩阵。这里刚才你还记得的线性代数知识可以用上了，1000 * 2 的矩阵乘一个 2 * 1000 的矩阵也可以得到一个 1000 * 1000 的矩阵，而这两个矩阵总共只有 4000 个元素，是 1000 * 1000 矩阵元素数量的 0.4%。</p><p>这就是 MLA 在数学上最核心的思路了，在 DeepSeek V2 中，本来一个 Token 应该被映射为一个 1*16k 的向量，而在使用 MLA 后会先通过一个压缩矩阵将这个 Token 映射为 1*512 的向量，等到需要的时候再通过一个 512 * 16k 的解压矩阵还原成 1*16k 的向量。在这里压缩矩阵和解压矩阵都是通过训练得来，是模型的一部分只会占用固定的显存，而运行时针对每个 Token 的显存占用就只剩这个  1*512 的向量，只有原来的 3%。</p><p>一个完整的对比如下图所示，原先的 MHA 需要 Cache 完整矩阵，而 MLA 只需要 Cache 中间压缩后的一个向量，再还原出完整矩阵。</p><p><img src="/../images/20250415000024.png"><br><img src="/../images/20250414235538.png"></p><p>这一切真的这么美好嘛？让我们想想 KVCache 的最初目的是什么，是为了减少 Token 的重复中间计算，MLA 虽然压缩了 KVCache，但是每次还需要一个解压操作，计算量又回来了。</p><p>这里就是一个更精彩的故事了，按照 Transformer 的计算，中间的 Cache 乘一个解压矩阵后还要再乘一个输出矩阵得到最终的结果，可以粗略理解为最终计算的公式是 Cache * W<sup>解压</sup>  * W<sup>输出</sup> ，根据矩阵计算的结合率，可以先计算后两个矩阵的乘积，将后两个矩阵融合乘一个新的矩阵。由于 W<sup>解压</sup>  和 W<sup>输出</sup> 在训练后是确定的，因此做个简单的后处理把这部分提前算出来就好了。作者在论文中也用 <strong>Fortunately</strong> 来形容这件事情。</p><p>也就是说我们最初是出于压缩 KVCache 的思路去做了压缩和解压，但在实际推理过程中根本不存在解压的过程。在大幅压缩了显存的同时由于过程中的矩阵都变小了，推理所需的计算量也变小了。</p><h2 id="模型能力"><a href="#模型能力" class="headerlink" title="模型能力"></a>模型能力</h2><p>但在这里我其实还有个疑问没有解开，本质上 MLA 是用两个小矩阵相乘得到一个大矩阵，但是并不是所有的大矩阵都能完美分解成两个小矩阵。MLA 实际的搜索空间是小于 MHA 的，理论上来讲 MLA 的模型能力应该更弱。但是按照 DeepSeek 论文里的评测，MLA 的模型能力是要略强于 MHA 的。</p><p><img src="/../images/20250415003909.png"></p><p>这个事情其实就不太好理解了，我倾向于认为 MLA 虽然搜索空间降低了，但是最优解的概率反而变大了，收敛到了一个相比 MHA 更优的解。另外虽然 MLA 的优化是从 MHA 出发，但最终的结果其实是一套全新的注意力机制，模型的架构都发生了很大的变化，或许 DeepSeek 真的发现了一个更有效的注意力机制。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>有不少性能优化方案其实是在玩跷跷板游戏，比如用 GPU 计算时间交换显存空间，或者牺牲模型能力来换成本下降。而 MLA 在把显存打脚踝斩的情况下同时还做到了计算需求下降和模型能力提升，简直匪夷所思。</p><p>另一个感触是在经历了国内移动互联网时代的我们很容易认为价格战就是要赔本赚吆喝，却忘了技术创新才应该是那个最大的杠杆。</p><blockquote><p>这篇博客只是介绍了 MLA 最核心的理念，在实际应用中还有很多具体的问题，例如：如何处理旋转位置编码？K 和 V 的解压矩阵融合其实略有不同，一个是直接应用结合律，一个是转置后再结合，等等。还是建议大家阅读 DeepSeek V2 的原始论文，有这篇文章做基础应该容易理解很多。</p><p>博客里部分图片源自 <a href="https://www.bilibili.com/video/BV1BYXRYWEMj/">DeepSeek-v2 MLA 原理讲解</a>,也建议大家看下这个视频。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;DeepSeek 第一次出名是因为 DeepSeek V2 做到了一百万 Token 只要 0.14 美元。同期的 GPT-4 是 30 美元，当时被认为极具性价比的 GPT-3.5 也要 1.5 美元。这个突破性价格的出现在国内引发了一轮价格战，大批大厂模型大幅降价甚至免</summary>
      
    
    
    
    
    <category term="DeepSeek" scheme="http://oilbeater.com/tags/DeepSeek/"/>
    
    <category term="LLM" scheme="http://oilbeater.com/tags/LLM/"/>
    
    <category term="Paper" scheme="http://oilbeater.com/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>混乱的 Llama 4</title>
    <link href="http://oilbeater.com/2025/04/06/llama-4-chaos/"/>
    <id>http://oilbeater.com/2025/04/06/llama-4-chaos/</id>
    <published>2025-04-06T15:57:23.000Z</published>
    <updated>2025-08-11T02:24:41.736Z</updated>
    
    <content type="html"><![CDATA[<p>前段时间 Meta AI 的负责人离职让人怀疑 Llama 4 的进度出现了问题，结果没过两天 Meta 就发布了 Llama 4，似乎是为了打破传言。然而看完了现在已经公布的模型基础信息，我反而更觉得 Llama 项目内部已经极度混乱了。下面是我根据已有信息的分析，欢迎指正。</p><h2 id="模型基础信息"><a href="#模型基础信息" class="headerlink" title="模型基础信息"></a>模型基础信息</h2><p>Llama 这次正式发布了一个 109B 和 一个 400B 参数量的模型，以及一个未发布的 2T 参数量模型的信息，我把关键的架构信息汇总如下，以下信息均来自 Llama 自己的博客和 huggingface 模型页面：</p><table><thead><tr><th>名称</th><th>参数量</th><th>激活参数量</th><th>专家数</th><th>上下文</th><th>语料量</th><th>GPU 时间</th></tr></thead><tbody><tr><td>Scout</td><td>109B</td><td>17B</td><td>16</td><td>10M</td><td>40T</td><td>5.0M</td></tr><tr><td>Maverick</td><td>400B</td><td>17B</td><td>128+1</td><td>1M</td><td>22T</td><td>2.38M</td></tr><tr><td>Behemoth</td><td>2T</td><td>288B</td><td>16</td><td>-</td><td>-</td><td>-</td></tr></tbody></table><p>这里都不用再看模型的评分表现了，这几个模型架构方面的对比就能看出很多问题了。</p><h2 id="奇怪的-MoE-架构"><a href="#奇怪的-MoE-架构" class="headerlink" title="奇怪的 MoE 架构"></a>奇怪的 MoE 架构</h2><p>Llama 4 这次从 Dense 模型全面转向了 MoE，但是诡异的点在于他们三个模型采用了两套 MoE 架构。最大的 Behemoth 和最小的 Scout 采用的是传统的 MoE，专家数也是 16 这个传统认为比较常规的一个专家数量，而中间的那个 Maverick 采用的却是 DeepSeek MoE 提出的一个新的细粒度专家加共享专家的模型是个 128 专家加 1 共享专家的架构。</p><p>一般来说一代模型都是采用同一个架构，只是在模型的层数和每层的宽度上做调整，两个有很大差异的模型在同一代就很奇怪。而且就算有变化也不应该是最大和最小的保持一致，把中间规模的给换了，给人的感觉是中间的这个 Maverick 其实是被 DeepSeek 冲击下重新仿照 DeepSeek 模型重新训练的，但是时间上来不及把三个都重做，只好就放在一块发布了。</p><h2 id="奇怪的成本投入"><a href="#奇怪的成本投入" class="headerlink" title="奇怪的成本投入"></a>奇怪的成本投入</h2><p>一般来讲，模型参数规模越大，需要投入的成本越高。一方面是更大的模型可以容纳更多的知识，会提供给更大规模模型更多的语料；另一方在语料相同的情况下，更大的模型需要训练的参数更多 GPU 开销也会更高。所以通常来讲模型规模越大，需要的成本会越高。</p><p>然而到了 Llama 4 这里出现了两个指标都相反的情况。Maverick 的参数规模是 Scout 的接近 4 倍，但是 Maverick 训练的语料量只有 Scout 的二分之一，消耗的 GPU 时间同样也只有二分之一。考虑到这两个模型的激活参数量是一致的这个 GPU 时间可以理解，但是语料量也只给一半这个事情就很奇怪了。给我的感觉是要么这次只是试水新型的 MoE 架构，并没有想做完整训练，要么就是训到后面训崩了，从中间那个 snapshot 出来了。</p><h2 id="奇怪的上下文长度"><a href="#奇怪的上下文长度" class="headerlink" title="奇怪的上下文长度"></a>奇怪的上下文长度</h2><p>一般来讲更大的模型，能力会越强。可在这一代 Llama，最让人感到震撼的 10M 上下文是给的最小规模的 Scout，更大的 Maverick 反而是 1M 上下文。考虑到目前扩充上下文的主流方法还是在后训练做微调，更大的 Maverick 在后训练的投入上还不如更小的 Scout。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>给我的感觉是 Llama 4 这一代本来是想走传统 MoE，被 DeepSeek 冲击后又半路开始看 DeepSeek MoE。但是训练可能已经开始了，停下来又有阻力，所以中间又插了一个中规模的 Maverick。按照这个参数量选择来看是想用比 DeepSeek V3 小的参数量实现类似的性能。但是 17B 的激活要追平 DeepSeek V3 的 39B 激活我觉得还是有很大难度的。不过最后能让这一代的模型以这么混乱的形式发布，还加了个期货模型，我还是觉得 Llama 项目内部出了不少的问题。</p><p><img src="/../images/llama4.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;前段时间 Meta AI 的负责人离职让人怀疑 Llama 4 的进度出现了问题，结果没过两天 Meta 就发布了 Llama 4，似乎是为了打破传言。然而看完了现在已经公布的模型基础信息，我反而更觉得 Llama 项目内部已经极度混乱了。下面是我根据已有信息的分析，欢迎指</summary>
      
    
    
    
    
    <category term="LLM" scheme="http://oilbeater.com/tags/LLM/"/>
    
    <category term="Llama" scheme="http://oilbeater.com/tags/Llama/"/>
    
  </entry>
  
  <entry>
    <title>DeepSeek MoE -- 创新型的 MoE 架构</title>
    <link href="http://oilbeater.com/2025/03/29/deepseek-moe/"/>
    <id>http://oilbeater.com/2025/03/29/deepseek-moe/</id>
    <published>2025-03-29T12:54:37.000Z</published>
    <updated>2025-08-11T02:24:41.736Z</updated>
    
    <content type="html"><![CDATA[<p>从 DeepSeek V3&#x2F;R1 开始关注 DeepSeek 工作的人很容易认为 DeepSeek 大量的工作都是在工程上优化效率，但是回看 DeepSeek 过去一年的论文才会发现他们其实一直在模型架构和训练方法上做各种创新，而 V3 和 R1 只是在之前架构创新的基础上进行 Scale。DeepSeek MoE 这篇论文就介绍了 DeepSeek 在 MoE 架构上的主要创新，现在看上去也很有希望成为未来 MoE 架构的标准。</p><h2 id="MoE-vs-Dense"><a href="#MoE-vs-Dense" class="headerlink" title="MoE vs Dense"></a>MoE vs Dense</h2><p>先说一下 MoE 和传统的 Dense 架构的区别。早期的 LLM 基本都是 Dense 架构，也就是每生成一个 Token 需要激活所有的神经元参与计算，这种方式其实和人脑的思考方式是有很大区别的，人脑是不会任何问题都需要调动所有脑细胞的，如果这样的话人早就累死了。所以很自然的一个想法就是生成 Token 的时候不要再激活所有的神经元了，每次只激活和当前任务最相关的神经元，于是就有了 MoE(Mixture of Experts) 架构，把 LLM 里每一层的神经元分割成 N 个 Expert，通过 Router 去选择 K 个最相关的 Expert 激活。</p><p><img src="/../images/20250329161016.png"></p><p>这个架构的好处就是在推理的时候不需要激活所有的神经元，计算量会大幅下降。在 DeepSeek MoE 前最常见的 8 选 2 模式下计算量可以下降到接近 Dense 模型的三分之一。</p><p>MoE 的架构看上去很理想，但本质上是在用少量 Experts 来模拟 Dense 模型的表现，所以关键是在每个 Expert 是否有足够的专业性，能否真的模拟 Dense 模型的表现。如果类比人脑，当神经元足够特化时，特定任务只需要激活少量神经元即可完成。</p><p>DeepSeek MoE 这篇论文就介绍了他们为了把每个 Expert 专业性推到极致所做的两个创新：</p><ul><li>更多更小的 Expert</li><li>知识共享 Expert</li></ul><h2 id="更多更小的-Expert"><a href="#更多更小的-Expert" class="headerlink" title="更多更小的 Expert"></a>更多更小的 Expert</h2><p><img src="/../images/20250329165838.png"></p><p>使用更多更小的 Expert 来增加每个 Expert 的专业性看似是个很符合直观的思路，但是之前主流 MoE 都是 8 个或者 16 个 Expert。可以想象 LLM 要处理的问题类型千千万，这个数量规模的 Expert 显然不可能做到高度的专业化，每个 Expert 都会有大量当前任务无关的知识。</p><p>但是随着 Expert 的数量变大，训练的难度也会变大，Router 很容易只选择少数几个 Expert 导致负载的极度不均衡。最终，理论上的 MoE 架构可能会变成每次只激活同一组 Expert 的小模型。因此，之前大部分 MoE 架构的 Expert 数量都不会太多。</p><p>DeepSeek 经过一组设计的损失函数，给重复选择同一个 Expert 增加了惩罚，从而迫使 Router 更均衡的去选择 Expert。通过这个方式 DeepSeek 解决了训练的问题，开始一步步尝试 scale Expert 的数量。从这篇论文里的 64 选 6，扩展到 128 选 12，到 V2 的 160 选 6，再到 V3 的 256 选 8。</p><p>可以看到 DeepSeek 一步步将 Expert 数量扩展，而且所需要选中的 Expert 比例也从 9% 一步步降低到 2%，证明了确实在 Expert 足够专业化后只需要更少部分的激活就可以完成对应的任务。</p><h2 id="知识共享-Expert"><a href="#知识共享-Expert" class="headerlink" title="知识共享 Expert"></a>知识共享 Expert</h2><p><img src="/../images/20250329170955.png"></p><p>随着 Expert 变小和 Expert 数量增加其实还会带来另外一个问题，那就是每个 Expert 除了需要特定领域的知识外，其实还需要一些通用知识，例如一些通用的语言理解和逻辑分析，可能是每个 Expert 都需要的。如果每个 Expert 都记忆了相关知识那么其实会造成大量的知识冗余，当 Expert 数量变多时，问题会更加明显。这其实会限制每个 Expert 的专业化，训练和推理过程中也会造成资源的浪费。</p><p>DeepSeek 提出的做法是增加一组共享 Expert，这一组 Expert 每个训练样本都会被激活，希望他们在训练过程中可以学到通用的知识，这样其他的 Expert 就无需再去学习这些通用知识，只需要学习专业知识了。当推理过程中这组共享 Expert 也会每次都被激活，来提供通用的知识信息。</p><p>这同样是一个很符合直觉的架构创新，但是由于之前 MoE 架构的 Expert 规模本来就不大，这个优化的意义其实并不明显，只有当规模上去了这个问题才会暴露出来。在这篇论文里 DeepSeek 还根据 Expert 数量按比例扩充了共享型 Expert 数量，但是随着更多的训练和实践，发现其实并不需要那么多共享型 Expert，等到 V3 的时候其实只使用到了 1 个共享型 Expert。</p><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>看完这篇论文我最大的感受是 DeepSeek 并不是拿一个已经验证过的架构无脑堆数据，而是真正的在做模型层面的创新。这也导致在 V3&#x2F;R1 大火之后很多框架第一时间都无法运行 DeepSeek 的模型或者性能也很差，因为 DeepSeek 的模型架构和其他人都有明显的差别。</p><p>并且相比 DeepSeek 其他论文里提到的 MLA、GRPO 和 NSA 这些需要复杂数学功底的创新不同，这两个模型创新都还是相对符合直觉的，但在那个时间点只有 DeepSeek 敢于这么尝试，其他人还在 Follow Llama 的 Dense 模型，敢于去做非主流的尝试，还是需要很大的勇气，这里只能对 DeepSeek 团队再次表达 Respect。</p><p><img src="/../images/20250329200735.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;从 DeepSeek V3&amp;#x2F;R1 开始关注 DeepSeek 工作的人很容易认为 DeepSeek 大量的工作都是在工程上优化效率，但是回看 DeepSeek 过去一年的论文才会发现他们其实一直在模型架构和训练方法上做各种创新，而 V3 和 R1 只是在之前架构创</summary>
      
    
    
    
    
    <category term="DeepSeek" scheme="http://oilbeater.com/tags/DeepSeek/"/>
    
    <category term="LLM" scheme="http://oilbeater.com/tags/LLM/"/>
    
    <category term="Paper" scheme="http://oilbeater.com/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>从 DeepSeek LLM 到 DeepSeek R1 —— DeepSeek LLM</title>
    <link href="http://oilbeater.com/2025/03/14/deepseek-from-llm-to-r1/"/>
    <id>http://oilbeater.com/2025/03/14/deepseek-from-llm-to-r1/</id>
    <published>2025-03-14T10:48:50.000Z</published>
    <updated>2025-08-11T02:24:41.736Z</updated>
    
    <content type="html"><![CDATA[<p>最近找到了 DeepSeek 发表过的论文合集，包含了从 DeepSeek 第一版的 LLM 到最新的 R1 的演变过程。从当下的角度我们当然知道 DeepSeek R1 在模型能力上已经接近了业界最领先的水平，但他是如何一步步从一个在中国一开始都没有被重视的量化公司走到这里的其实更吸引我的注意。</p><p>这个系列的博客我会从论文阅读的角度，试图去寻找他们一步步探索的轨迹，从论文的路径上来看就是 DeepSeek LLM -&gt; DeepSeek MoE -&gt; DeepSeek V2 -&gt; DeepSeek V3 -&gt; DeepSeek R1。在整理论文时我才发现，DeepSeek 第一篇对外发布的论文是在 2024 年的 1 月，当时他们刚发布第一版模型，即使在 AI 行业内也不被认为是个主要竞争者。然而仅仅一年后的 2025 年 1 月，就已经进化到了 R1 这种业界领先水平。都说 AI 一天，人间一年，但是当真看到人间一年的进展时，还是深深的被 DeepSeek 的速度所震撼。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>DeepSeek LLM 是 DeepSeek 开源的第一个 LLM，当时开源 LLM 里最受关注的是 LLaMA-2，很多模型也是基于它的架构和基础进行的。现在从后视的角度我们知道 DeepSeek 最终选择了和 LLaMA 这类 Dense 架构不同的 MoE 架构，但是在当时第一版的时候还是基本上照搬了 LLaMA-2 的架构，进行局部的调整。可以猜测当时团队内部还处于探索模型架构的阶段。</p><p>尽管架构大体和 LLaMA-2 相同，训练的数据量也都是 2T tokens，但是在性能评测上，如下图所示 DeepSeek LLM 基本上是全面超越了 LLaMA-2。论文里介绍了他们发现的一些有趣的关于数据，训练和微调的方法。</p><p><img src="/../images/deepseekllm.png" alt="alt text"></p><p>值得注意的是 DeepSeek 在训练过程中是可以使用更多的数据，使用更大参数量的模型的，显然这样做会提升模型性能。但是这篇论文目的主要是和 LLaMA-2 对比，因此特意把数据规模和参数量都做到尽可能相近，来比较在其他方面还有哪些地方可以提升。</p><h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>LLaMA-2 和 DeepSeek LLM 在数据的选择上还是有很大的区别的。虽然都是 2T 的 token 量，LLaMA-2 里接近 90% 的语料都是英文，而 DeepSeek LLM 虽然没有详细说语言的比例，但是看表述语料里的英文和中文比例应该是比较接近的。所以逻辑上来看 DeepSeek LLM 在中文的评测上大幅领先并不是个意外。意外的反而是在英文评测指标上，DeepSeek LLM 在训练量明显少的情况下依然取得了接近的性能结果。</p><p>我猜测这种现象的原因有两个，第一是 DeepSeek LLM 的语料质量更高，弥补了数量上的劣势。LLaMA-2 在介绍预训练语料的时候说除了一些敏感信息没有对数据集进行过滤，而 DeepSeek LLM 中介绍了为了提高数据质量专门做了模型去评估数据质量，还特意把一些冷门领域的数据占比放大，已获得更好的数据多样性。因此可以推测英文部分的语料质量 DeepSeek LLM 要高一些。作为参考 LLaMA-3 也在数据准备过程中引入了去重和质量过滤来提升语料的质量。</p><p>另一个原因我猜大概是中文语料的引入也提升了模型最终在英文上的表现。OpenAI GPT 3.5 的时候训练语料也是英文为主，但是最终在中文的表现上不差，一个猜测的原因就是在英文语料上学习到的一些知识迁移到了中文。同样中文语料里学习到的一些知识也可以迁移到英文。此外由于中文和英文在语法和表现形式上也有比较大的区别，这种多样化的数据是不是一定程度上也提升了模型的能力？还有就是不同语言本身就有不同的文化背景和内容倾向，这其实也是进一步增加了数据的多样性。如果这个猜测成立的话，那准备语料其实应该刻意地去增加不同语言的比重，让模型可以学习更丰富的语言表达形式。</p><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>模型的架构层面 DeepSeek LLM 和 LLaMA-2 几乎完全一样，各个路径上用到的技术比如 Pre-Norm，FFN 的激活函数，位置编码器都一模一样。最大的区别在于使用 GQA（Group<br>Query Attention）上。GQA 相比最原始的 MHA（Multi Head Attention），可以理解为为了节省训练和推理的 kv cache 占用，直接让多个 Query 头共享一组 Key 和 Value 的参数矩阵，这样可以大幅压缩显存的使用。但是带来的问题就是减少了 Key 和 Value 的潜空间个数，模型的表达能力也出现了下降。LLaMA-2 的做法是通过增加 FFN 网络的宽度来提供更多的非线性表达能力，而 DeepSeek LLM 的做法是增加 Attention 的层数。可以粗略理解尽管模型参数量相同，但是 LLaMA-2 是一个更宽的模型，而 DeepSeek LLM 是一个更深的模型。当然从后视的角度来看，DeepSeek 后续在 V2 公布的 MLA 对 Attention 层做了一个极其激进的更新，直接把推理所需的 KV Cache 降低了一个数量级。</p><p>另一个区别在于 LLaMA-2 使用的是 cosine learning scheduler 而 DeepSeek LLM 使用的是 Multi-Step learning scheduler。给出的理由是当增加数据量的时候，Multi-Step 可以更好的利用前一阶段的结果，持续训练速度会更快。</p><p>此外论文里还花了很大篇幅来介绍如何在不同的数据规模，数据质量，模型规模下选择合适的超参，如何去画 scaling law 曲线。这块是作者当成最大亮点来讲的，但是我看上去感觉和炼丹一样，看的我脑壳疼，感兴趣的同学可以自己看看。</p><h2 id="后训练"><a href="#后训练" class="headerlink" title="后训练"></a>后训练</h2><p>在论文发表的那个时间点，后训练主要是做对齐，也就是通过 SFT 和 RLHF 来对齐人类的偏好，增加模型的安全性。用到的数据基本上也都是一些带标记的对话文本，并没有对数据的分布做特别的处理。DeepSeek LLM 在这里对数据的选择又做出了和 LLaMA-2 很不一样的选择。</p><p>如果看最上面模型性能评估的对比图，可以看到 DeepSeek LLM 在 MATH，HumanEval 和 MBPP 几个非中文的指标表现也要好很多。因为 DeepSeek 在后训练的 SFT 阶段将近 70% 的样本都是 Math 和 Code 相关数据。可见他们根本就没把对齐作为后训练的重点，而是把提升模型能力作为后训练的重点，所以这更像是一个鸡贼的刷榜优化。</p><p>当时主流的做法还是在 base model 训练好了后再 SFT 一个代码和数学领域的模型，比如 Code LLaMA  和 OpenAI Codex 是分别在 LLaMA-2 和 OpenAI GPT3 上 SFT 出来的。Meta 当时甚至还在 Code LLaMA 上再 SFT 一个 Python 专用的 LLM 出来。</p><p>现在我们当然知道在后训练阶段通过在 Math 和 Code 样本上进行 RL 可以激发出模型 CoT 的推理能力，R1 的想法可能在这个时候就已经诞生了。</p><p>此外 DeepSeek LLM 在这里并没有用当时很流行的 RLHF，而是选择 DPO(Direct Preference Optimization) 进行和人类偏好的对齐。这种方法直接对两个不同生成结果的概率差作为优化目标进行训练，这相比 RL 其实更直观也更容易设计，在 LLaMA-3 的后训练过程中也用到了 DPO。可以看出 DeepSeek 团队当时对已有的 RL 算法还是不太满意的，还在探索。这也就造就了后来在 DeepSeek Math 中公布的 GRPO。 </p><h2 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h2><p>在我上学的时候，老师和我说真正的 Future Work 不要写在论文里，自己偷偷做再发下一篇，论文的 Future Work 里就写你觉得没戏的和你觉得做不出来的。而 DeepSeek LLM 最后 Future Work 的几句话从现在的视角来看都太真诚了，几乎已经把 R1 的路子给指出来了。</p><blockquote><p>DeepSeek LLM 将会是一个长期项目，专注于促进开源模型进步，</p></blockquote><p>这个不好说，毕竟满打满算也就一年多。</p><blockquote><p>Soon，我们将会发布 Code 和 MoE 架构的技术报告。MoE 的架构看上去很有希望。</p></blockquote><p>这个 Soon 指的是一周发布 MoE，半个月发布 DeepSeek Code。而我们已经知道 MoE 成为了 V2，V3 和 R1 模型的基础架构，参数量也上升到了 671B。</p><blockquote><p>我们现在已经有了大得多质量好得多的数据集，下一代模型所有指标都会显著提升。</p></blockquote><p>数据量半年后从 2T 变成了 8T，不过同期 LLaMA-3 变成了 15T。DeepSeek V2 的各项指标相比同期的 LLaMA-3 在英文上是稍微落后的，而且在 V2 的时候他们的重点就已经变成了疯狂降低成本。</p><blockquote><p>我们的对齐团队发现强化学习能够增强模型的复杂推理能力。</p></blockquote><p>从今天回头来看，这不就是 R1 最重要的方法么。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从今天的视角来看，DeepSeek 当时应该还在探索期，还在和业界的开源模型对齐，还做了很多理论上的研究。但是从论文的各个细节上来看，一年后那个石破天惊的 R1 诞生的条件已经差不多具备了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近找到了 DeepSeek 发表过的论文合集，包含了从 DeepSeek 第一版的 LLM 到最新的 R1 的演变过程。从当下的角度我们当然知道 DeepSeek R1 在模型能力上已经接近了业界最领先的水平，但他是如何一步步从一个在中国一开始都没有被重视的量化公司走到这</summary>
      
    
    
    
    
    <category term="DeepSeek" scheme="http://oilbeater.com/tags/DeepSeek/"/>
    
    <category term="LLM" scheme="http://oilbeater.com/tags/LLM/"/>
    
    <category term="Paper" scheme="http://oilbeater.com/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>从 Network Binding Plugin 看 KubeVirt 的扩展方式</title>
    <link href="http://oilbeater.com/2025/01/12/kubevirt-network-binding/"/>
    <id>http://oilbeater.com/2025/01/12/kubevirt-network-binding/</id>
    <published>2025-01-12T08:16:07.000Z</published>
    <updated>2025-08-11T02:24:41.736Z</updated>
    
    <content type="html"><![CDATA[<p>在 KubeVirt v1.4 的新版本里将 <a href="https://kubevirt.io/user-guide/network/network_binding_plugins/">Network Binding Plugin</a> 提升到了 Beta，提供了一种新的扩展 KubeVirt 网络的方式。虽然名义上是为了扩展网络的能力，但实际上从实现上来看，这个机制能做的事情远不止网络，所有和 libvirt domain xml 相关的变更都可以通过这个机制来实现。</p><h2 id="KubeVirt-Network-Overview"><a href="#KubeVirt-Network-Overview" class="headerlink" title="KubeVirt Network Overview"></a>KubeVirt Network Overview</h2><p>先从网络的角度来看下 KubeVirt 之前的网络机制有什么问题，新的机制又是如何进行扩展的。</p><p>由于 KubeVirt 使用的是 Pod 里面跑 VM 的架构，所以复用了 CNI 的网络机制。这样的话就将网络分成了两个部分，一个是 Pod 网络由各个 CNI 提供。另一部分就是如何将 CNI 提供的网络接入 VM，在 libvirt 里这部分叫做 Domain 网络。</p><p>KubeVirt 之前的各种网络机制（Bridge，Masquerade，Passt，Slirp）所做的事情就是通过不同的技术方案将 Pod 里的 eth0 接入到 VM 的 tap0 网卡。例如 Bridge 将 tap0 和 eth0 接入到同一个网桥，Masquerade 将 tap0 的流量经过 iptables nat 规则导入 eth0，Passt 和 Slirp 通过用户态的网络栈做流量重定向。</p><p><img src="/../images/kubevirt-networking-tradition.png" alt="alt text"></p><p>这些方法在实现上都是类似的，在 Pod 内部做一些网络相关的配置，然后修改 libvirt 的启动参数接入对应的网络。但是现有的机制都是写死在 KubeVirt Core 里的，并没有扩展机制，想要新增一种机制或者修改已有的机制都需要修改 KubeVirt 的代码很不灵活，例如默认的 bridge 插件会劫持 DHCP 请求，但是又不支持 IP, 所以 bridge 模式下的双栈就很难实现，而 Kube-OVN 中已经实现的 DHCP 又被这个机制绕过去了，之前想做 bridge 的双栈就需要改 KubeVirt 的代码来关闭默认的 DHCP 十分麻烦。因此新版本中将这套机制抽象出来提供了一套通用的机制。</p><h2 id="Hook-Sidecar"><a href="#Hook-Sidecar" class="headerlink" title="Hook Sidecar"></a>Hook Sidecar</h2><p>先来看一种在 KubeVirt 中已经存在的扩展机制 <a href="https://kubevirt.io/user-guide/user_workloads/hook-sidecar/">Hook Sidecar</a>。</p><p>这套机制是在 VM 正式创建前，可以加载一个用户自定义的镜像，或者一段 ConfigMap 里保存的 Shell 或者 Python 脚本，来修改 VM 启动前 libvirt 的启动参数和 cloud-init 参数。</p><p>它的执行机制和 CNI 有些类似，virt-handler 在启动 VM 前会去对应目录寻找 <code>/usr/bin/onDefineDomain</code> 和 <code>/usr/bin/preCloudInitIso</code> 两个二进制文件，前者传入 virt-handler 生成的 libvirt XML 配置，返回修改后的配置；后者传入 cloudInit 配置，返回修改后的 cloudInit 配置。这样的话所有 KubeVirt 本身不支持的 libvirt 和 cloudInit 参数都可以通过这种机制来注入修改。并且由于 Sidecar 内实际可以执行任意代码，所能做的事情远不止修改这两个配置，所有初始化阶段 KubeVirt 没有实现的能力其实都可以在这里来实现。</p><h2 id="Network-Binding-Plugin"><a href="#Network-Binding-Plugin" class="headerlink" title="Network Binding Plugin"></a>Network Binding Plugin</h2><p>现在可以到 Network Binding Plugin 这个机制了，这个机制其实和 Hook Sidecar 基本上大同小异。主要区别是将二进制调用改成了 gRPC 调用，gRPC 里注册的方法还是  <code>onDefineDomain</code> 和 <code>preCloudInitIso</code> 参数传递从命令行参数改为了 gRPC Request 里的参数，其他都是一样的。</p><p>具体的例子可以参考目前还在 KubeVirt 代码里的 <a href="https://github.com/kubevirt/kubevirt/tree/main/cmd/sidecars/network-slirp-binding">Slirp Binding</a> 的实现。尽管在 Network Binding Plugin 的规范里还增加了<code>networkAttachmentDefinition</code> 字段可以选择一个 CNI，但这个其实使用之前的网卡选择机制也能实现，甚至由于 Sidecar 里可以执行任意代码，在里面再实现一个 CNI 覆盖 Pod 原先的网络也是可以的。</p><p>那么之后的网络架构就变成了下图这样：</p><p><img src="/../images/networking-binding.png" alt="alt text"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>虽然 Network Binding Plugin 的机制是为 Network 扩展准备的，但实际上几乎可以扩展所有 KubeVirt 在 virt-handler 侧的处理逻辑。甚至可以把 KubeVirt 也只当一个框架，所有的逻辑都通过 Sidecar 来处理，相信未来可以玩出不少花活来。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://kubevirt.io/user-guide/user_workloads/hook-sidecar/">https://kubevirt.io/user-guide/user_workloads/hook-sidecar/</a></li><li><a href="https://kubevirt.io/user-guide/network/network_binding_plugins/">https://kubevirt.io/user-guide/network/network_binding_plugins/</a></li><li><a href="https://github.com/kubevirt/kubevirt/blob/main/docs/network/network-binding-plugin.md">https://github.com/kubevirt/kubevirt/blob/main/docs/network/network-binding-plugin.md</a></li><li><a href="https://github.com/kubevirt/kubevirt/tree/main/cmd/sidecars/network-slirp-binding">https://github.com/kubevirt/kubevirt/tree/main/cmd/sidecars/network-slirp-binding</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在 KubeVirt v1.4 的新版本里将 &lt;a href=&quot;https://kubevirt.io/user-guide/network/network_binding_plugins/&quot;&gt;Network Binding Plugin&lt;/a&gt; 提升到了 Beta，提供了</summary>
      
    
    
    
    
    <category term="kubevirt" scheme="http://oilbeater.com/tags/kubevirt/"/>
    
    <category term="networking" scheme="http://oilbeater.com/tags/networking/"/>
    
  </entry>
  
  <entry>
    <title>加速容器镜像下载：从缓存到按需加载</title>
    <link href="http://oilbeater.com/2024/10/31/docker-pull/"/>
    <id>http://oilbeater.com/2024/10/31/docker-pull/</id>
    <published>2024-10-31T11:59:25.000Z</published>
    <updated>2025-08-11T02:24:41.736Z</updated>
    
    <content type="html"><![CDATA[<p>在容器的启动过程中，镜像下载速度往往是影响启动速度的最主要因素，通常占据了启动时间的 70% 以上。特别是对于体积庞大的 VM、AI 镜像，它们的大小可能达到数十 GB，导致下载和解压速度都成为启动的瓶颈。本文将探讨镜像下载的主要瓶颈、常见的优化方案以及最新的按需加载技术，以加速容器启动。</p><h3 id="镜像下载速度慢的原因"><a href="#镜像下载速度慢的原因" class="headerlink" title="镜像下载速度慢的原因"></a>镜像下载速度慢的原因</h3><p>容器镜像下载慢的原因主要有以下几点：</p><ul><li><strong>镜像体积过大</strong>：VM、AI 镜像体积通常较大，可能达到数十 GB，使得下载时间显著。</li><li><strong>gzip 解压耗时</strong>：特别是在内网环境中，解压时间往往远高于网络传输时间，导致解压成为新的瓶颈。</li></ul><h3 id="常见的镜像优化思路"><a href="#常见的镜像优化思路" class="headerlink" title="常见的镜像优化思路"></a>常见的镜像优化思路</h3><p>为了解决下载和解压的速度问题，业界提出了多种优化方案：</p><ol><li><p><strong>镜像缓存</strong><br>镜像缓存是提升镜像下载速度的一种方法，通过缓存镜像可以避免重复下载。然而，缓存无法解决冷启动问题，并且镜像频繁变更（如应用更新或安全更新）会导致缓存失效。要实现高效的缓存管理，还需要较复杂的机制来管理缓存更新。</p></li><li><p><strong>减小镜像体积</strong><br>减少镜像体积也有助于缩短下载时间，但在某些场景下，例如 VM、AI、CUDA 镜像，体积优化空间有限。它们通常需要使用超过 7 GB 的存储空间，难以进一步缩减。</p></li></ol><h3 id="按需加载：是否可行？"><a href="#按需加载：是否可行？" class="headerlink" title="按需加载：是否可行？"></a>按需加载：是否可行？</h3><p>目前，大多数容器在启动时并不需要完整的镜像内容。一些论文表明，启动期间仅需 6.4% 的镜像内容，因此理论上可以通过按需下载来优化启动速度。然而，现有的镜像格式存在以下问题，限制了按需下载的实现：</p><ul><li><strong>OverlayFS 的限制</strong>：需要所有镜像层下载完毕后才能得知最终文件结构。</li><li><strong>gzip 不支持随机访问</strong>：即使只需下载单个文件，也要下载并解压整个层。</li><li><strong>校验问题</strong>：镜像 digest 是按整个层计算的，无法针对单个文件校验。</li></ul><h3 id="eStargz：实现按需加载"><a href="#eStargz：实现按需加载" class="headerlink" title="eStargz：实现按需加载"></a>eStargz：实现按需加载</h3><p>为了解决上述问题，eStargz 提出了针对 gzip 层的优化方案，即每个文件单独压缩并增加文件级别索引。eStargz 引入了如下优化：</p><ol><li><strong>独立压缩</strong>：每个文件单独压缩并索引，解决了 gzip 无法随机访问的问题。</li><li><strong>文件校验</strong>：可以对单个文件进行校验，无需校验整个层。</li></ol><p>具体的存储格式如下图：</p><p><img src="/../images/estartgz.png" alt="alt text"></p><p>每个文件被单独压缩合并成一个大的 blob，在 blob 最后增加一个 TOC 的描述文件记录每个文件的偏移量和校验值，这样就实现了按文件的索引和校验。</p><p>以下是 eStargz 的 TOC 格式示例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;entries&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bin/&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;dir&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;modtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-08-20T10:30:43Z&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;mode&quot;</span><span class="punctuation">:</span> <span class="number">16877</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;NumLink&quot;</span><span class="punctuation">:</span> <span class="number">0</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bin/busybox&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;reg&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;size&quot;</span><span class="punctuation">:</span> <span class="number">833104</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;modtime&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2019-06-12T17:52:45Z&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;mode&quot;</span><span class="punctuation">:</span> <span class="number">33261</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;offset&quot;</span><span class="punctuation">:</span> <span class="number">126</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;NumLink&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;digest&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sha256:8b7c559b8cccca0d30d01bc4b5dc944766208a53d18a03aa8afe97252207521f&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;chunkDigest&quot;</span><span class="punctuation">:</span> <span class="string">&quot;sha256:8b7c559b8cccca0d30d01bc4b5dc944766208a53d18a03aa8afe97252207521f&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>通过这种改进，eStargz 实现了对单个文件的按需加载，且可以实现文件级别校验。</p><h3 id="性能权衡：优先级加载"><a href="#性能权衡：优先级加载" class="headerlink" title="性能权衡：优先级加载"></a>性能权衡：优先级加载</h3><p>虽然按需加载大幅优化了下载性能，但也可能带来运行时性能的下降。为此，eStargz 采用特殊标识来实现优先级加载，将启动所需文件放置于 <code>prioritized zone</code> 中，确保这些文件优先下载，进而提升运行时性能。</p><p><img src="/../images/estargz-optimize.png" alt="alt text"></p><p>按照作者测试的性能表现如下：</p><p><img src="/../images/estargz-perf.png" alt="alt text"></p><h3 id="代价与挑战"><a href="#代价与挑战" class="headerlink" title="代价与挑战"></a>代价与挑战</h3><p>尽管 eStargz 带来了按需加载的性能提升，但也带来了以下代价：</p><ul><li><strong>存储空间增加</strong>：每个文件单独压缩会增加额外的 metadata，降低压缩率。</li><li><strong>额外插件支持</strong>：eStargz 需要插件支持，例如在容器镜像推送和拉取时需要特定处理插件。</li></ul><h3 id="如何使用-eStargz"><a href="#如何使用-eStargz" class="headerlink" title="如何使用 eStargz"></a>如何使用 eStargz</h3><p>以下是 eStargz 的使用方法，适用于 containerd 的子项目以及一些支持 eStargz 的工具：</p><ol><li><p><strong>Docker, kaniko, nerdctl 命令行参数</strong>：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker buildx build -t ghcr.io/ktock/hello:esgz \</span><br><span class="line">    -o <span class="built_in">type</span>=registry,oci-mediatypes=<span class="literal">true</span>,compression=estargz,force-compression=<span class="literal">true</span> \</span><br><span class="line">    /tmp/buildctx/</span><br><span class="line"></span><br><span class="line">nerdctl image convert --estargz --oci ghcr.io/ktock/hello:1 ghcr.io/ktock/hello:esgz</span><br></pre></td></tr></table></figure></li><li><p><strong>containerd 插件配置</strong>：</p> <figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version</span> = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="section">[proxy_plugins]</span></span><br><span class="line">  <span class="section">[proxy_plugins.stargz]</span></span><br><span class="line">    <span class="attr">type</span> = <span class="string">&quot;snapshot&quot;</span></span><br><span class="line">    <span class="attr">address</span> = <span class="string">&quot;/run/containerd-stargz-grpc/containerd-stargz-grpc.sock&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="section">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd]</span></span><br><span class="line">  <span class="attr">snapshotter</span> = <span class="string">&quot;stargz&quot;</span></span><br><span class="line">  <span class="attr">disable_snapshot_annotations</span> = <span class="literal">false</span></span><br></pre></td></tr></table></figure></li></ol><p>此外，GKE 等云平台的集群已默认启用类似方案，进一步加速了镜像启动速度。看阿里也发表了基于 block device 的按需加载，这类的实现看上去在云厂商都有了比较大规模的落地。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>从传统的镜像缓存、镜像体积优化，到按需加载，eStargz 提供了一种兼顾性能和灵活性的方案，使得容器可以在仅下载部分内容的情况下启动。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="https://github.com/containerd/stargz-snapshotter/blob/main/docs/estargz.md">eStargz: Standard-Compatible Extension to Container Image Layers for Lazy Pulling</a></li><li><a href="https://medium.com/nttlabs/startup-containers-in-lightning-speed-with-lazy-image-distribution-on-containerd-243d94522361">Startup Containers in Lightning Speed with Lazy Image Distribution on Containerd</a></li><li><a href="https://www.usenix.org/conference/fast16/technical-sessions/presentation/harter">Slacker: Fast Distribution with Lazy Docker Containers</a></li><li><a href="https://github.com/containerd/accelerated-container-image">Accelerated Container Image</a></li><li><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/image-streaming">Use Image streaming to pull container images</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在容器的启动过程中，镜像下载速度往往是影响启动速度的最主要因素，通常占据了启动时间的 70% 以上。特别是对于体积庞大的 VM、AI 镜像，它们的大小可能达到数十 GB，导致下载和解压速度都成为启动的瓶颈。本文将探讨镜像下载的主要瓶颈、常见的优化方案以及最新的按需加载技术，</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>AI Gateway 调研之 Kong, Gloo 和 Higress</title>
    <link href="http://oilbeater.com/2024/08/26/kong-gloo-higress/"/>
    <id>http://oilbeater.com/2024/08/26/kong-gloo-higress/</id>
    <published>2024-08-26T07:39:23.000Z</published>
    <updated>2025-08-11T02:24:41.736Z</updated>
    
    <content type="html"><![CDATA[<p>上篇<a href="2024-08-25-ai-gateway-cloudflare">博客</a>介绍了 Cloudflare AI Gateway，这篇集中介绍一下 Kong, Gloo 和 Higress 因为这三者有一定的相似性，都是从原有的 API 网关基础上进行扩展，通过插件的方式支持了一系列 AI 相关的功能，在交付上也是传统的软件部署方式。这几个算是传统 API Gateway 迎接 AI 浪潮的代表，其中 Higress 更是把产品 Slogan 直接从 Cloud Native API Gateway 变成了 AI Gateway，虽然打不过就加入，但这样变来变去不怕人说没根么：）</p><p>由于这三款产品都需要额外的部署开通，有的 AI 功能还是商业版才有，所以下面的分析都是根据看文档总结而来，可能存在着和实际不符的情况。</p><table><thead><tr><th>功能</th><th>Kong</th><th>Gloo</th><th>Higress</th><th>备注</th></tr></thead><tbody><tr><td>技术栈</td><td>Nginx + Lua</td><td>Envoy + Go</td><td>Envoy + WASM</td><td>虽然几家都提供了插件机制，但是和网关的耦合程度都比价高，非网关的开发者上手还是有一定难度</td></tr><tr><td>日志监控</td><td>每个 AI 插件会将元信息，如模型名、Token 开销费用等信息加入到 Audit Log 中，但是似乎没有自定义元信息的功能，需要通过其他插件来辅助完成</td><td>似乎没有在 AI 这块对日志有功能增强，还是通用的监控</td><td>日志和监控中增加了 Token 的用量，提供的信息和 Kong 类似，也不具备自定义元信息的功能</td><td>如果能增加一些自定义元信息，并支持记录 Request 和 Response 里 Message 信息就更好了</td></tr><tr><td>Proxy</td><td>Kong 提供了归一化的 API 能够用一套统一的 API 去调用不同的 LLM API，这对开发者还是比较友好的能够不需要大改应用代码就能用不同的 LLM</td><td>Gloo 没有提供归一化的 API，只是反向代理到上游 LLM API</td><td>Higress 支持将不同的 LLM API 统一转换成 OpenAI API，这对开发者来说也比较友好，毕竟目前生态里还是直接用 OpenAI API 的比较多</td><td>虽然我觉得是否提供归一化的 API 没那么重要，不过一定要归一化的话归一化成 OpenAI 格式的会好些</td></tr><tr><td>API Key 管理</td><td>客户端的 Key 可以和上游的 Key 不一样，相当于把 Key 在网关层做了一层屏蔽</td><td>客户端的 Key 可以和上游的 Key 不一样，相当于把 Key 在网关层做了一层屏蔽</td><td>直接从客户端透传 Key 给上游</td><td>个人感觉 Gloo 这个功能还比较实用，避免了在 LLM 那里真实的 Key 被过多业务方知道，安全和可控性会更好一些</td></tr><tr><td>Cache</td><td>当前版本没有提供 LLM Cache 相关能力，据说会在 3.8 版本提供</td><td>提供了语义 Cache，看配置是调用了 OpenAI 的 Embedding 和 Redis 的 Vector，不过没看到更细粒度的比如 TTL 相似度的配置</td><td>提供的还是文本匹配的缓存，相比全文本可以通过 JSON PATH 的语法选择部分 Message 做缓存，看配置也是利用 Redis，不过不支持语义 Cache</td><td>Gloo 提供的语义 Cache 看起来更高级一些</td></tr><tr><td>请求&#x2F;响应改写</td><td>可以在 Request 和 Response 阶段分别加 prompt 对 message 进行改写，相当于一个小型的 workflow</td><td>只提供了 prepend system prompt 的能力，感觉提升有限</td><td>和 Kong 类似提供了用 prompt 进行改写的能力，不过现在只支持通义千问的 LLM 感觉不够开放</td><td></td></tr><tr><td>RAG</td><td>目前没有相关功能的插件</td><td>可以对接一个 postgres 和 OpenAI 的 embedding Token 这样可以自己提供一些文本来做 RAG</td><td>和 Gloo 的功能类似，不过只支持阿里云的向量服务和通义千问，还是感觉不够开放</td><td>感觉 RAG 的配置参数都比较少没有相似度，或者爬取网页的接口，只能做比较简单的 RAG</td></tr></tbody></table><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这三款产品由于只是看文档没有真实使用过，里面的内容很可能有不准确的地方，希望了解的同学可以指正。</p><p>总体来看三款产品都是 LLM 爆发前就存在的，之前也不是专门为 AI 场景设计的，很多使用的配置可能懂 Kubernetes 的更能看懂。尤其是 Gloo 的文档全是 YAML 和 CRD 的配置，浓浓的 Cloud Native 味道，Higress 看上去也是各种 ConfigMap 脱离 Kubernetes 是否还能用好我是心里存在疑虑的。</p><p>Higress 虽然没根了，但是整体看 AI 功能做的还是最完整的，发力也比较明显。Kong 感觉还只是试探性的做了些功能，而 Gloo 是把所有 AI 相关功能都放到商业版里了。如果 Higress 能把开发性做好不是被通义千问和阿里云上各种服务绑定的话我觉得还是个不错的项目。</p><p>最后的依赖还是这三款产品的扩展性可能都存在一定难度，需要高度了解网关相关的逻辑并掌握 Lua 或者 WASM 这样非主流的语言。而 AI 应用现在的形态其实还存在很多变化的可能，对应的 API 和需要的通用能力可能也有比较大的变化，比如怎么做 RAG，怎么做 Cache，怎么编排 LLM 都没有确定下来。不知道现在的架构会不会对他们未来的功能灵活变化产生影响。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;上篇&lt;a href=&quot;2024-08-25-ai-gateway-cloudflare&quot;&gt;博客&lt;/a&gt;介绍了 Cloudflare AI Gateway，这篇集中介绍一下 Kong, Gloo 和 Higress 因为这三者有一定的相似性，都是从原有的 API 网关基础上进</summary>
      
    
    
    
    
    <category term="AI" scheme="http://oilbeater.com/tags/AI/"/>
    
    <category term="Gateway" scheme="http://oilbeater.com/tags/Gateway/"/>
    
    <category term="Kong" scheme="http://oilbeater.com/tags/Kong/"/>
    
    <category term="Gloo" scheme="http://oilbeater.com/tags/Gloo/"/>
    
    <category term="Higress" scheme="http://oilbeater.com/tags/Higress/"/>
    
  </entry>
  
  <entry>
    <title>AI Gateway 调研之 Cloudflare AI Gateway</title>
    <link href="http://oilbeater.com/2024/08/25/ai-gateway-cloudflare/"/>
    <id>http://oilbeater.com/2024/08/25/ai-gateway-cloudflare/</id>
    <published>2024-08-25T14:10:45.000Z</published>
    <updated>2025-08-11T02:24:41.736Z</updated>
    
    <content type="html"><![CDATA[<p>随着 AI 的火热，眼看着之前调研的各家竞品 API 网关产品纷纷把自己的介绍改为 AI Gateway，于是就想调研一下这些所谓的 AI Gateway 究竟做了些啥。这次调研的对象有一些之前靠 API 网管或者云原生 Ingress Controller 起家加入 AI 功能的，例如：<a href="https://konghq.com/products/kong-ai-gateway">Kong</a>，<a href="https://www.solo.io/products/gloo-ai-gateway/">Gloo</a> 和 <a href="https://higress.io/en/">Higress</a>。也包括一些第一天就是借着 AI 起来的我认为真正 AI 原生的网关，例如 <a href="https://portkey.ai/features/ai-gateway">Portkey</a> 和 <a href="https://github.com/songquanpeng/one-api">OneAPI</a>。以及这篇博客介绍的基于公有云 Serverless 的 <a href="https://developers.cloudflare.com/ai-gateway/">Cloudflare AI Gateway</a>。</p><p>大体来看目前的 AI Gateway 主要能力在三个方面：</p><p><strong>常规 API 网关功能在 AI API 上的应用</strong>，例如：监控，日志，限速，反向代理，请求或响应改写，集成用户系统等。这些功能其实和 AI 关系不大就是把 LLM 的 API 当成了一个普通的 API 进行接入。</p><p><strong>部分 API 网关功能针对 AI 进行优化</strong>，例如限速功能增加基于 Token 的限速，缓存功能增加基于 Prompt 的缓存，防火墙基于 prompt 和 LLM 返回进行过滤，多个 LLM API Key 之间的负载均衡，多个 LLM Provider 的 API 转换。这些功能在原有的 API 网关就存在类似的概念，不过在 AI 场景下又有了相应的扩展。</p><p><strong>基于 AI 应用的场景增加的新功能</strong>，例如部分 AI 网关增加了 Embedding 和 RAG 的功能，把向量数据库和文本数据库的功能通过 API 的形式提供出来。还有一些针对 token 用量的性能优化，比如 Prompt 简化，语义化 Cache 等。还有一些更偏应用层的功能，例如对 LLM Output 提供打分功能等。</p><p>这篇博客介绍 <a href="https://developers.cloudflare.com/ai-gateway/">Cloudflare AI Gateway</a> 这款 AI Gateway 的特点。</p><h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><p>Cloudflare 的这款 AI Gateway 主要功能其实就是一个反向代理，看完了我甚至觉得我用 Cloudflare Worker 捣鼓一阵也能做个功能类似的。如果你原来用的是 OpenAI 的 API 那么现在你要做的就是把 SDK 里的 baseURL 换成 <code>https://gateway.ai.cloudflare.com/v1/$&#123;accountId&#125;/$&#123;gatewayId&#125;/openai</code> 就可以了。在这个过程中由于流量进出都是过 Cloudflare 的，Cloudflare 平台上就可以提供对应的监控，日志，缓存等功能。</p><p>这个方案有下面几个优点：</p><ul><li>接入很简单，改一下 baseURL 就接入进来了，API 格式也没有任何变化。并且完全是 Serverless 的，不需要自己额外管理任何服务器，这个功能现在是免费的，直接就白嫖了监控数据。</li><li>借助 Cloudflare 的全球网络可以实现一定的用户接入加速，不过这个用户接入的加速相比 LLM 本身的延迟比重应该很小，顶多在首个 Token 的延迟会有明显变化。</li><li>通用借助 Cloudflare 的全球网络可以一定程度隐藏掉源 IP，对于一些 OpenAI API 访问受限的区域用这个可以绕过去。</li></ul><p>但对应的也有下面的缺点：</p><ul><li>所有请求信息包括 API Key 都要在 Cloudflare 上过一道，会有安全方面的一些隐患。</li><li>Gateway 本身没有什么插件机制，想扩展功能的话会比较麻烦，只能在外面再套一层。</li><li>同样是因为 Cloudflare 的全球网路欧，如果一个 Key 一直变换 IP 地址访问，不知道会不会触发 OpenAI 那边的拉黑。</li></ul><h1 id="主要能力"><a href="#主要能力" class="headerlink" title="主要能力"></a>主要能力</h1><h2 id="多个-Provider-支持"><a href="#多个-Provider-支持" class="headerlink" title="多个 Provider 支持"></a>多个 Provider 支持</h2><p>由于 Cloudflare AI Gateway 并没有对 LLM API 进行修改，只是做反向代理，所以几乎主流的 LLM API 它都可以支持，只需要把 baseURL 改成对应 Provider 如 <code>https://gateway.ai.cloudflare.com/v1/$&#123;accountId&#125;/$&#123;gatewayId&#125;/&#123;provider&#125;</code> 即可。</p><p>它唯一多提供的一个 API 叫做 <a href="https://developers.cloudflare.com/ai-gateway/providers/universal/">Universal Endpoint</a> 可以做简单的 fallback。用法是在一个 API 请求里可以填写多个 Provider 的<br>query，这样当前面的 Provider 请求失败时会自动调用下一个 Provider。</p><h2 id="可观测"><a href="#可观测" class="headerlink" title="可观测"></a>可观测</h2><p>监控层面除了基础的 QPS 和 Error Rate 这些监控面板，还针对 LLM 的场景提供了 Token，Cost 以及 Cache 命中率的面板。</p><p>日志方面和 Worker 的日志很类似，只有实时日志无法查询历史日志。这里感觉做的不太好，Worker 至少还有第三方的方案能保存日志，但是 Gateway 这里却没有了。虽然通过一些实时日志 API 再自己保存的方式也可以，但还是太麻烦了。分析 LLM 请求和响应日志应该是很多 AI 应用后续做优化甚至 fine-tuning 的一个重要环节，这里没有直接集成持久化的方案其实是个硬伤。</p><h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><p>缓存方面，Cloudflare 提供的还是基于文本内容完全匹配的缓存，目测是通过 <a href="https://developers.cloudflare.com/kv/">Cloudflare Workers KV</a> 来实现的。也可以通过 <code>cf-aig-cache-key</code> 来实现自定义 Cache Key，包括设置缓存的 TTL 以及忽略 Cache。但是整体看起来基于现在的功能是无法实现语义缓存的，官方文档的说法是语义缓存会在未来提供。</p><h2 id="Rate-Limiting"><a href="#Rate-Limiting" class="headerlink" title="Rate Limiting"></a>Rate Limiting</h2><p>限速方面，Cloudflare 提供的还是传统的基于 QPS 的限速，这块并没有基于 AI 的场景提供基于 Token 的限速，这里未来还有改善的空间。</p><h2 id="Custom-metadata"><a href="#Custom-metadata" class="headerlink" title="Custom metadata"></a>Custom metadata</h2><p>可以在请求的 Header 中增加一些自定义字段，比如用户信息。这些信息可以通过日志进行检索。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>整体来看 Cloudflare AI Gateway 胜在简单易用，对于之前没有使用 AI Gateway 的用户可以两三分钟就接进来，提供了基础的监控和缓存能力。而且 Cloudflare 还有一些其他配套的 AI 服务例如 Works AI 提供了大量的开源模型的 Serving 和 Worker 提供边缘计算，几个一结合就能搭一套完全 Serverless 的 AI 系统。</p><p>他的问题主要在于更深入的功能提供的比较少，而且功能扩展比较麻烦，只能在外围通过 Worker 再来包一层。与其这样 Cloudflare 还不如直接把 AI Gateway 开源出来变成一个模板，用户可以根据自己需求去更改代码或者写插件，没准还能形成一个新的生态。毕竟我高度怀疑现在的 AI Gateway 其实就是个 Worker 模板。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;随着 AI 的火热，眼看着之前调研的各家竞品 API 网关产品纷纷把自己的介绍改为 AI Gateway，于是就想调研一下这些所谓的 AI Gateway 究竟做了些啥。这次调研的对象有一些之前靠 API 网管或者云原生 Ingress Controller 起家加入 AI</summary>
      
    
    
    
    
    <category term="AI" scheme="http://oilbeater.com/tags/AI/"/>
    
    <category term="Gateway" scheme="http://oilbeater.com/tags/Gateway/"/>
    
    <category term="Cloudflare" scheme="http://oilbeater.com/tags/Cloudflare/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 多集群通信的五种方案</title>
    <link href="http://oilbeater.com/2024/05/24/5-way-kubernetes-multcluster-communication/"/>
    <id>http://oilbeater.com/2024/05/24/5-way-kubernetes-multcluster-communication/</id>
    <published>2024-05-24T08:25:30.000Z</published>
    <updated>2025-08-11T02:24:41.736Z</updated>
    
    <content type="html"><![CDATA[<p>随着企业的业务规模不断扩大，Kubernetes 的使用也从单集群逐步扩展到多集群部署。多集群环境下，集群之间的通信成为一个重要的研究课题。本文将介绍五种跨 Kubernetes 集群通信的方案的基本原理，优点及其局限性。</p><h2 id="1-Underlay-网络"><a href="#1-Underlay-网络" class="headerlink" title="1. Underlay 网络"></a>1. Underlay 网络</h2><p>这类网络插件包括 <a href="https://www.cni.dev/plugins/current/main/macvlan/">macvlan</a>&#x2F;<a href="https://www.cni.dev/plugins/current/main/ipvlan/">ipvlan</a>&#x2F;<a href="https://kubeovn.github.io/docs/stable/start/underlay/">Kube-OVN underlay</a> 以及各种云上的 VPC CNI 等。</p><p><strong>基本原理</strong>：</p><p>Underlay 网络从 CNI（容器网络接口）的角度来看是最简单的方式。这种方式依赖于底层基础设施在网络层面实现打通。例如，使用公有云上的 VPC Peering 或者物理网络配置路由和大二层。当底层网络实现了连通，跨集群的容器网络就自然连通了。</p><p><strong>优点</strong>：</p><ul><li>CNI 角度最为简单，无需额外操作。</li><li>架构上清晰，把跨集群通信的责任交给底层网络。</li></ul><p><strong>局限性</strong>：</p><ul><li>依赖于特定 CNI，Underlay 类型的网络场景使用范围有限，有些情况只能使用 Overlay 网络。</li><li>在异构的环境，比如多云之间、公有云和私有云之间打通存在困难。</li><li>只是做了基础的容器网络通信打通，更上层的服务发现，域名和网络策略等功能存在缺失。</li><li>一次性打通所有集群的容器网络，缺乏细粒度的控制。</li></ul><h2 id="2-提供跨集群通信能力的-Overlay-CNI"><a href="#2-提供跨集群通信能力的-Overlay-CNI" class="headerlink" title="2. 提供跨集群通信能力的 Overlay CNI"></a>2. 提供跨集群通信能力的 Overlay CNI</h2><p>在无法使用 Underlay 网络的情况下，一些特定的 CNI 在 Overlay 层面也实现了跨集群。例如 <a href="https://cilium.io/use-cases/cluster-mesh/">Cilium Cluster Mesh</a>, <a href="https://antrea.io/docs/v2.0.0/docs/multicluster/quick-start/">Antrea Multi-Cluster</a> 和 <a href="https://kubeovn.github.io/docs/stable/en/advance/with-ovn-ic/">Kube-OVN with ovn-ic</a>。</p><p><strong>基本原理</strong>：</p><p>这些 CNI 的实现方案其实大体一致，通过选择一组集群内的节点作为网关节点，然后网关节点之间建立隧道，跨集群流量通过网关节点转发。</p><p><strong>优点</strong>：</p><ul><li>CNI 自包含跨集群功能，无需额外组件支持。</li></ul><p><strong>局限性</strong>：</p><ul><li>依赖特定 CNI，无法实现不同 CNI 集群之间的通信。</li><li>无法处理 CIDR 重叠的情况，需要提前规划好网段。</li><li>除了 Cilium 实现的比较完整，把跨集群的服务发现和网络策略都实现了，其他的仅实现基础容器网络通信。</li><li>一次性打通所有集群的容器网络，缺乏细粒度的控制。</li></ul><h2 id="3-Submariner"><a href="#3-Submariner" class="headerlink" title="3. Submariner"></a>3. Submariner</h2><p>由于跨集群网络互通存在着通用的需求，在实现上也存在着类似，各个 CNI 其实存在着一些重复造轮子的工作。<a href="https://submariner.io/">Submariner</a> 作为一款 CNI 无关的跨集群网络插件提供了一种通用的能力，能将不同 CNI 的集群组成一个网络达到互通。Submariner 最早由 Rancher 的工程师创建，现在已经是 CNCF Sandbox 项目了，目前看红帽的工程师也在积极参与这个项目。</p><p><strong>基本原理</strong>：</p><p>Submariner 选择集群内的网关节点，网关节点通过 VXLAN 通信。跨集群流量通过 VXLAN 传输。Submariner 依赖 CNI 将 egress 流量先发送到宿主机的网络内，然后再进行转发。此外，Submariner 部署了一组 CoreDNS 实现跨集群服务发现，并使用 Globalnet Controller 解决 CIDR 重叠问题。</p><p><strong>优点</strong>：</p><ul><li>一定程度上 CNI 无关，可以连接不同 CNI 的集群。</li><li>实现了跨集群服务发现，支持 Service 和域名解析。</li><li>支持 CIDR 重叠的集群通信，避免了集群部署后想互联却发现 IP 冲突的尴尬。</li></ul><p><strong>局限性</strong>：</p><ul><li>并不是所有的 CNI 都可用，如果像 macvlan 或者 cilium 短路这种宿主机看不到流量的情况，就没有办法拦截流量到自己的隧道了。</li><li>Gateway 目前是主备模式，没法横向负载均衡，在大流量的场景下可能存在性能瓶颈。</li><li>一次性打通所有集群的容器网络，缺乏细粒度的控制。</li></ul><h2 id="4-Skupper"><a href="#4-Skupper" class="headerlink" title="4. Skupper"></a>4. Skupper</h2><p><a href="https://skupper.io/index.html">Skupper</a> 是我认为几个方案里最有意思的，它能够按需进行 Service 层面的网络打通，避免了一次完全打通的控制问题。而且很创新的使用了七层消息队列的方式来实现，可以说是对底层网络和 CNI 完全无依赖，上手也十分简单。目前 Skupper 看贡献者主要是红帽的工程师。</p><p><strong>基本原理</strong>：</p><p>和上述几个方案使用隧道将容器 IP 直接打通不同，Skupper 提出了一个 VAN（Virtual Application Networks）的概念，要在 7 层将网络打通。简单说就是不把 IP 直接打通而是把 Service 拉通，概念上和 ServiceExporter，ServiceImporter 类似，但是这个项目启动的比较早，当时还没有这些社区提出来的概念，在当时应该算个很创新的想法了</p><p>在实现上 Skupper 也另辟蹊径，用的是个消息队列的实现，多个集群之间组成了一个大的消息队列，跨集群通信的数据包发送到这个消息队列里。另一端再去消费这个数据包。思路上其实类似反向代理，但用消息队列来实现也是个很开脑洞的想法。把 Service 变成了一个消息的订阅点来提供消费，这样就可以按需的在服务端和客户端之间建立一个消息队列，通过消息队列的概念去管理和控制这个消息通路。</p><p><strong>优点</strong>：</p><ul><li>CNI 兼容性好，完全不依赖 CNI 的行为在应用层进行数据包的打通</li><li>上手简单，不需要太复杂的前期网络规划，也没有 CIDR 不重叠要求。提供了 CLI 方便临时测试和快速演示</li><li>并不是容器网络互通而是按需将 Service 打通，可以做到细粒度的控制，对底层要求也更低</li></ul><p><strong>局限性</strong>：</p><ul><li>文档介绍目前只支持 TCP 协议，UDP 和更底层的协议比如 ICMP 会有问题</li><li>由于是通过消息队列转发消息，IP 信息会丢失</li><li>通过消息队列转发的思路还是有些奇怪，在性能方面比如延迟和吞吐量上可能会有一些损失</li><li>而且 TCP 本身是有状态的，能否完全转换成消息队列的形式，兼容性如何是个疑问</li></ul><h2 id="5-KubeSlice"><a href="#5-KubeSlice" class="headerlink" title="5. KubeSlice"></a>5. KubeSlice</h2><p><a href="https://kubeslice.io/documentation/open-source/1.3.0">KubeSlice</a> 是刚刚进入 CNCF Sandbox 的一个项目。上述的方案基于隧道做的没法做到细粒度控制和 CNI 完全兼容，基于应用层做的又没法做到网络协议完全兼容。KubeSlice 提供了一个新的思路，尝试同时解决这两个问题。</p><p><strong>基本原理</strong>：</p><p>KubeSlice 最底层的思路十分简单粗暴，就是按需给 Pod 动态插入一块网卡，在这个网卡上面做跨集群的 Overlay，然后再在这个 Overlay 网络上面实现服务发现，和网络策略。用户可以按需的将跨集群的几个 Namespace 或者 某几个 Pod 组成一个网络，在可以使实现灵活的细粒度管控基础上由于走的是基于网卡的二层网络，也实现了网络协议的最大兼容性。</p><p><strong>优点</strong>：</p><ul><li>CNI 兼容性好，由于是额外插入了一块网卡完全不关心原先的 CNI 是什么，并且是额外的网络也不用担心原先网络地址是否有冲突的问题</li><li>网络协议兼容性好，同样是由于额外有了一块网卡做流量转发，所有的网络协议都是兼容的。</li><li>灵活度高，KubeSlice 提供了命令行工具可以动态的创建和加入网络，用户可以选择按应用的需求创建多个跨集群的虚拟网络</li><li>功能完善，按照文档的说法，该项目在额外的这个 Overlay 网络上实现了服务发现，DNS，QoS，Networkpolicy 和监控，可以说把各个方面都覆盖到了</li></ul><p><strong>局限性</strong>：</p><ul><li>由于本质上是双网卡，应用侧需要感知这个事情选择合适的网络，可能会涉及到应用改造</li><li>由于跨集群走的是另一个网卡不是 Pod 主网卡的 IP，对监控和追踪等外部系统可能涉及到改造</li><li>目前看文档应该是一个内部的项目进行开源，文档里很多使用方式和 API 介绍的不是特别清楚，需要对着 reference 慢慢猜每个参数到底是什么意思。功能上看上去确实比较全，但能让外部用户很好的使用的话文档还有很多需要完善的地方。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在 Kubernetes 多集群环境中实现高效通信有多种方案可供选择。每种方案都有其优点和局限性，用户可以根据具体需求和环境选择合适的方案。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;随着企业的业务规模不断扩大，Kubernetes 的使用也从单集群逐步扩展到多集群部署。多集群环境下，集群之间的通信成为一个重要的研究课题。本文将介绍五种跨 Kubernetes 集群通信的方案的基本原理，优点及其局限性。&lt;/p&gt;
&lt;h2 id=&quot;1-Underlay-网络</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/tags/kubernetes/"/>
    
    <category term="network" scheme="http://oilbeater.com/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>k8gb: 云原生最佳开源 GSLB 方案</title>
    <link href="http://oilbeater.com/2024/04/18/k8gb-best-cloudnative-gslb/"/>
    <id>http://oilbeater.com/2024/04/18/k8gb-best-cloudnative-gslb/</id>
    <published>2024-04-18T10:01:10.000Z</published>
    <updated>2025-08-11T02:24:41.736Z</updated>
    
    <content type="html"><![CDATA[<p>如何将流量在多个 Kubernetes 集群之间进行负载均衡，并做到自动的灾备切换一直是一个让人头疼的问题。我们之前调研了公有云，<a href="https://github.com/karmada-io/multi-cluster-ingress-nginx">Karmada Ingress</a>，自己也做了一些手动 DNS 的方案，但这些方案在成本，通用性，灵活度以及自动化程度上都有所欠缺。直到调研到了 <a href="https://www.k8gb.io/">k8gb</a> 这个由南非银行 <a href="https://www.absa.africa/">Absa Group</a> 为了做金融级别的多活而启动的一个项目。k8gb 巧妙的利用了 DNS 的各种协议完成了一个通用且高度自动化的 GSLB 方案，看完后我就再也不想用其他的方案了。这篇博客会简单介绍一下其他几个方案各自存在的问题，以及 k8gb 究竟是如何巧妙的利用 DNS 来实现的 GSLB。</p><h2 id="什么是-GSLB"><a href="#什么是-GSLB" class="headerlink" title="什么是 GSLB"></a>什么是 GSLB</h2><p>GSLB（Global Service Load Balancer）是相对于单集群负载均衡的一个概念，单集群负载均衡主要作为一个集群的入口将流量分发到集群内部，而 GSLB 通常作为再上一层多个集群的流量入口，进行流量负载均衡和故障处理。一方面 GSLB 可以设置一些地理亲和的规则达到流量就近转发提升整体的性能，另一方面当某个集群出现问题后可以自动将流量切换到正常集群，减少单个集群故障对用户的影响。</p><h2 id="其他方案的问题"><a href="#其他方案的问题" class="headerlink" title="其他方案的问题"></a>其他方案的问题</h2><h3 id="商用负载均衡"><a href="#商用负载均衡" class="headerlink" title="商用负载均衡"></a>商用负载均衡</h3><p>GSLB 并不是一个新出的概念，所以不少商业公司都在这方面有很成熟的产品，例如 <a href="https://www.f5.com/solutions/use-cases/global-server-load-balancing-gslb">F5 GSLB</a>。这类产品通常有以下几个缺点：</p><ol><li>没有很好的和云原生对接，通常需要在 Kubernetes 集群外独立部署专有的软硬件，无法做到统一管理。</li><li>成本高，且有厂商锁定的风险。</li></ol><h3 id="公有云全局负载均衡"><a href="#公有云全局负载均衡" class="headerlink" title="公有云全局负载均衡"></a>公有云全局负载均衡</h3><p>公有云为了解决流量多地域分发会提供多集群负载均衡的产品，例如 AWS 的 <a href="https://aws.amazon.com/global-accelerator/">Global Accelerator</a> 和 GCP 的 <a href="https://cloud.google.com/load-balancing/docs/https">External Application Load Balancer</a>。GCP 上甚至还有一组自定义的 <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress">Multi Cluster Ingress</a> 资源，可以很好的和 Kubernetes 里的 Ingress 进行对接。但是他们也有一下几个问题：</p><ol><li>虽然是多集群负载均衡，但是多个集群必须是同一个公有云，不能在多云间进行流量调度。</li><li>私有云无法使用这个方案。</li></ol><h3 id="Karmada-Multi-Cluster-Ingress"><a href="#Karmada-Multi-Cluster-Ingress" class="headerlink" title="Karmada Multi  Cluster Ingress"></a>Karmada Multi  Cluster Ingress</h3><p>Karmada 是一个多集群的编排工具，也提供了自己的多集群流量调度方案 <a href="https://karmada.io/docs/userguide/service/multi-cluster-ingress/">Karmada Multi Cluster Ingress</a>。该方案通过在某个集群内部署一个由 Karmada 社区提供的 ingress-nginx 以及定义 <code>MultiClusterIngress</code> 来完成多集群流量调度，但这个方案有以下几个问题：</p><ol><li>依赖多集群间容器网络打通，和 ServiceImporter ServiceExporter 等 CRD，整体要求比较高。</li><li>需要额外再去管理这个提供 GSLB 服务的 ingress-nginx 实例，该部署在哪，该部署多少，怎么分配都是运维期间需要考虑的问题。</li><li>这个社区改造后的 <a href="https://github.com/karmada-io/multi-cluster-ingress-nginx">multi-cluster-ingress-nginx</a> 近两年基本没有什么代码提交，是否可用会让人有担忧。</li></ol><h3 id="简单的-DNS-方案"><a href="#简单的-DNS-方案" class="headerlink" title="简单的 DNS 方案"></a>简单的 DNS 方案</h3><p>如果手动攒出来一个简单的基于 DNS 的方案其实也是可以的，大部分 DNS 厂商都提供了健康检查的功能，因此我们可以将多个集群的出口 IP 地址加入到 DNS 的解析记录里，同时配置健康检查来做故障切换。但是这个简单的方案在规模扩大后就会有一些明显的限制：</p><ol><li>无法很好的自动化，一个集群下可能有多个不同的域名和多个不同的 Ingress IP 的组合，手动去管理他们的映射关系会随着规模增加变得难以维护。</li><li>DNS 厂商的健康检查通常是基于 TCP 和 ICMP 的，因此如果一个集群的出口完全挂掉这种故障是可以检测到并进行切换的。但是如果是局部故障就无法探测，例如多个 Ingress 复用一个 ingress-controller 并通过域名进行流量转发的情况下，如果其中一个服务的后端实例全部异常了，但是 ingress-controller 上的其他服务正常，那么健康检查还是会正常通过，并没有办法把流量切换到另一个集群。</li><li>DNS 本身存在各级缓存，更新时间可能较长。</li><li>DNS 健康检查本身也是个厂商提供的能力，并不能保证所有厂商都能提供这个能力，尤其是在私有云的场景。</li></ol><h2 id="k8gb-的解决方案"><a href="#k8gb-的解决方案" class="headerlink" title="k8gb 的解决方案"></a>k8gb 的解决方案</h2><p>k8gb 的解决方案其实也是用 DNS，但是通过自己的一系列巧妙的设计，解决了上面提到的简单 DNS 方案的一系列缺陷。</p><p>简单 DNS 方案的问题本质是没有和 Kubernetes 进行很好的对接，Kubernetes 内的一些动态信息，例如新增的 Ingress，新增的域名，服务的健康状态没法很好的同步到上游 DNS 服务器，而上游 DNS 服务器简单的健康检查也没办法应付 Kubernetes 里这种复杂的变化。因此 k8gb 最核心的一个变化就是上游的 DNS 记录不再是通过 A 记录或者 AAAA 记录指向集群的一个出口地址，而是 forward 到集群内的一个自己配置的 CoreDNS 进行 DNS 解析，将真正复杂的 DNS 逻辑下沉到集群里来自己控制。这样上游 DNS 只需要做简单的代理，不再需要配置健康检查，也不再需要动态的调整多个地址映射。</p><p>调整后用户请求 DNS 的流程如下图所示：</p><ul><li><img src="https://www.k8gb.io/docs/images/gslb-basic-multi-cluster.svg" alt="K8GB multi-cluster interoperability"></li></ul><ol><li>用户向外部 DNS 服务商请求一个域名的 IP 记录。</li><li>外部 DNS 将这个请求代理发送给集群内由 k8gb 管控的一个 CoreDNS。</li><li>k8gb 根据 Ingress Spec 里的域名，Ingress Status 里的 Ingress IP 以及集群内对应后端 Pod 的健康状态，负载均衡策略等信息分析出一个可用的 Ingress IP 返回给用户。</li><li>用户通过这个 IP 就可以直接访问到对应的 Ingress Controller。</li><li>当某一个集群的 k8gb 管控的 CoreDNS 出问题时由于上游 DNS 会同时将 DNS 请求代理到多个集群，另一个集群也可以返回自己的 Ingress IP，用户端可以通过多个返回的可用 IP 选择一个进行访问。</li></ol><p>这种方式管理员只需要在上游 DNS 注册几个域名后缀并代理到每个集群的 CoreDNS 就可以了，k8gb 本身也提供了自动化的能力，只要配好证书可以自动分析 Ingress 内使用的域名自动注册给上游 DNS，大幅简化了管理员的操作。</p><p>整个流程中还有一个特殊的点要注意，就是每个集群自己的 CoreDNS 不能只记录本集群 Ingress IP 的地址，还需要记录其他集群同一个 Ingress 的 Ingress IP 地址。因为如果只记录本集群的，当本集群对应服务的 Pod 都 Not Ready 时，CoreDNS 会返回 NXDomain 如果客户端收到了这个返回就会按照域名无法解析处理，此时另一个集群服务其实是可以正常提供服务的。因此 k8gb 还需要同步所有集群同一个域名 Ingress 对应的 Ingress IP 和它的健康状态。</p><p>众所周知多集群之间的数据同步也是一个世界难题，但是 k8gb 同样通过 DNS 巧妙的实现了数据的同步。</p><p>同步的流程图如下所示：</p><p><img src="https://www.k8gb.io/docs/images/k8gb-multi-cluster-interoperabililty.svg" alt="k8gb multi-cluster-interoperability"></p><p>大致的思路是每个集群的 k8gb 会把自己的 CoreDNS 的 Ingress IP 同样注册到上游 DNS，这样每个集群就可以直接访问另一个集群的 CoreDNS 了。然后每个集群内的 CoreDNS 再按照一个特殊的域名格式比如 <code>localtargets-app.cloud.example.com</code> 来保存本集群内 <code>app.cloud.example.com</code> Ingress 的 Ingress IP 并维护其健康状态。这样每个集群就都可以通过这个特殊的域名来获得其他集群这个域名对应的 Ingress IP 然后加入到自己的返回结果里，实现了域名解析的多集群同步。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>k8gb 作为一款开源的 GSLB 实现了和 Kubernetes 的无缝对接，能够很好的对跨集群的域名和流量进行自动化管理，并且对外部的依赖降到了只需要一个添加 DNS 记录的 API，真正实现了一套可以多云统一的 GSLB 方案。尽管目前这个项目还没有那么火热，但在我心里它已经是这个领域内的最佳方案了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;如何将流量在多个 Kubernetes 集群之间进行负载均衡，并做到自动的灾备切换一直是一个让人头疼的问题。我们之前调研了公有云，&lt;a href=&quot;https://github.com/karmada-io/multi-cluster-ingress-nginx&quot;&gt;Karm</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/tags/kubernetes/"/>
    
    <category term="network" scheme="http://oilbeater.com/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>如何轻松将 AI 生成内容整合入 Logseq 笔记</title>
    <link href="http://oilbeater.com/2024/03/21/sentence-to-logseq/"/>
    <id>http://oilbeater.com/2024/03/21/sentence-to-logseq/</id>
    <published>2024-03-21T10:39:37.000Z</published>
    <updated>2025-08-11T02:24:41.736Z</updated>
    
    <content type="html"><![CDATA[<p>我的日常生活已经融入了很多 AI 的协助，比如翻译一段文字、解释某个名词、回答一个问题等等。但是频繁使用后发现了另一个问题就是这些 AI 的产出对我的知识来说是个有益的补充，可是使用的时候通常是一次性的解决问题，并没有把这些内容沉淀下来。当需要把他们收集到笔记里的时候还要再回去整理，很是不方便。想象一下，如果这些临时查询的宝贵信息可以自动保存到自己的笔记系统中，就再也不用担心丢失重要的知识片段了。于是就萌生了做个工作流把这些 AI 生成的内容自动存到我的笔记系统里，然后再定期回顾。这里会介绍我调研的一些工具的局限，以及如何用了一个简单的框架把 AI 生成的内容自动导入到 logseq，并做成 flashcard。</p><h1 id="问题分解和初步调研"><a href="#问题分解和初步调研" class="headerlink" title="问题分解和初步调研"></a>问题分解和初步调研</h1><p>这里以我一个日常翻译的场景为例：当遇到读不懂的句子是我希望能够划词翻译，然后让 AI 帮我分析这个句子里疑难的单词，最后把这些内容以 flashcard 的格式保存到 logseq 里，这样借助手机端的同步我就可以在碎片时间来复习这个句子了。所以问题大致就是三个，1. 取出划线的文本 2. 编写一个 prompt 生成我想要的内容 3. 保存到 logseq。</p><p>前两步其实很多工具都已经集成了包括 Raycast AI 和 OpenAI-translator，但是第他们都没提供把前两步的结果导出到第三方的扩展接口。Raycast 理论上可以通过自己的插件体系来完成这件事，但是我这边不太熟悉 JS，并且也不想太依赖 Raycast AI，毕竟后面不打算续费了。于是决定自己写个 python 脚本做这些事，然后通过 Raycast 的 script command 和快捷键直接调用脚本，这样理论上也可以一个快捷键完成整个工作流。</p><h1 id="如何获取划选的文本"><a href="#如何获取划选的文本" class="headerlink" title="如何获取划选的文本"></a>如何获取划选的文本</h1><p>第一个难题就是获取划选的文本，看上去是个操作系统级别的操作，并不是很好实现。直到我看到了一个查词软件的实现我才恍然大悟：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/raycast/script-commands/blob/master/commands/apps/dictionary/look-up-in-dictionary.applescript</span></span><br><span class="line">tell application <span class="string">&quot;System Events&quot;</span></span><br><span class="line">    keystroke <span class="string">&quot;c&quot;</span> using &#123;<span class="built_in">command</span> down&#125;</span><br><span class="line">    delay 0.1</span><br><span class="line">    <span class="keyword">do</span> shell script <span class="string">&quot;open dict://&quot;</span> &amp; the clipboard</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>这个脚本本质上是模拟键盘摁下了 <code>cmd + v</code>，然后直接读剪切板去了，就这么绕过了获取选中文本的问题。考虑到我在用 Raycast 的时候经常碰到划选的文本识别错误，已经养成了 <code>cmd + v</code> 的习惯，那就不用费劲心思找获取划选的文本的方法了，直接读剪贴板就完了。</p><h1 id="调用-AI"><a href="#调用-AI" class="headerlink" title="调用 AI"></a>调用 AI</h1><p>这块基本经轻车熟路了，关键的是设置一个合适的 prompt，在翻译的同时生成一个句子的教学指南，并尽可能的按照 logseq 的格式来输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">message_text = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a university English teacher, below is a paragraph in English. &quot;</span> +</span><br><span class="line">        <span class="string">&quot;Please first translate it into Chinese. Then extract difficult words and phrases from the source paragraph, sort them in descending order of importance choose only the top 3 output them with explain of their usage to me in detail from a linguistic perspective.&quot;</span> +</span><br><span class="line">        <span class="string">&quot;The overall output should look like this: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- &#123;The Chinese Translation&#125; \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- Explanation: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;  - &#123;word or phrase&#125;: &#123;explanation&#125;\n&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>:<span class="string">&quot;user&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: content</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h1 id="输出到-logseq"><a href="#输出到-logseq" class="headerlink" title="输出到 logseq"></a>输出到 logseq</h1><p>在输出到 logseq 这一步的时候本来是打算调用 logseq 的 API，logseq 的开发者模式提供了一个本地的 HTTP 服务可以通过 HTTP 去调用。但是当我看了他们的 API 文档后差点给整崩溃了，所有的操作都要用 id，page id 又没办法去直接索引，要 getAll 后自己过滤。appendBlock 也不允许插入带层级的 Block，要串好几个 API 才能完成一个简单的操作。</p><p>崩溃的时候转念一想，logseq 不就是一堆 markdown 的渲染器么，既然 API 那么难用我直接去写文件不就好了，于是一组复杂的 API 调用变成了轻松愉快的文件 append 操作。这样既绕开了 logseq API 的限制，甚至有可能接入其他基于 markdown 的笔系统。</p><p>最后完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyperclip</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> AzureOpenAI</span><br><span class="line"></span><br><span class="line">content = pyperclip.paste()</span><br><span class="line"></span><br><span class="line">azure_endpoint = <span class="string">&quot;YOUR AZURE ENDPOINT&quot;</span></span><br><span class="line">api_key = <span class="string">&quot;YOUR API KEY&quot;</span></span><br><span class="line">model = <span class="string">&quot;YOUR MODEL NAME&quot;</span></span><br><span class="line">logseq_path = <span class="string">&quot;YOUR LOGSEQ PAGE FILE PATH&quot;</span></span><br><span class="line"></span><br><span class="line">client = AzureOpenAI(</span><br><span class="line">  azure_endpoint = azure_endpoint,</span><br><span class="line">  api_key=api_key,</span><br><span class="line">  api_version=<span class="string">&quot;2024-02-15-preview&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">message_text = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a university English teacher, below is a paragraph in English. &quot;</span> +</span><br><span class="line">        <span class="string">&quot;Please first translate it into Chinese. Then extract difficult words and phrases from the source paragraph, sort them in descending order of importance choose only the top 3 output them with explain of their usage to me in detail from a linguistic perspective.&quot;</span> +</span><br><span class="line">        <span class="string">&quot;The overall output should look like this: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- &#123;The Chinese Translation&#125; \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- Explanation: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;  - &#123;word or phrase&#125;: &#123;explanation&#125;\n&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>:<span class="string">&quot;user&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: content</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">completion = client.chat.completions.create(</span><br><span class="line">  model=model, <span class="comment"># model = &quot;deployment_name&quot;</span></span><br><span class="line">  messages = message_text,</span><br><span class="line">  temperature=<span class="number">0.7</span>,</span><br><span class="line">  max_tokens=<span class="number">500</span>,</span><br><span class="line">  top_p=<span class="number">0.95</span>,</span><br><span class="line">  frequency_penalty=<span class="number">0</span>,</span><br><span class="line">  presence_penalty=<span class="number">0</span>,</span><br><span class="line">  stop=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> completion.choices[<span class="number">0</span>].message.content == <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;No response&quot;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">response = completion.choices[<span class="number">0</span>].message.content</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(logseq_path, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&quot;\n- &quot;</span> + content.rstrip() + <span class="string">&quot; #card #English&quot;</span> + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> response.splitlines():</span><br><span class="line">        <span class="keyword">if</span> line.startswith(<span class="string">&quot;```&quot;</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> line.rstrip() == <span class="string">&quot;&quot;</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line.startswith(<span class="string">&quot;- &quot;</span>):</span><br><span class="line">            line = <span class="string">&quot;- &quot;</span> + line.rstrip()</span><br><span class="line">        f.write(<span class="string">&quot;  &quot;</span> + line + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure><p>后需要做的就是在 Raycast 里给这个脚本设置一个快捷键，这样下次碰到不会的句子，选中复制后就可以通过快捷键完成翻译，难点提取，和生成 flashcard 的整个工作流。</p><p>最后生成的 flashcard 效果大概如下：</p><p><img src="/../images/logseq-card.png" alt="alt text"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过将 AI 生成的内容自动化集成到 Logseq 笔记中，我们不仅提高了信息管理的效率，还优化了学习和工作流程。通过一个简单的脚本每个人都可以轻松地将这种强大的技术整合到日常生活中，实现知识的积累和复习。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我的日常生活已经融入了很多 AI 的协助，比如翻译一段文字、解释某个名词、回答一个问题等等。但是频繁使用后发现了另一个问题就是这些 AI 的产出对我的知识来说是个有益的补充，可是使用的时候通常是一次性的解决问题，并没有把这些内容沉淀下来。当需要把他们收集到笔记里的时候还要再</summary>
      
    
    
    
    
    <category term="logseq" scheme="http://oilbeater.com/tags/logseq/"/>
    
    <category term="ai" scheme="http://oilbeater.com/tags/ai/"/>
    
    <category term="learning" scheme="http://oilbeater.com/tags/learning/"/>
    
  </entry>
  
  <entry>
    <title>实现 VS Code Remote SSH 下的自动关机</title>
    <link href="http://oilbeater.com/2024/03/13/shutdown-remote-ssh-server/"/>
    <id>http://oilbeater.com/2024/03/13/shutdown-remote-ssh-server/</id>
    <published>2024-03-13T10:09:26.000Z</published>
    <updated>2025-08-11T02:24:41.736Z</updated>
    
    <content type="html"><![CDATA[<p>我现在所有的开发环境都转移到了 GCP 的 Spot Instance 实例，然后用 VS Code Remote SSH 插件进行连接。这种方式的好处是 GCP 可以按秒计费，即使开了很高规格的机器只要及时关机费用也是可控的，缺点是如果中间忘了关机赶上过节那费用就烧开了。再被烧掉了 10 多美元后，痛定思痛决定找个能在 VS Code 空闲时自动关机的方法。过程中为了能够在 Docker 容器内执行 shutdown 命令还搞了一些黑魔法。</p><h1 id="如何判断空闲"><a href="#如何判断空闲" class="headerlink" title="如何判断空闲"></a>如何判断空闲</h1><p>这个功能其实类似 CodeSpace 里的 idle timeout，但是 Remote SSH 这个插件并没有暴露这个功能，所以只能自己实现。其实主要的难点在于如何在 Server 端判断空闲，找了一圈没在 VS Code 里看到有暴露的接口，于是就想能不能跳出 VS Code 看看有什么方法能够简洁的判断空闲发生。</p><p>最直接的想法就是去看 SSH 的连接，因为 Remote SSH 也是通过 SSH 连接上来的，如果当前机器没有存活的 SSH 连接，那么就可以认为是空闲直接关机了。但是问题是 Remote SSH 的连接超时时间会特别长，搜索了一些 Issue 有说是 4 个小时的，我也尝试了直接关闭 VS Code 的客户端，发现 Server 端的 SSH 连接也一直没有消失。如果在 Server 端设置 SSH 超时，Client 那边很快就会重连，连接数量也不会减少。</p><p>既然没法通过直接看 SSH 连接数量来判断，那么就进一步去看能不能判断已有的 SSH 连接是不是已经没有流量了。用 <code>tcpdump</code> 抓包看了一下，即使客户端没有任何交互，还是会有 1s 一次的 TCP 心跳数据包，所以也不能有流量为 0 来判断。不过观察下来心跳数据包的大小都是固定的，都是 44 字节，正好可以根据这个特征来判断，如果一段时间内 SSH 端口没有大于 44 字节的数据包就可以判断空闲了。</p><p>于是第一版本的脚本就出来了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span></span><br><span class="line">  <span class="keyword">if</span> [ -f /root/dump ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">timeout</span> 1m tcpdump -nn -i ens4 tcp and port 22 and greater 50 -w /root/dump</span><br><span class="line"></span><br><span class="line">  line_count=$(<span class="built_in">wc</span> -l &lt; /root/dump)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> [ <span class="variable">$line_count</span> -gt 0 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">    shutdown -h now</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>这样就实现了 1 分钟内 SSH 端口没有非心跳数据包就关机的功能，下一步就是要让这个脚本自动运行了。</p><h1 id="Docker-黑魔法"><a href="#Docker-黑魔法" class="headerlink" title="Docker 黑魔法"></a>Docker 黑魔法</h1><p>在把脚本打包成 Docker 镜像时发现了一个有趣的问题，那就是所有的 Base Image 里都没有 <code>shutdown</code> 命令，<code>shutdown</code> 命令也没法很容易的安装。为了能够执行主机上的 <code>shutdown</code> 命令，就需要在 Docker 容器里切换到主机的命名空间，再去关机。所以需要把之前的脚本稍微修改一下，包装一下 <code>shutdown</code> 命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsenter -a -t 1 shutdown -h now</span><br></pre></td></tr></table></figure><p>这个 <code>nsenter</code> 命令的作用是选定 Pid 为 1 的进程，然后进入这个进程的所有(pid, mount, network) Namespaces，这样当 Docker 运行在共享主机 Pid 模式下我们相当于就进入了主机 1 号进程的所有 Namespaces，看上去就和 SSH 到主机上一样可以执行操作了。这样只要再用下面的命令启动容器，就可以不担心忘记随时关机了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name=close --pid=host --network=host --privileged --restart=always -d close:v0.0.1</span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过监控 SSH 的流量情况可以一定程度上猜测 VS Code 已经空闲了，然后再用 Docker 的一些黑魔法就可以实现自动关机了。不过整个链路的黑魔法都太多了，有没有什么简单的方式呢？</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;我现在所有的开发环境都转移到了 GCP 的 Spot Instance 实例，然后用 VS Code Remote SSH 插件进行连接。这种方式的好处是 GCP 可以按秒计费，即使开了很高规格的机器只要及时关机费用也是可控的，缺点是如果中间忘了关机赶上过节那费用就烧开了。</summary>
      
    
    
    
    
    <category term="docker" scheme="http://oilbeater.com/tags/docker/"/>
    
    <category term="vscode" scheme="http://oilbeater.com/tags/vscode/"/>
    
    <category term="tools" scheme="http://oilbeater.com/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>minikube，kind 和 k3d 大比拼</title>
    <link href="http://oilbeater.com/2024/02/22/minikube-vs-kind-vs-k3d/"/>
    <id>http://oilbeater.com/2024/02/22/minikube-vs-kind-vs-k3d/</id>
    <published>2024-02-22T14:42:22.000Z</published>
    <updated>2025-08-11T02:24:41.736Z</updated>
    
    <content type="html"><![CDATA[<p>作为云原生生态的一个开发者，开发中经常碰到的一个需求是要频繁测试应用在 Kubernetes 环境下的运行状态，在 CI 中可能还要快速测试多个不同 Kubernetes 集群的配置，例如单点，高可用，双栈，多集群等等。因此能够低成本的在本地单机环境快速创建管理 Kubernetes 集群就成了一个刚需。本文将介绍几个常见的单机 Kubernetes 管理工具 minikube, kind 和 k3d 各自的特点、使用场景以及可能的坑。</p><blockquote><p>TL;DR<br>如果你只关心快不快，那么 k3d 是最好的选择。如果你关心的是兼容性以及测试尽可能模拟真实场景，那么 minikube 是最稳妥的选择。kind 算是在这两个之间的一个平衡。</p></blockquote><ul><li><a href="#%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF%E6%AF%94%E8%BE%83">技术路线比较</a><ul><li><a href="#minikube">minikube</a></li><li><a href="#kind">kind</a></li><li><a href="#k3d">k3d</a></li></ul></li><li><a href="#%E5%90%AF%E5%8A%A8%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83">启动性能比较</a><ul><li><a href="#%E6%B5%8B%E8%AF%95%E6%96%B9%E6%B3%95">测试方法</a></li><li><a href="#%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C">测试结果</a></li></ul></li><li><a href="#%E6%80%BB%E7%BB%93">总结</a></li></ul><h1 id="技术路线比较"><a href="#技术路线比较" class="headerlink" title="技术路线比较"></a>技术路线比较</h1><p>这三者大体功能是类似的，都能够完成单机管理 Kubernetes 的任务，但是由于一些历史原因和技术选项导致了一些细节和使用场景的差异。</p><h2 id="minikube"><a href="#minikube" class="headerlink" title="minikube"></a>minikube</h2><p><a href="https://minikube.sigs.k8s.io/docs/">minikube</a> 是 Kubernetes 社区最早的一款快速在本地创建 Kubernetes 的软件，也是很多老人第一次上手 Kubernetes 的工具。早期版本是通过在本机创建 VM 来模拟多节点集群，这个方案的好处是能够最大程度还原真实场景，一些操作系统级别的变化，例如不同 OS，不同内核模块都可以覆盖到。缺点就是资源消耗太大，而且在一些虚拟化环境如果没有嵌套虚拟化的支持是没办法运行的，并且启动的速度也比较慢。不过社区最近也推出了 Docker 的 Driver 这些问题都得到了比较好的解决，不过对应代价就是一些虚拟机级别的模拟就不好做了。此外 minikube 还提供了不少的 addon，比如 dashboard，nginx-ingress 等常见的社区组件都能快速的安装使用。</p><h2 id="kind"><a href="#kind" class="headerlink" title="kind"></a>kind</h2><p><a href="https://kind.sigs.k8s.io/">kind</a> 是近几年流行起来的一个本地部署 Kubernetes 的工具，他的主要特点就是用 Docker 容器模拟节点，并且基本只专注在 Kubernetes 标准部署这一个事情上，其他社区组件都需要额外自己去安装。目前 Kubernetes 本身的 CI 也是通过 kind 来跑的。优点就是启动速度很快，熟悉 Docker 的人用起来也很顺手。缺点是用了容器模拟缺乏操作系统级别的隔离，而且和宿主机共享内核，一些操作系统相关的测试就不好测试了。我之前在测一个内核模块的时候就因为宿主机加了一些 netfilter 功能，结果 kind 里的 Kubernetes 集群挂了。</p><h2 id="k3d"><a href="#k3d" class="headerlink" title="k3d"></a>k3d</h2><p><a href="https://k3d.io/stable/">k3d</a> 是一个超轻量的本地部署 Kubernetes 工具，他的大体思路和 kind 类似，都是通过 Docker 来模拟节点，主要区别在于部署的不是个标准 Kubernetes 而是一个轻量级的 <a href="https://k3s.io/">k3s</a>，所以他的大部分优缺点也来自于下面这个 k3s。优点就是安装极致的快，你先别管对不对，你就问快不快吧。缺点主要来自于为了速度做出的一些牺牲，比如镜像用的是个超精简的操作系统，连 glibc 都没有，因此一些要在操作系统层面的操作都会无比困难。还有就是他的安装方式和常见的 kubeadm 也不一样，Kubernetes 组件都不是容器启动的，如果依赖标准部署的一些特性可能都会比较困难。</p><h1 id="启动性能比较"><a href="#启动性能比较" class="headerlink" title="启动性能比较"></a>启动性能比较</h1><p>minikube 社区有一些<a href="https://minikube.sigs.k8s.io/docs/benchmarks/timetok8s/v1.32.0/">性能测试报告</a>，正好对比的就是本文关注的三款软件的启动速度，不过我更关注的是其他的一些方面，比如镜像大小，内存占用以及最小化安装的启动时间，所以还是再做了一组测试。</p><h2 id="测试方法"><a href="#测试方法" class="headerlink" title="测试方法"></a>测试方法</h2><p>由于三款软件都是一行命令就可以启动，测试还是比较方便的，主要注意以下几个点：</p><ol><li>minikube 采用 Docker Driver，因为真要测启动速度还用虚拟机的 Driver 就没什么意义了。</li><li>所有测试都是镜像已经下载到本地的结果，不会涉及网络下载时间。</li><li>测试的每个软件都是当前的最新版，但是他们支持的 Kubernetes 版本不一致，不是很严谨，但是定性分析应该够了。</li><li>都只启动最基本的组件，不安装其他插件，但是基础 CNI 和 CoreDNS 以及 CSI 都是有的，保证应用的基本运行。</li><li>使用 <code>docker image</code> 命令查看镜像大小，使用 <code>docker stat</code> 查看内存用量。</li></ol><p>测试的命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#minikube</span></span><br><span class="line"><span class="keyword">time</span> minikube start --driver=docker --force</span><br><span class="line"></span><br><span class="line"><span class="comment">#kind</span></span><br><span class="line"><span class="keyword">time</span> kind create cluster</span><br><span class="line"></span><br><span class="line"><span class="comment">#k3d</span></span><br><span class="line"><span class="keyword">time</span> k3d cluster create mycluster --k3s-arg <span class="string">&#x27;--disable=traefik,metrics-server@server:*&#x27;</span> --no-lb</span><br></pre></td></tr></table></figure><h2 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h2><table><thead><tr><th>名称</th><th>软件版本</th><th>Kubernetes 版本</th><th>镜像大小</th><th>启动时间</th><th>内存消耗</th></tr></thead><tbody><tr><td>minikube</td><td>v1.32.0</td><td>v1.28.3</td><td>1.2GB</td><td>29s</td><td>536MiB</td></tr><tr><td>kind</td><td>v0.22</td><td>v1.29.2</td><td>956MB</td><td>20s</td><td>463MiB</td></tr><tr><td>k3d</td><td>v5.6.0</td><td>v1.27.4</td><td>263MB</td><td>7s</td><td>423MiB</td></tr></tbody></table><p>可见单从启动性能这个指标，k3d 在镜像大小，启动时间和内存消耗几个方面都有比较大的优势，对于用 Github 免费 Action 跑 CI 的穷人还是很有吸引力的。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>如果快和更少的资源占用是最重要的目标，那么 k3d 相当合适，如果要测试需要操作系统级别隔离的功能，那么 minikube 的虚拟机 Driver 是唯一的选择，其他场景下 kind 会在兼容和性能之间比较平衡。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;作为云原生生态的一个开发者，开发中经常碰到的一个需求是要频繁测试应用在 Kubernetes 环境下的运行状态，在 CI 中可能还要快速测试多个不同 Kubernetes 集群的配置，例如单点，高可用，双栈，多集群等等。因此能够低成本的在本地单机环境快速创建管理 Kuber</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/tags/kubernetes/"/>
    
    <category term="ci" scheme="http://oilbeater.com/tags/ci/"/>
    
  </entry>
  
  <entry>
    <title>Golang 中预分配 slice 内存对性能的影响（续）</title>
    <link href="http://oilbeater.com/2024/01/09/alloc-slice-for-golang-2-md/"/>
    <id>http://oilbeater.com/2024/01/09/alloc-slice-for-golang-2-md/</id>
    <published>2024-01-09T08:16:22.000Z</published>
    <updated>2025-08-11T02:24:41.736Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#%E5%9F%BA%E7%A1%80%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95">基础性能测试</a></li><li><a href="#%E6%95%B4%E4%B8%AA-slice-append">整个 Slice Append</a></li><li><a href="#%E5%A4%8D%E7%94%A8-slice">复用 Slice</a></li><li><a href="#syncpool">sync.Pool</a></li><li><a href="#bytebufferpool">bytebufferpool</a></li><li><a href="#%E6%80%BB%E7%BB%93">总结</a></li></ul><p>之前写了一篇 <a href="https://oilbeater.com/2023/07/19/pre-alloc-slice-for-golang/">Golang 中预分配 slice 内存对性能的影响</a>，探讨了一下在 Slice 中预分配内存对性能的影响，之前考虑的场景比较简单，最近又做了一些其他测试，补充一下进一步的信息。包括整个 Slice append，sync.Pool 对性能的影响。</p><h1 id="基础性能测试"><a href="#基础性能测试" class="headerlink" title="基础性能测试"></a>基础性能测试</h1><p>最初的 BenchMark 代码，只考虑了 Slice 是否初始化分配空间的情况，具体的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> prealloc_test</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;sync&quot;</span></span><br><span class="line">    <span class="string">&quot;testing&quot;</span></span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">var</span> length = <span class="number">1024</span></span><br><span class="line"><span class="keyword">var</span> testtext = <span class="built_in">make</span>([]<span class="type">byte</span>, length, length)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocateByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">        <span class="keyword">var</span> init []<span class="type">byte</span></span><br><span class="line">        <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">            init = <span class="built_in">append</span>(init, testtext[j])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocateByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">        <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">            init = <span class="built_in">append</span>(init, testtext[j])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        569978              2151 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          804807              1304 ns/op            1024 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>可见没有预分配的情况下多了 8 次内存分配，两个相对比可以粗略的认为 40% 的时间消耗在了这额外的 8 次内存分配。</p><p>这两个测试用例使用的是循环里逐个 append 元素，但是 Slice 还支持整个 Slice 进行 append 在这种情况下的性能差距是没有体现出来的。而且在这两个测试用例里我们其实无法知道内存分配所占的时间消耗占整个时间的占比。</p><h1 id="整个-Slice-Append"><a href="#整个-Slice-Append" class="headerlink" title="整个 Slice Append"></a>整个 Slice Append</h1><p>因此加入两个整个 Slice Append 的测试用例，观察预分配内存对性能还有没有这么大的影响。新增的用例代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">        <span class="keyword">var</span> init []<span class="type">byte</span></span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        569978              2151 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          804807              1304 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                3829890               311.5 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3968048               306.7 ns/op          1024 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>可见两个用例都只用了一次内存分配，消耗时间几乎相同，而且大幅低于逐个元素进行 append 的情况。一方面整个 Slice append，在 Slice 扩容时就知道了最终的大小没必要进行动态内存分配，降低了内存分配的开销。另一方面整个 Slice append 在实现上会进行整段复制，降低了循环的开销，性能会提升很多。</p><p>但在这里每次还是会有一次内存分配，我们依然无法确定这一次内存分配所占的整体时间比例。</p><h1 id="复用-Slice"><a href="#复用-Slice" class="headerlink" title="复用 Slice"></a>复用 Slice</h1><p>为了计算一次内存分配的消耗，我们设计一个新的测试用例，将 Slice 的创建放到循环外，循环内每次最后将 Slice 的 length 设为 0，给下次进行复用。这样在大量的测试下只会进行一次内存分配，平均下来就可以忽略不计了。具体的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate2</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">        init = init[:<span class="number">0</span>]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        514904              2171 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          761772              1333 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                4041459               320.9 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3854649               320.1 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate2-12                63147178                18.63 ns/op            0 B/op          0 allocs/op</span><br></pre></td></tr></table></figure><p>可见这次测试统计上没有内存分配了，整体消耗时间也降为了之前的 5%。因此大致可以计算出在之前的测试用例里每一次内存分配会消耗 95% 的时间，这个占比还是很惊人的。因此对于性能敏感的场景还是需要尽可能的复用对象，避免反复的对象创建的内存开销。</p><h1 id="sync-Pool"><a href="#sync-Pool" class="headerlink" title="sync.Pool"></a>sync.Pool</h1><p>简单的场景下可以像上个测试用例里一样手动的清空 Slice 在循环内进行复用，但是真实场景里对象的创建通常会发生在代码的各个地方，就需要统一的进行管理和复用了，Golang 里的 <code>sync.Pool</code> 就是做这个事情的，而且使用起来也很简单。但是内部实现还是比较复杂的，为了性能进行了大量无锁化的设计，具体实现可以参考<a href="https://unskilled.blog/posts/lets-dive-a-tour-of-sync.pool-internals/">Let’s dive: a tour of sync.Pool internals</a>。</p><p>使用 <code>sync.Pool</code> 重新设计的测试用例如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> sPool = &amp;sync.Pool&#123; </span><br><span class="line">        New: <span class="function"><span class="keyword">func</span><span class="params">()</span></span> any &#123;</span><br><span class="line">                b := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">                <span class="keyword">return</span> &amp;b</span><br><span class="line">        &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPoolByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">        b.ResetTimer()</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">                <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">                b := sPool.Get().(*[]<span class="type">byte</span>)</span><br><span class="line">                buf := *b</span><br><span class="line">                <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">                        buf = <span class="built_in">append</span>(buf, testtext[j])</span><br><span class="line">                &#125;</span><br><span class="line">                buf = buf[:<span class="number">0</span>]</span><br><span class="line">                sPool.Put(b)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPool</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">        b.ResetTimer()</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">                <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">                bufPtr := sPool.Get().(*[]<span class="type">byte</span>)</span><br><span class="line">                buf := * bufPtr</span><br><span class="line">                buf = <span class="built_in">append</span>(buf, testtext...)</span><br><span class="line">                buf = buf[:<span class="number">0</span>]</span><br><span class="line">                sPool.Put(bufPtr)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中 <code>New</code> 用来给 <code>sync.Pool</code> 一个在没有可用对象时创建对象的构造函数，使用的时候使用 <code>Get</code> 方法从 Pool 里获取一个对象，用完了再用 <code>Put</code> 方法把对象还给 <code>sync.Pool</code>。这里主要注意一下对象的生命周期，以及放回到 <code>sync.Pool</code> 时需要清空对象，避免脏数据。测试结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        469431              2313 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          802392              1339 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPoolByElement-12                1212828               961.5 ns/op             0 B/op          0 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                3249004               370.2 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3268851               368.2 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate2-12                62596077                18.63 ns/op            0 B/op          0 allocs/op</span><br><span class="line">BenchmarkPool-12                        32707296                35.59 ns/op            0 B/op          0 allocs/op</span><br></pre></td></tr></table></figure><p>可见使用 <code>sync.Pool</code> 也可以避免内存分配，由于 <code>sync.Pool</code> 还有一些额外的处理性能消耗会比手动复用 Slice 稍高一些，不过考虑到使用的便利性以及相比不使用还是有明显的性能提升，还是一个不错的方案。</p><p>但是直接使用 <code>sync.Pool</code> 也有下面两个问题：</p><ol><li>对于 Slice 的情况 <code>New</code> 分配的初始内存是固定的，运行时使用空间超出的话，可能还会有大量动态的内存分配调整。</li><li>另一个极端是 Slice 被动态扩容很大后放回到 <code>sync.Pool</code> 中，可能会造成内存的泄漏和浪费。</li></ol><h1 id="bytebufferpool"><a href="#bytebufferpool" class="headerlink" title="bytebufferpool"></a>bytebufferpool</h1><p>为了达到实际运行时更优的性能，<a href="https://github.com/valyala/bytebufferpool">bytebufferpool</a> 这个项目在 <code>sync.Pool</code> 的基础上运用了一些简单的统计规律，尽可能的减少了上面提到的两个问题在运行时的影响。（该项目的作者是俄罗斯人，手下还有 fasthttp, quicktemplate 和 VictoriaMetrics 几个项目，个顶个都是性能优化的优秀案例，战斗民族经常会搞这种性能推极限的项目。</p><p>代码里主要的结构如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// bytebufferpool/pool.go</span></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">  minBitSize = <span class="number">6</span> <span class="comment">// 2**6=64 is a CPU cache line size</span></span><br><span class="line">  steps      = <span class="number">20</span></span><br><span class="line"> </span><br><span class="line">  minSize = <span class="number">1</span> &lt;&lt; minBitSize</span><br><span class="line">  maxSize = <span class="number">1</span> &lt;&lt; (minBitSize + steps - <span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">  calibrateCallsThreshold = <span class="number">42000</span></span><br><span class="line">  maxPercentile           = <span class="number">0.95</span></span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">type</span> Pool <span class="keyword">struct</span> &#123;</span><br><span class="line">  calls       [steps]<span class="type">uint64</span></span><br><span class="line">  calibrating <span class="type">uint64</span></span><br><span class="line"> </span><br><span class="line">  defaultSize <span class="type">uint64</span></span><br><span class="line">  maxSize     <span class="type">uint64</span></span><br><span class="line"> </span><br><span class="line">  pool sync.Pool</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中 <code>defaultSize</code> 的作用是 <code>New</code> 的时候给 Slice 分配的大小，<code>maxSize</code> 的作用是超过这个大小的 Slice <code>Put</code> 时会拒绝。核心的算法其实就是在运行时根据统计到的 Slice 使用大小信息动态的去调整 <code>defaultSize</code> 和 <code>maxSize</code> ，避免额外的内存分配同时还要避免内存泄漏。</p><p>这个动态统计的过程也比较简单，就是将 <code>Put</code> 到 Pool 里的 Slice 大小划分了 20 个区间范围进行统计，当 <code>Put</code> 次数达到 <code>calibrating</code> 后就进行一次排序，将这个时间段内使用最为频繁的区间大小作为 <code>defaultSize</code> 这样在统计上就可以避免不少额外的内存分配。然后按大小排序，将 95% 分位大小设置为  <code>maxSize</code>，这样就避免了在统计上长尾大的对象进入 Pool。就靠着这样动态调整这两个值，在统计上可以在运行时获得更优的性能。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>Slice 初始化尽可能指定 capacity</li><li>避免在循环中初始化 Slice</li><li>性能敏感路径考虑使用 <code>sync.Pool</code></li><li>内存分配的性能开销可能远大于业务逻辑</li><li>bytebuffer 的复用可以考虑看下 bytebufferpool</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%9F%BA%E7%A1%80%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95&quot;&gt;基础性能测试&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%95%B4%E4%B8%AA-slice-append&quot;&gt;整个</summary>
      
    
    
    
    
    <category term="性能" scheme="http://oilbeater.com/tags/%E6%80%A7%E8%83%BD/"/>
    
    <category term="golang" scheme="http://oilbeater.com/tags/golang/"/>
    
  </entry>
  
</feed>
