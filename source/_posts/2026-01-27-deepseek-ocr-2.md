---
title: DeepSeek OCR 2 —— 一个新的视觉模型架构
date: 2026-01-27 15:23:51
tags:
- deepseek
- paper
---

看了下 DeepSeek OCR 2 的论文，以我二把刀的知识分析一下。

这次是一个视觉模型编码器架构上的创新，主流的视觉模型编码器的方式是把图片分成一个个小块，变成一个个 token，然后按照从左上到右下逐行扫描的顺序一个个输入给模型。这其实和现在的 LLM 工作原理很像，只不过 LLM 是一串文本的 token，这里是一个个图片块的 token。然后还是 attention 那套两两比较，计算出每个 token 之间的关联关系，这样就可以通过不同 token 之间 attention 的权重关系得出不同图像块之间的关联，模型也就理解了整个图片。

看上去这个思路很好，统一了图片和文本的处理方式，但是图片和文本有个显著的区别，文本可以看做是一个一维的表现形式，你从左到右按顺序读，文本的逻辑大概率也是按照这个方向展开的。但是图片是个二维的表现形式，图片的逻辑并不是按照从左上到右下，图片里可能有斜线有曲线甚至有螺旋线，还是按照从左上到右下拆成的一维顺序是不是最有效的？至少人类看图片是不遵循这个顺序的。

虽然通过位置编码的方式可以把位置信息也加入到 attention 的计算，但是主流的位置编码也是基于文本一维顺序做的，是不是能很好的对图片这种二维表现形式进行位置编码呢？

DeepSeek OCR 2 的架构创新就在这里，他们在编码器里附加了一个小型的 LLM 可以推理出图片之间各个块的因果关系，然后按照关联关系对 token 进行重排序。可以认为模型是按照逻辑顺序去看图片而不是无脑的从左上到右下，这个逻辑顺序也会更接近人看图片的顺序。

性能提升，开销下降这属于 DeepSeek 的常规操作这里就不提了。我在想有没有可能视觉模型才是能力更强的模型的方向？

论文里提到这个模型主要作用还是读文档来生成语料，给 V 系列的语言模型用，但有没有可能把 OCR 系列和 V 系列直接融合。一方面图片里其实包含了文本里无法体现的内容信息，另一方面图片还比文本更节约 token。
按照 DeepSeek 一贯喜欢融合模型的作风，我觉得可以期待一下 V4 是个有初步视觉能力的模型了。
